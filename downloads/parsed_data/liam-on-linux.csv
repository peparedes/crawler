"names","values"
"itemid","191"
"event","<p dir=""ltr"">(Repurposed email reply)</p><p dir=""ltr"">Although I was educated & worked with DEC systems, I didn't have much to do with the company itself. Its support was good, the kit ludicrously expensive, and the software offerings expensive, slow and lacking competitive features. However, they also scored in some ways.</p><p dir=""ltr"">My 60,000' view:</p><p dir=""ltr"">Microsoft knew EXACTLY what it was doing with its practices when it built up its monopoly. It got lucky with the technology: its planned future super products flopped, but it turned on a dime & used what worked.</p><p dir=""ltr"">But killing its rivals, any potential rival? Entirely intentional.</p><p dir=""ltr"">The thing is that no other company was poised to effectively counter the MS strategy. Nobody.</p><p dir=""ltr"">MS' almost-entirely-software-only model was almost unique. Its ecosystem of apps and 3rd party support was unique.</p><p dir=""ltr"">In the end, it actually did us good. Gates wanted a computer on every desk. We got that.</p><p dir=""ltr"">The company's strategy called for open compatible generic hardware. We got that.</p><p dir=""ltr"">Only one platform, one OS, was big enough, diverse enough, to compete: Unix.</p><p dir=""ltr"">But commercial, closed, proprietary Unix couldn't. 2 ingredients were needed:</p><p dir=""ltr"">#1 COTS hardware - which MS fostered;<br>#2 FOSS software.</p><p dir=""ltr"">Your point about companies sharing their source is noble, but I think inadequate. The only thing that could compete with a monolithic software monopolist on open hardware was open software.</p><p dir=""ltr"">MS created the conditions for its own doom.</p><p dir=""ltr"">Apple cleverly leveraged FOSS Unix and COTS X86 hardware to take the Mac brand and platform forward.</p><p dir=""ltr"">Nobody else did, and they all died as a result.</p><p dir=""ltr"">If Commodore, Atari and Acorn had adopted similar strategies (as happened independently of them later, after their death, resulting in AROS, AFROS & RISC OS Open), they might have lived.</p><p dir=""ltr"">I can't see it fitting the DEC model, but I don't know enough. Yes, cheap low-end PDP-11s with FOSS OSes might have kept them going longer, but not saved them.</p><p dir=""ltr"">The deal with Compaq was catastrophic. Compaq was in Microsoft's pocket. I suspect that Intel leant on Microsoft and Microsoft then leant on Compaq to axe Alpha, and Compaq obliged. It also knifed HP OpenMail, possibly the Unix world's only viable rival to Microsoft Exchange.</p><p dir=""ltr"">After that it was all over bar the shouting.</p><p dir=""ltr"">Microsoft could not have made a success of OS/2 3 without Dave Cutler... But DEC couldn't have made a success out of PRISM either, I suspect. Maybe a stronger DEC would have meant Windows NT would never have happened.</p>"
"eventtime","2016-05-13 13:50:00"
"url","http://liam-on-linux.livejournal.com/49047.html"
"userid","8744"
"itemid","190"
"event","My contention is that a large part of the reason that we have the crappy computers that we do today -- lowest-common-denominator boxes, mostly powered by one of the kludgiest and most inelegant CPU architectures of the last 40 years -- is not technical, nor even primarily commercial or due to business pressures, but rather, it&#39;s cultural.

When I was playing with home micros (mainly Sinclair and Amstrad; the American stuff was just too expensive for Brits in the early-to-mid 1980s), the culture was that Real Men programmed in assembler and the main battle was Z80 versus 6502, with a few weirdos saying that 6809 was better than either. BASIC was the language for beginners, and a few weirdos maintained that Forth was better.

At university, I used a VAXcluster and learned to program in Fortran-77. The labs had Acorn BBC Micros in -- solid machines, <b>the</b> best 8-bit BASIC ever, and they could interface both with lab equipment over IEEE-488 and with generic printers and so on over Centronics parallel and its <a href=""https://en.wikipedia.org/wiki/RS-423"" target=""_blank"">RS-423</a> interface [EDIT: fixed!], which could talk to RS-232 kit.

As I discovered when I moved into the professional field a few years later (1988), this wasn&#39;t that different from the pro stuff. A lot of apps were written in various BASICs, and in the old era of proprietary OSes on proprietary kit, for performance, you used assembler.

But a new wave was coming. MS-DOS was already huge and the Mac was growing strongly. Windows was on v2 and was a toy, but Unix was coming to mainstream kit, or at least affordable kit. You could run Unix on PCs (e.g. SCO Xenix), on Macs (A/UX), and my employers had a demo IBM RT-6150 running AIX 1.

Unix wasn&#39;t only the domain (pun intentional) of expensive kit priced in the tens of thousands.

A new belief started to spread: that if you used C, you could get near-assembler performance without the pain, and the code could be ported between machines. DOS and Mac apps started to be written (or rewritten) in C, and some were even ported to Xenix. In my world, nobody used stuff like A/UX or AIX, and Xenix was specialised. I was aware of Coherent as the only &quot;affordable&quot; Unix, but I never saw a copy or saw it running.

So this second culture of C code running on non-Unix OSes appeared. Then the OSes started to scramble to catch up with Unix -- first OS/2, then Windows 3, then the for a decade parallel universe of Windows NT, until XP became established and Win9x finally died. Meanwhile, Apple and IBM flailed around, until IBM surrendered, Apple merged with NeXT and switched to NeXTstep.

Now, Windows is evolving to be more and more Unix-like, with GUI-less versions, clean(ish) separation between GUI and console apps, a new rich programmable shell, and so on.

While the Mac is now a Unix box, albeit a weird one.

Commercial Unix continues to wither away. OpenVMS might make a modest comeback. IBM mainframes seem to be thriving; every other kind of big iron is now emulated on x86 kit, as far as I can tell. IBM has successfully killed off several efforts to do this for z Series.

So now, it&#39;s Unix except for the single remaining mainstream proprietary system: Windows. Unix today means Linux, while the weirdoes use FreeBSD. Everything else seems to be more or less a rounding error.

C always was like carrying water in a sieve, so now, we have multiple C derivatives, trying to patch the holes. C++ has grown up but it&#39;s like Ada now: so huge that nobody understands it all, but actually, a fairly usable tool.

There&#39;s the kinda-sorta FOSS &quot;safe C++ in a VM&quot;, Java. The proprietary kinda-sorta &quot;safe C++ in a VM&quot;, C#. There&#39;s the not-remotely-safe kinda-sorta C in a web browser, Javascript.

And dozens of others, of course.

Even the safer ones run on a basis of C -- so the lovely cuddly friendly Python, that everyone loves, has weird C printing semantics to mess up the heads of beginners.

Perl has abandoned its base, planned to move onto a VM, then the VM went wrong, and now has a new VM and to general amazement and lack of interest, Perl 6 is finally here.

All the others are still implemented in C, mostly on a Unix base, like Ruby, or on a JVM base, like Clojure and Scala.

So they still have C like holes and there are frequent patches and updates to try to make them able to retain some water for a short time, while the &quot;cyber criminals&quot; make hundreds of millions.

Anything else is &quot;uncommercial&quot; or &quot;not viable for real world use&quot;.

Borland totally dropped the ball and lost a nice little earner in Delphi, but it continues as Free Pascal and so on.

Apple goes its own way, but has forgotten the truly innovative projects it had pre-NeXT, such as Dylan.

There were real projects that were actually used for real work, like Oberon the OS, written in Oberon the language. Real pioneering work in UIs, such as Jef Raskin&#39;s machines, the original Mac and Canon Cat -- forgotten. People rhapsodise over the Amiga and forget that the planned OS, CAOS, to be as radical as the hardware, never made it out of the lab. Same, on a smaller scale, with the Acorn Archimedes.

Despite that, of course, Lisp never went away. People still use it, but they keep their heads down and get on with it.

Much the same applies to Smalltalk. Still there, still in use, still making real money and doing real work, but forgotten all the same.

The Lisp Machines and Smalltalk boxes lost the workstation war. Unix won, and as history is written by the victors, now the alternatives are forgotten or dismissed as weird kooky toys of no serious merit.

The senior Apple people didn&#39;t understand the essence of what they saw at PARC: they only saw the chrome. They copied the chrome, not the essence, and now all that <b>any</b> of us have is the chrome. We have GUIs, but on top of the nasty kludgy hacks of C and the like. A late-&#39;60s skunkware project now runs the world, and the real serious research efforts to make something better, both before and after, are forgotten historical footnotes.

Modern computers are a vast disappointment to me. We have no thinking machines. The Fifth Generation, Lisp, all that -- gone.

What did we get instead?

Like dinosaurs, the expensive high-end machines of the &#39;70s and &#39;80s didn&#39;t evolve into their successors. They were just replaced. First little cheapo 8-bits, not real or serious at all, although they were cheap and people did serious stuff with them because it&#39;s all they could afford. The early 8-bits ran semi-serious OSes such as CP/M, but when their descendants sold a thousand times more, those descendants weren&#39;t running descendants of that OS -- no, it and its creator died.

CP/M evolved into a multiuser multitasking 386 OS that could run multiple MS-DOS apps on terminals, but it died.

No, then the cheapo 8-bits thrived in the form of an 8/16-bit hybrid, the 8086 and 8088, and a cheapo knock-off of CP/M.

This got a redesign into something grown-up: OS/2.

Predictably, that died.

So the hacked-together GUI for DOS got re-invigorated with an injection of OS/2 code, as Windows 3. That took over the world.

The rivals - the Amiga, ST, etc? 680x0 chips, lots of flat memory, whizzy graphics and sound? All dead.

Then Windows got re-invented with some OS/2 3 ideas and code, and some from VMS, and we got Windows NT.

But the marketing men got to it and ruined its security and elegance, to produce the lipstick-and-high-heels Windows XP. That version, insecure and flakey with its terrible bodged-in browser, that, of course, was the one that sold.

Linux got nowhere until it copied the XP model. The days of small programs, everything&#39;s a text file, etc. -- all forgotten. Nope, lumbering GUI apps, CORBA and RPC and other weird plumbing, huge complex systems, but it looks and works kinda like Windows and a Mac now so it looks like them and people use it.

Android looks kinda like iOS and people use it in their billions. Newton? Forgotten. No, people have Unix in their pocket, only it&#39;s a bloated successor of Unix.

The efforts to fix and improve Unix -- Plan 9, Inferno -- forgotten. A proprietary microkernel Unix-like OS for phones -- Blackberry 10, based on QNX -- not Androidy enough, and bombed.

We have less and less choice, made from worse parts on worse foundations -- but it&#39;s colourful and shiny and the world loves it.

That makes me despair.

We have poor-quality tools, built on poorly-designed OSes, running on poorly-designed chips. Occasionally, fragments of older better ways, such as functional-programming tools, or Lisp-based development environments, are layered on top of them, but while they&#39;re useful in their way, they can&#39;t fix the real problems underneath.

Occasionally someone comes along and points this out and shows a better way -- such as Curtis Yarvin&#39;s Urbit. Lisp Machines re-imagined for the 21st century, based on top of modern machines. But nobody gets it, and its programmer has some unpleasant and unpalatable ideas, so it&#39;s doomed.

And the kids who grew up after C won the battle deride the former glories, the near-forgotten brilliance that we have lost.

And it almost makes me want to cry sometimes.

We should have brilliant machines now, not merely Steve Jobs&#39; &quot;bicycles for the mind&quot;, but Gossamer Albatross-style hang-gliders for the mind.

But we don&#39;t. We have glorified 8-bits. They multitask semi-reliably, they can handle sound and video and 3D and look pretty. On them, layered over all the rubbish and clutter and bodges and hacks, inspired kids are slowly brute-forcing machines that understand speech, which can see and walk and drive.

But it could have been so much better.

Charles Babbage didn&#39;t finish the Difference Engine. It would have paid for him to build his Analytical Engine, and that would have given the Victorian British Empire the steam-driven computer, which would have transformed history.

But he got distracted and didn&#39;t deliver.

We started to build what a few old-timers remember as brilliant machines, machines that helped their users to think and to code, with brilliant -- if flawed -- software written in the most sophisticated computer languages yet devised, by the popular acclaim of the people who really know this stuff: Lisp and Smalltalk.

But we didn&#39;t pursue them. We replaced them with something cheaper -- with Unix machines, an OS only a nerd could love. And then we replaced the Unix machines with something cheaper still -- the IBM PC, a machine so poor that the &pound;125 ZX Spectrum had better graphics and sound.

And now, we all use descendants of that. Generally acknowledged as one of the poorest, most-compromised machines, based on descendants of one of the poorest, most-compromised CPUs.

Yes, over the 40 years since then, most of rough edges have been polished out. The machines are now small, fast, power-frugal with tons of memory and storage, with great graphics and sound. But it&#39;s taken decades to get here.

And the OSes have developed. Now they&#39;re feature-rich, fairly friendly, really very robust considering the stone-age stuff they&#39;re built from.

But if we hadn&#39;t spent 3 or 4 decades making a pig&#39;s ear into silk purse -- if we&#39;d started with a silk purse instead -- where might we have got to by now?"
"eventtime","2016-04-27 19:06:00"
"url","http://liam-on-linux.livejournal.com/48669.html"
"userid","8744"
"itemid","189"
"event","More retrocomputing meanderings -- whatever became of the ST, Amiga and Acorn operating systems?

The Atari ST&#39;s GEM desktop also ran on MS-DOS, DR&#39;s own DOS+ (a forerunner of the later DR-DOS) and today is included with FreeDOS.&nbsp;In fact the first time I installed FreeDOS I was *very* surprised to find my name in the credits. I debugged some batch files used in installing the GEM component.

The ST&#39;s GEM was the same environment. ST GEM was derived from GEM 1; PC GEM from GEM 2, crippled after an Apple lawsuit. Then they diverged. FreeGEM attempted to merge them again.

But the ST&#39;s branch prospered, before the rise of the PC killed off all the alternative platforms. Actual STs can be quite cheap now, or you can even buy a modern clone:

http://harbaum.org/till/mist/index.shtml

If you don&#39;t want to lash out but have a PC, the Aranym environment gives you something of the feel of the later versions. It&#39;s not exactly an emulator, more a sort of compatibility environment that enhances the &quot;emulated&quot; machine as much as it can using modern PC hardware.

http://aranym.org/

And the ST GEM OS was so modular, different 3rd parties cloned every components, separately. Some commercially, some as FOSS. The Aranym team basically put together a sort of &quot;distribution&quot; of as many FOSS components as they could, to assemble a nearly-complete OS, then wrote the few remaining bits to glue it together into a functional whole.

So, finally, after the death of the ST and its clones, there was an all-FOSS OS for it. It&#39;s pretty good, too. It&#39;s called AFROS, Atari Free OS, and it&#39;s included as part of Aranym.

I longed to see a merger of FreeGEM and Aranym, but it was never to be.

The history of GEM and TOS is complex.

Official Atari TOS+GEM evolved into TOS 4, which included the FOSS Mint multitasking later, which isn&#39;t much like the original ROM version of the first STs.

The underlying TOS OS is not quite like anything else.

AIUI, CP/M-68K was a real, if rarely-seen, OS.

However, it proved inadequate to support GEM, so it was discarded. A new kernel was written using some of the tech from what was later to become DR-DOS on the PC -- something less like CP/M and more like MS-DOS: directories, separated with backslashes; FAT format disks; multiple executable types, 8.3 filenames, all that stuff.

None of the command-line elements of CP/M or any DR DOS-like OS were retained -- the kernel booted the GUI directly and there was no command line, like on the Mac.

This is called GEMDOS and AIUI it inherits from both the CP/M-68K heritage and from DR&#39;s x86 DOS-compatible OSes.

The PC version of GEM also ran on Acorn&#39;s BBC Master 512 which had an&nbsp;Intel 80186 coprocessor. It was a very clever machine,&nbsp;in a limited way.

Acorn&#39;s series of machines are not well-known in the US, AFAICT, and that&#39;s a shame. They were technically interesting, more so IMHO than the Apple II and III, TRS-80 series etc.

The original Acorns were 6502-based, but with good graphics and sound, a plethora of ports, a clear separation between OS, BASIC and add-on ROMs such as the various DOSes, etc. The BASIC was, I&#39;d argue strongly, *the* best 8-bit BASIC ever: named procedures, local variables, recursion, inline assembler, etc. Also the fastest BASIC interpreter ever, and quicker than some compiled BASICs.

Acorn built for quality, not price; the machines were aimed at the educational market, which wasn&#39;t so price-sensitive, a model that NeXT emulated. Home users were welcome to buy them &amp; there was one (unsuccessful) home model, but they were unashamedly expensive and thus uncompromised.

The only conceptual compromise in the original BBC Micro was that there was provision for ROM bank switching, but not RAM. The 64kB memory map was 50:50 split ROM and RAM. You could switch ROMs, or put RAM in their place, but not have more than 64kB. This meant that the high-end machine had only 32kB RAM, and high-res graphics modes could take 21kB or so, leaving little space for code -- unless it was in ROM, of course.

The later BBC+ and BBC Master series fixed that. They also allowed ROM cartridges, rather than bare chips inserted in sockets on the main board, and a numeric keypad.

Acorn looked at the 16-bit machines in the mid-80s, mostly powered by Motorola 68000s of course, and decided they weren&#39;t good enough and that the tiny UK company could do better. So it did.

But in the meantime, it kept the 6502-based, resolutely-8-bit BBC Micro line alive with updates and new models, including ROM-based terminals and machines with a range of built-in coprocessors: faster 6502-family chips for power users, Z80s for CP/M, Intel&#39;s 80186 for kinda-sorta PC compatibility, the NatSemi 32016 with PANOS for ill-defined scientific computing, and finally, an ARM copro before the new ARM-based machines were ready. 

Acorn designed the ARM RISC chip in-house, then launched its own range of ARM-powered machines, with an OS based on the 6502 range&#39;s. Although limited, this OS is still around today and can be run natively on a Raspberry Pi:

https://www.riscosopen.org/content/

It&#39;s very idiosyncratic -- both the filesystem, the command line and the default editor are totally unlike anything else. The file-listing command is CAT, the directory separator is a full stop (i.e. a period), while the root directory is called $. The editor is a very odd dual-cursor thing. It&#39;s fascinating, totally unrelated to the entire DEC/MS-DOS family and to the entire Unix family. There is literally and exactly nothing else even slightly like it.

It was the first GUI OS to implement features that are now universal across GUIs: anti-aliased font rendering, full-window dragging and resizing (as opposed to an outline), and significantly, the first graphical desktop to implement a taskbar, before NeXTstep and long before Windows 95.

It supports USB, can access the Internet and WWW. There are free clients for chat, email, FTP, the WWW etc. and a modest range of free productivity tools, although most things are commercial.

But there&#39;s no proper inter-process memory protection, GUI multitasking is cooperative, and consequently it&#39;s not amazingly stable in use. It does support pre-emptive multitasking, but via the text editor, bizarrely enough, and only of text-mode apps. There was also a pre-emptive multitasking version of the desktop, but it wasn&#39;t very compatible, didn&#39;t catch on and is not included in current versions.

But saying all that, it&#39;s very interesting, influential, shared-source, entirely usable today, and it runs superbly on the &pound;25 Raspberry Pi, so there is little excuse not to try it. There&#39;s also a FOSS emulator which can run the modern freeware version:

http://www.marutan.net/rpcemu/

For users of the old hardware, there&#39;s a much more polished commercial emulator for Windows and Mac which has its own, proprietary fork of the OS:

http://www.virtualacorn.co.uk/index2.htm

There&#39;s an interesting parallel with the Amiga. Both Acorn and Commodore had ambitious plans for a modern multitasking OS which they both referred to as Unix-like. In both cases, the project didn&#39;t deliver and the ground-breaking, industry-redefiningly capable hardware was instead shipped with much less ambitious OSes, both of which nonetheless were widely-loved and both of which still survive in the form of multiple, actively-maintained forks, today, 30 years later -- even though Unix in fact caught up and long surpassed these 1980s oddballs.

AmigaOS, based in part on the academic research OS Tripos, has 3 modern forks: the FOSS AROS, on x86, and the proprietary MorphOS and AmigaOS 4 on PowerPC.

Acorn RISC OS, based in part on Acorn MOS for the 8-bit BBC Micro, has 2 contemporary forks: RISC OS 5, owned by Castle Technology but developed by RISC OS Open, shared source rather than FOSS, running on Raspberry Pi, BeagleBoard and some other ARM boards, plus some old hardware and RPC Emu; and RISC OS 4, now owned by the company behind VirtualAcorn, run by an ARM engineer who apparently made good money selling software ARM emulators for x86 to ARM holdings.

Commodore and the Amiga are both long dead and gone, but the name periodically changes hands and reappears on various bits of modern hardware.

Acorn is also long dead, but its scion ARM Holdings designs the world&#39;s most popular series of CPUs, totally dominates the handheld sector, and outsells Intel, AMD &amp; all other x86 vendors put together something like tenfold.

Funny how things turn out.</swiftgriggs@gmail.com>"
"eventtime","2016-04-25 15:55:00"
"url","http://liam-on-linux.livejournal.com/48593.html"
"userid","8744"
"itemid","188"
"event","I am told it&#39;s lovely to use. Sadly, it only runs on <a href=""http://www.a-eon.com/?page=x1000"" target=""_blank"">obscure PowerPC-based kit</a> that costs a couple of thousand pounds and can be out-performed by
a &pound;300 PC. 

AmigaOS&#39;s owners -- <a href=""http://hyperion-entertainment.biz/"" target=""_blank"">Hyperion</a>, I believe -- chose the wrong platform.

On a Raspberry Pi or something, it would be great. On obscure&nbsp;expensive PowerPC kit, no.

Also, saying that, I got my first Amiga in the early 2000s. If I&#39;d had&nbsp;one 15y earlier, I&#39;d probably have loved it, but I bought a 2nd hand
Archimedes instead (and still think it was the right choice for a&nbsp;non-gamer and dabbler in programming).

A few years ago, with a <b>LOT</b> of work using 3 OSes and 3rd-party&nbsp;disk-management tools, I managed to coax MorphOS onto my Mac mini G4.
Dear hypothetical gods, that was a hard install.

It&#39;s... well, I mean, it&#39;s fairly fast, but... no Wifi? No Bluetooth?

And the desktop. It got hit hard with the ugly stick. I mean, OK, it&#39;s&nbsp;not as bad as KDE, but... ick.

Learning <a href=""http://www.amigaos.net/"" target=""_blank"">AmigaOS</a> when you already know more modern OSes -- OS X,&nbsp;Linux, gods help us, even Windows -- well, the Amiga seems pretty
weird, and often for no good reason. E.g. a graphical file manager, but not all files have icons. They&#39;re not hidden, they just don&#39;t have
icons, so if you want to see them, you have to do a second show-all&nbsp;operation. And the dependence on RAMdisks, which are a historical&nbsp;curiosity now. And the needing to right-click to show the menu-bar&nbsp;when it&#39;s on a screen edge.

A lot of pointless arcana, just so Apple didn&#39;t sue, AFAICT.

I understand the love if one loved it back then. But now?&nbsp;Yeeeeeeaaaaaah, not so much.

Not that I&#39;m proclaiming <a href=""https://www.riscosopen.org/content/"" target=""_blank"">RISC OS</a> to be the business now. I like it,&nbsp;but it&#39;s weird too. But AmigaOS does seem a bit primitive now. OTOH,&nbsp;if they sorted out multiprocessor support and memory protection and it&nbsp;ran on cheap ARM kit, then yeah, I&#39;d be interested."
"eventtime","2016-04-05 14:57:00"
"url","http://liam-on-linux.livejournal.com/48183.html"
"userid","8744"
"itemid","187"
"event","<div>I recently read that a friend of mine claimed that <span style=""line-height: 1.4;"">&quot;Both the iPhone and iPod </span><span style=""line-height: 1.4;"">were copied from other manufacturers, to a large extent.&quot;

This is a risible claim, AFAICS.

There were <a href=""https://en.wikipedia.org/wiki/Personal_Jukebox"" target=""_blank"">pocket MP3 jukeboxes</a> before the iPod. I still <a href=""https://en.wikipedia.org/wiki/Archos_Jukebox_series#Jukebox_Recorder"" target=""_blank"">own one</a>. They were fairly tragic efforts.

There were smartphones before the iPhone. I still have at least <a href=""https://en.wikipedia.org/wiki/Nokia_E90_Communicator"" target=""_blank"">one of them</a>, too. Again, really tragic from a human-computer interaction point of view.</span>

<span style=""line-height: 1.4;"">AIUI, the iPhone originated internally as a shrunk-down tablet. The tablet originated from a personal comment from Bill Gates to Steve Jobs that although tablets were a great idea, people simply didn&rsquo;t want tablets because Microsoft had made them and they didn&rsquo;t sell.</span>
<lj-cut></lj-cut></div><div>Jobs&rsquo; response was that the Microsoft ones didn&rsquo;t sell because they were no good, not because people didn&rsquo;t want tablets. In particular, Jobs stated that using a stylus was a bad idea. (This is also a pointer was to why he cancelled the Newton. And guess what? I&#39;ve got <a href=""https://en.wikipedia.org/wiki/MessagePad"" target=""_blank"">one of them</a>, too.)

<span style=""line-height: 1.4;"">Gates, naturally, contested this, and J</span><a href=""http://www.businessinsider.com/steve-jobs-describing-the-moment-he-decided-to-do-the-ipad-2013-5"" style=""line-height: 1.4;"" target=""_blank"">obs started an internal project to prove him wrong</a><span style=""line-height: 1.4;"">: a stylus-free finger-operated slim light tablet. However, when it was getting to prototype form, he allegedly realised, with remarkable prescience, that the market wasn&rsquo;t ready yet, and that people needed a first step &mdash; a smaller, lighter, simpler, pocketable device, based on the finger-operated tablet. </span>

<span style=""line-height: 1.4;"">Looking for a role or function for such a device, the company came up with the idea of a smartphone. </span>

<span style=""line-height: 1.4;"">Smartphones certainly existed, but they were a geek toy, nothing more.</span>

<span style=""line-height: 1.4;"">Apple was bold enough to make a move that would kill its most profitable line &mdash; the iPod &mdash; with a new product. Few would be so bold.</span>

<span style=""line-height: 1.4;"">I can&rsquo;t think of any other company that would have been bold enough to invent the iPhone. We might have got to devices as capable as modern smartphones and tablets, but I suspect they&rsquo;d have still been festooned in buttons and a lot clumsier to use.</span>

<span style=""line-height: 1.4;"">It&rsquo;s the GUI story again. Xerox sponsored the invention and original development but didn&rsquo;t know WTF to do with it. Contrary to the popular history, it did productise it, but as a vastly expensive specialist tool. It took Apple to make it the standard method of HCI, and it took Apple two goes and many years. The Lisa was still too fancy and expensive, and the original Mac too cut-down and too small and compromised.</span>

<span style=""line-height: 1.4;"">The many rivals&rsquo; efforts were, in hindsight, almost embarrassingly bad. </span><a href=""https://en.wikipedia.org/wiki/IBM_TopView"" style=""line-height: 1.4;"" target=""_blank"">IBM&rsquo;s TopView</a><span style=""line-height: 1.4;""> was a pioneering GUI and it was rubbish. Windows 1 and 2 were rubbish. OS/2 1.x was rubbish, and to be honest, OS/2 2.x was the pre-iPhone smartphone of GUI OSes: very capable, but horribly complex and fiddly.</span>

<span style=""line-height: 1.4;"">Actually, arguably &mdash; and demonstrably, from the Atari ST market &mdash; DR GEM was a far better GUI than Windows 1 or 2. GEM was a rip-off of the Mac; the PC version got sued and crippled as a result, so blatant was it. It took MS over a decade to learn from the Mac (and GEM) and produce the first version of Windows with a GUI good enough to rival the Mac&rsquo;s, </span><b style=""line-height: 1.4;"">while being d</b><b style=""line-height: 1.4;"">ifferent enough not to get sued</b><span style=""line-height: 1.4;"">: Windows 95.</span>

<span style=""line-height: 1.4;"">Now, 2 decades later, </span><a href=""http://www.theregister.co.uk/2013/06/03/thank_microsoft_for_linux_desktop_fail/"" style=""line-height: 1.4;"" target=""_blank"">everyone&rsquo;s GUI borrows from Win95</a><span style=""line-height: 1.4;"">. Linux is still struggling to move on from Win95-like desktops, and even Mac OS X, based on a product which inspired Win95, borrows some elements from the Win95 GUI.</span>

<span style=""line-height: 1.4;"">Everyone copies MS, and MS copies Apple. Apple takes bleeding-edge tech and turns geek toys into products that the masses actually want to buy. </span>

<span style=""line-height: 1.4;"">Microsoft&rsquo;s success is founded on the IBM PC, and that was IBM&rsquo;s response to the Apple ][.</span>

<span style=""line-height: 1.4;"">Apple has been doing this consistently for about 40 years. It often takes it 2 or 3 goes, but it does.</span></div><ul>
<li>First time: 8-bit home micros (the Apple ][, an improved version of a DIY kit.)</li>
<li><span style=""line-height: 1.4;"">Second time: GUIs (first the Lisa, then the Mac).</span></li>
<li><span style=""line-height: 1.4;"">Third time: USB (on the iMac, arguably the first general-purpose PC designed and sold for Internet access as its primary function).</span></li>
<li><span style=""line-height: 1.4;"">Fourth time: digital music players (the iPod wasn&rsquo;t even the first with a hard disk).</span></li>
<li><span style=""line-height: 1.4;"">Fifth time: desktop Unix (OS X, based on NeXTstep).</span></li>
<li><span style=""line-height: 1.4;"">Sixth time: smartphones (based on what became the iPad, remember).</span></li>
<li><span style=""line-height: 1.4;"">Seventh time: tablets (the iPad, actually progenitor of the iPhone rather than the other way round).</span></li>
</ul><div><span style=""line-height: 1.4;"">Yes, there are too many Mac fans, and they&rsquo;re often under-informed. But there are also far to many Microsoft apologists, and too many Linux ones, too.</span>

<span style=""line-height: 1.4;"">I use an Apple desktop, partly because with a desktop, I can choose my own keyboard and pointing device. I hate modern Apple ones.</span>

<span style=""line-height: 1.4;"">I don&rsquo;t use Apple laptops or phones. I&rsquo;ve owned multiple examples of both. I prefer the rivals.</span>

<span style=""line-height: 1.4;"">My whole career has been largely propelled by Microsoft products. I still use some, although my laptops run Linux, which I much prefer.</span>

<span style=""line-height: 1.4;"">I am not a fanboy of any of them, but sadly, anyone who expresses fondness or admiration for anything Apple will be inevitably branded as one by the Anti-Apple fanboys, whose ardent advocacy is just as strong and just as irrational.</span>

<span style=""line-height: 1.4;"">As will this.</span></div>"
"eventtime","2016-03-30 20:33:00"
"url","http://liam-on-linux.livejournal.com/48018.html"
"userid","8744"
"itemid","186"
"event","<div>I&#39;m very fond of Spectrums (Spectra?) because they&#39;re the first computer I owned. I&#39;d used my uncle&#39;s ZX-81, and one belonging to a neighbour, and Commodore PETs at school, but the PET was vastly too expensive and the ZX-81 too limited to be of great interest to me.

<span style=""line-height: 1.4;"">I read an article once that praised Apple for bringing home computers to the masses with the Apple ][, the first home computer for under US$ 1000. A thousand bucks? That was fantasy winning-the-football-pools money!</span>

<span style=""line-height: 1.4;"">No, for me, the hero of the home computer revolution was Sir Clive Sinclair, for bringing us the first home computer for under GB &pound;100. A hundred quid was achievable. A thousand would have gone on a newer car or a family holiday.</span></div><lj-cut><div>
<span style=""line-height: 1.4;"">In 1982, my parents could just about afford a 2nd hand 48k Spectrum. I think they paid &pound;80 for it, postage included. I was so excited to receive it, I couldn&#39;t wait to try it out. Rearranging a corner with a desk and a portable TV would take too long. So I lay on the lounge floor, Spectrum plugged into the family colour TV and sitting on the carpet. Said carpet, of course, blocked the vents on the bottom of the Speccy so it overheated and died in half an hour.</span>

<span style=""line-height: 1.4;"">Happily, it was under guarantee. I sent it back, the original owner got a warranty repair, returned it to me, and I took much better care of it after that.</span>

<span style=""line-height: 1.4;"">I learned BASIC and programming on it. My favourite program was Beta BASIC, which improved the language and the editor. I wrote programs under Beta BASIC, being careful to use a subset of BASIC that I could compile with HiSoft BASIC for the best of the Speccy&#39;s meagre performance.</span>

<span style=""line-height: 1.4;"">I put it in an </span><a href=""http://www.computinghistory.org.uk/det/12269/LMT-68FX2-Keyboard-for-the-Spectrum/"" style=""line-height: 1.4;"" target=""_blank"">LMT 68FX2 keyboard</a><span style=""line-height: 1.4;""> for it.</span>

<span style=""line-height: 1.4;"">Then an Interface 1 and a microdrive. A terrible storage system. I told myself I was used to Sinclair cost-cutting and it would be OK. It wasn&#39;t. It was slow and unreliable and the sub-100 kB capacity was crap. I bought special formatting tools to get more capacity, and the reliability got even worse. My watchword became &quot;save 2 copies of everything!&quot; It still stands me in good stead today, when Microsoft Word crashes occasionally corrupt a document or I absent-mindedly save over something important.</span>

<span style=""line-height: 1.4;"">So I replaced the 48k Spectrum with a discount ex-demo 128, bought from Curry&#39;s. I could save work-in-progress programs to the RAMdisk, then onto Microdrive when they sort of worked. Annoyingly it wouldn&#39;t fit into the keyboard. I put the 48&#39;s PCB back into its old case and sold it, and mothballed the keyboard. To my surprise and joy, I found it in 2014 when packing up my house to move abroad. It </span><a href=""http://blog.tynemouthsoftware.co.uk/2015/12/day-8-zx-spectrum-lmt-68fx2-case-with.html"" style=""line-height: 1.4;"" target=""_blank"">now has a Raspberry Pi 2 in it</a><span style=""line-height: 1.4;"">, and any day now I will fit my new RasPi 3 into it for extra WLAN goodness.</span>

<span style=""line-height: 1.4;"">At Uni, I bought an MGT +D and a 5&frac14;&quot; floppy, plus a cheap Panasonic 9-pin dot-matrix printer. The luxury of fast, reliable storage -- wow! 780kB per disk! Yes, the cool kids had the fancy new 3&frac12;&quot; drives, but they cost more and the media were 10x more expensive and I was a poor student.</span>

<span style=""line-height: 1.4;"">The +D was horrendously unreliable, and MGT were real stars. They invited me to their Cambridge office where </span><a href=""http://www.worldofsam.org/node/250"" style=""line-height: 1.4;"" target=""_blank"">Alan Miles</a><span style=""line-height: 1.4;""> plied me with coffee, showed me around and chatted while </span><a href=""http://www.worldofsam.org/node/326"" style=""line-height: 1.4;"" target=""_blank"">Bruce Gordon</a><span style=""line-height: 1.4;""> fixed my interface. The designer himself! How&#39;s that for customer service?</span>

<span style=""line-height: 1.4;"">I am not sure now, 30 years later, but I think they gave me a very cheap deal on a DISCiPLE because they couldn&#39;t get the +D to run reliably. Total stars. I later bought a SAM Coup&eacute; out of loyalty, but lovely machine as it was, my Acorn Archimedes A310 was my real love by then. There was really no comparison between even one of the best-designed 8-bit home computers ever and a 32-bit RISC workstation.</span>

<span style=""line-height: 1.4;"">I wrote my uni essays on that Spectrum 128; I was the only person in my year at Uni to have their own computer!</span>

<span style=""line-height: 1.4;"">Years later, I bought a second 128 from an ad in Micro Mart, just to get the super-rare numeric keypad, Spanish keycaps and all. I sold the computer and kept the keypad.</span>

<span style=""line-height: 1.4;"">So I was a Sinclair fan because their low cost meant I could slowly, piecemeal, acquire and expand the machines and peripherals. I never had the money for an up-front purchase of a machine with a decent keyboard and a good BASIC and a disk interface, such as a BBC Micro, much as I would have liked one. I was never interested in the C64 because its BASIC was so poor.</span>

<span style=""line-height: 1.4;"">The modern fascination with them mystifies me a bit. I loved mine because it was cheap enough to be accessible to me; those of its limitations that I couldn&#39;t fix, such as the poor graphics, or the lack of integer variables and consequent poor BASIC performance, really annoyed me. The crappy keyboard, I replaced. Then the crappy (but, yes, impressively cheap) mass storage: replaced. The BASIC, kinda sorta replaced. Subsequent additions: proper mass storage, better DOS, proper dot-matrix printer on proper Centronics interface.</span>

<span style=""line-height: 1.4;"">Later, I even replaced the DOS in my DISCiPLE with </span><a href=""http://www.worldofspectrum.org/infoseek.cgi?regexp=^Uni-Dos$&amp;pub=^S%2eD%2e+Software$&amp;loadpics=1"" style=""line-height: 1.4;"" target=""_blank"">Uni-DOS</a><span style=""line-height: 1.4;"">, replacing sequential file handling (which I never used) with subdirectories (which were massively handy. I mean, nearly a megabyte of storage!)</span>

<span style=""line-height: 1.4;"">I was never much of a gamer. I&#39;m still not. At school, I collected and swapped games like some kids collect stamps -- the objective was to own a good collection, not to play them. I usually tried each game a couple of times, but no more. Few kept my attention: The Hobbit, the Stranglers&#39; Aural Quest, The Valley, Jet-Pac. Some of the big hits that everyone else loved, like Manic Miner and Jet Set Willy, I hated. Irritating music, very hard gameplay, and so repetitive.</span>

<span style=""line-height: 1.4;"">And yet now, people are so nostalgic for the terrible keyboard, they crowd-funded a new version! One of the first things I replaced, it was so bad! There are new models, new hardware, all to play the to be honest really quite bad games. Poor graphics, lousy sound on the 48. And yet everyone rhapsodises about them.</span>

<span style=""line-height: 1.4;"">I agree that, back then, game design was more innovative and gameplay often more varied and interesting than it is today. Now, the graphics look amazing but there seem to me to be about half a dozen different basic styles of gameplay, but with different plots, visuals and soundtracks. Where is the innovation of the level of The Sentinel or Elite or Marble Madness or Q-Bert?</span>

<span style=""line-height: 1.4;"">I have a few times played Jet-Pac in an emulator, but I am not a retro-gamer. I enjoy playing with my Toastrack, immaculately restored by </span><a href=""http://www.mutant-caterpillar.co.uk/"" style=""line-height: 1.4;"" target=""_blank"">Mutant Caterpillar</a><span style=""line-height: 1.4;"">, and my revivified LMT 68FX2, given a brain-transplant by </span><a href=""http://www.tynemouthsoftware.co.uk/"" style=""line-height: 1.4;"" target=""_blank"">Tynemouth Software</a><span style=""line-height: 1.4;"">. The things I loved about my Sinclairs seem to be forgotten now, and modern Spectrum aficionadi as nostalgic about the very things I resented -- the poor graphics and bargain-basement sound -- or replaced: the rotten keyboard. It is so weird that I can&#39;t relate to it, but hey, I&#39;m happy that the machines still exist and that there&#39;s an active user community.</span></div></lj-cut>"
"eventtime","2016-03-21 14:57:00"
"url","http://liam-on-linux.livejournal.com/47798.html"
"userid","8744"
"itemid","185"
"event","In lieu of real content, a repurposed FB comment, &#39;cos I thought it stood alone fairly well. I&#39;m meant to be writing about containers and the FB comment was a displacement activity.

<hr />
The first single-user computers started to appear in the mid-1970s, such as the MITS Altair. These had no storage at all in their most minimal form -- you entered code into their few hundreds of bytes of memory (not MB, not kB, just 128 bytes or so.)

One of the things that was radical is that they had a microprocessor: the CPU was a single chip. Before that, processors were constructed from lots of components, e.g. the KENBAK-1.

A single-user desktop computer with a microprocessor was called a microcomputer.

So, in the mid- to late-1970s, hard disks were *extremely* expensive -- thousands of $/&pound;, more than the computer itself. So nobody fitted them to microcomputers.

Even floppy drives were quite expensive. They&#39;d double the price of the computer. So the first mass-produced &quot;micros&quot; saved to audio tape cassette. No disk drive, no disk controller -- it was left out to save costs.

If the machine was modular enough, you could add a floppy disk controller later, and plug a floppy drive into that.

With only tape to load and save from, working at 1200 bits per second or so, even small programs of a few kB took minutes to load. So the core software was built into a permanent memory chip in the computer, called a ROM. The computer didn&#39;t boot: you turned it on, and it started running the code in the ROM. No loading stage necessary, but you couldn&#39;t update or change it without swapping chips. Still, it was really tiny, so bugs were not a huge problem.

Later, by a few years into the 1980s, floppy drives fell in price so that high-end micros had them as a common accessory, although still not built in as standard for most.

But the core software was still on a ROM chip. They might have a facility to automatically run a program on a floppy, but you had to invoke a command to trigger it -- the computer couldn&#39;t tell when you inserted a diskette.

By the 16-bit era, the mid-1980s, 3.5&quot; drives were cheap enough to bundle as standard. Now, the built-in software in the ROM just had to be complex enough to start the floppy drive and load the OS from there. Some machines still kept the whole OS in ROM though, such as the Atari ST and Acorn Archimedes. Others, like the Commodore Amiga, IBM PC &amp; Apple Macintosh, loaded it from diskette.

Putting it on diskette was cheaper, it meant you could update it easily, or even replace it with alternative OSes -- or for games, do without an OS altogether and boot directly into the game.

But hard disks were still seriously expensive, and needed a separate hard disk controller to be fitted to the machine. Inexpensive home machines like the early or basic-model Amigas and STs didn&#39;t have one -- again, it was left out for cost-saving reasons.

On bigger machines with expansion slots, you could add a hard disk controller and it would have a ROM chip on it that added the ability to boot from a hard disk connected to the controller card. But if your machine was a closed box with no internal slots, it was often impossible to add such a controller, so you might get a machine which later in its life had a hard disk controller and drive added, but the ROMs couldn&#39;t be updated so it wasn&#39;t possible to boot from the hard disk.

But this was quite rare. The 2nd ever model of Mac, the Mac Plus, added SCSI ports, the PC was always modular, and the higher-end models of STs, Amigas and Archimedes had hard disk interfaces.

The phase of machines with HDs but booting from floppy was fairly brief and they weren&#39;t common.

If the on-board ROMs could be updated, replaced, or just supplemented with extra ones in the HD controller, you could add the ability to boot from HD. If the machine booted from floppy anyway, this wasn&#39;t so hard.

<hr />
Which reminds me -- I am still looking for an add-on hard disk for an Amstrad PCW, if anyone knows of such a thing!"
"eventtime","2016-02-29 23:36:00"
"url","http://liam-on-linux.livejournal.com/47402.html"
"userid","8744"
"itemid","184"
"event","Today, Linux is Unix. And Linux is a traditional, old-fashioned, native-binary, honking great monolithic lump of code in a primitive, unsafe, 1970s language.

The sad truth is this:

Unix is not going to evolve any more. It hasn&#39;t evolved much in 30 years. It&#39;s just being refined: the bugs are gradually getting caught, but no big changes have happened since the 1980s.

Dr Andy Tanenbaum was right in 1991. Linux <b>is</b> obsolete.

Many old projects had a version numbering scheme like, e.g., SunOS:

Release 1.0, r2, r3, r4...

Then a big rewrite: Version 2! Solaris! (AKA SunOS 5)

Then Solaris 2, 3, 4, 5... now we&#39;re on 11 and counting.

Windows reset after v3, with NT. Java did the reverse after 1.4: Java 1.5 was &quot;Java 5&quot;. Looks more mature, right? Right? 

Well, Unix dates from between 1970 and the rewrite in C in 1972. Motto: &quot;Everything&#39;s a file.&quot;

Unix 2.0 happened way back in the 1980s and was released in 1991: Plan 9 from Bell Labs.

It was Unix, but with even more things turned into files. Integrated networking, distributed processes and more.

The world ignored it.

Plan 9 2.0 was Inferno: it went truly platform-neutral. C was replaced by Limbo, type-safe, compiling code down to binaries that ran on Dis, a universal VM. Sort of like Java, but better and reaching right down into the kernel.

The world ignored that, too.

Then came the idea of microkernels. They&#39;ve been tried lots of times, but people seized on the idea of early versions that had problems -- Mach 1 and Mach 2 -- and failed projects such as the GNU HURD.

They ignore successful versions:
* Mach 3 as used in Mac OS X and iOS
* DEC OSF/1, later called DEC Tru64 Unix, also based on Mach
* QNX, a proprietary true-microkernel OS used widely around the world since the 1980s, now in Blackberry 10 but also in hundreds of millions of embedded devices.

All are proper solid commercial successes.

Now, there&#39;s Minix 3, a FOSS microkernel with the NetBSD userland on top.

But Linux is too established.

Yes, NextBSD is a very interesting project. But basically, it&#39;s just fitting Apple userland services onto FreeBSD.

So, yes, interesting, but FreeBSD is a sideline. Linux is the real focus of attention. FreeBSD jails are over a decade old, but look at the fuss the world is making about Docker.

There is now too much legacy around Unix -- and especially Linux -- for any other Unix to get much traction.

We&#39;ve <b>had</b> Unix 2.0, then Unix 2.1, then a different, less radical, more conservative kind of Unix 2.0 in the form of microkernels. Simpler, cleaner, more modular, more reliable.

And everyone ignored it.

So we&#39;re stuck with the old one, and it won&#39;t go away until something totally different comes along to replace it altogether."
"eventtime","2016-02-18 13:58:00"
"url","http://liam-on-linux.livejournal.com/47262.html"
"userid","8744"
"itemid","183"
"event","Since it looks like my FB comment is about to get censored, I thought I&#39;d repost it...

-----

<span data-ft=""{&quot;tn&quot;:&quot;K&quot;}"">Gods, you are such a bunch of newbies! Only one comment out of 20&nbsp;knows the actual answer.

History lesson. Sit down and shaddup, ya dumb punks.

Early microcomputers did not have a single PCB with all the components on it. They were on separate cards, and all connected together via a bus. This was called a backplane and there were 2 types: active and passive. It didn&#39;t do anything except interconnect other components.

Then, with increasing integration, a main board with the main controller logic on it became common, but this had slots on it for other components that were too expensive to include. The pioneer was the Apple II, known affectionately as the Apple ][. The main board had the processor, RAM and glue logic. Cards provided facilities such as printer ports, an 80 column display, a disk controller and so on.

But unlike the older S100 bus and similar machines, these boards did nothing without the main board. So they were called daughter boards, and the one they plugged into was the motherboard.

Then came the Mac. This had no slots so there could be no daughterboards. Nothing plugged into it, not even RAM -- it accepted no expansions at all; therefore it made no sense to call it a motherboard.

It was not the only PCB in the computer, though. The original Mac, remember, had a 9&quot; mono CRT built in. An analogue display, it needed analogue electronics to control it. These were on the Analog Board (because Americans can&#39;t spell.)

The board with the digital electronics on it -- the bits that did the computing, in other words the logic -- was the Logic Board.

2 main boards, not one. But neither was primary, neither had other subboards. So, logic board and analog board.

And it&#39;s stuck. There are no expansion slots on any modern Mac. They&#39;re all logic boards, *not* motherboards because they have no children.

<a dir=""ltr"" href=""https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.ifixit.com%2FTeardown%2FMacintosh%2B128K%2BTeardown%2F21422&amp;h=aAQEieMVA"" rel=""nofollow"" target=""_blank"">https://www.ifixit.com/Teardown/Macintosh+128K+Teardown/21422</a></span>"
"eventtime","2016-02-05 18:51:00"
"url","http://liam-on-linux.livejournal.com/46934.html"
"userid","8744"
"itemid","182"
"event","<div>A friend of mine who is a Commodore enthusiast commented that if the company had handled it better, the Amiga would have killed the Apple Mac off.

<span data-ft=""{&quot;tn&quot;:&quot;K&quot;}"">But I wonder. I mean, the $10K Lisa (&#39;83) and the $2.5K Mac (&#39;84) may only have been a year or two before the $1.3K Amiga 1000 (&#39;85), but in those years, chip prices were plummeting -- maybe rapidly enough to account for the discrepancy.

The 256kB Amiga 1000 was half the price of the original 128kB Mac a year earlier.

Could Tramiel&#39;s Commodore have sold Macs at a profit for much less? I&#39;m not sure. Later, yes, but then, Mac prices fell, and anyway, Apple has long been a premium-products-only sort of company. But the R&amp;D process behind the Lisa &amp; the Mac was long, complex &amp; expensive. (Yes, true, it was behind the Amiga chipset, too, but less so on the OS -- the original CAOS got axed, remember. The TRIPOS thing was a last-minute stand-in, as was Arthur/RISC OS on the <a class=""profileLink"" data-hovercard=""/ajax/hovercard/hovercard.php?id=45424713940&amp;extragetparams=%7B%22hc_location%22%3A%22ufi%22%7D"" dir=""ltr"" href=""https://www.facebook.com/Acorn-Archimedes-45424713940/?hc_location=ufi"" target=""_blank"">Acorn Archimedes</a>.)

The existence of the Amiga also pushed development of the Mac II, the first colour model. (Although I think it probably more directly prompted the Apple ][GS.)

It&#39;s <b>much</b> easier to copy something that someone else has already done. Without the precedent of the Lisa, the Mac would have been a much more limited 8-bit machine with a 6809. Without the precedent of the Mac, the Amiga would have been a games console.</span> 

I think the contrast between the Atari ST and the Sinclair QL, in terms of business decisions, product focus and so on, is more instructive.</div><div>The QL could have been one of the imporant 2nd-generation home computers. It was launched a couple of weeks <i>before</i> the Mac.</div><div>But Sinclair went too far with its hallmark cost-cutting on the project, and the launch date was too ambitious. The result was a 16-bit machine that was barely more capable than an 8-bit one from the previous generation. Most of the later 8-bit machines had <i>better</i> graphics and sound; some (Memotech, Elan Enterprise) as much RAM, and some (e.g. the <span style=""line-height: 19.6px;"">SAM Coup&eacute;) </span><span style=""line-height: 19.6px;"">also supported built-in mass storage.</span></div><div>But Sinclair&#39;s OS, QDOS, was impressive. An excellent BASIC, front &amp; centre like an 8-bit machine, but also full multitasking, modularity so it readily handled new peripherals -- but no GUI by default.</div><div>The Mac, similarly RAM deprived and with even poorer graphics, blew it away. Also, with the Lisa and the Mac, Apple had spotted that the future lay in GUIs, which Sinclair had missed -- the QL didn&#39;t get its &quot;pointer environment&quot; until later, and when it did, it was primitive-looking. Even the modern version is:

<img alt="""" src=""http://www.ql-qvd.com/images/launchpaddesktop.jpg"" width=""600"" />

Atari, entering the game a year or so later, had a much better idea where to spend the money. The ST was an excellent demonstration of cost-cutting. Unlike the bespoke custom chipsets of the Mac and the Amiga, or Sinclair&#39;s manic focus on cheapness, Atari took off-the-shelf hardware and off-the-shelf software and assembled something that was good enough. A decent GUI, an OS that worked well in 512kB, graphics and sound that were good enough. Marginally faster CPU than an Amiga, and a floppy format interchangeable with PCs.</div><div>Yes, the Amiga was a better machine in almost every way, but the ST was good enough, and at first, significantly cheaper. Commodore had to cost-trim the Amiga to match, and the first result, the Amiga 500, was a good games machine but too compromised for much else. </div><div>
The QL was built down to a price, and suffered for it. Later replacement motherboards and third-party clones such as the Thor fixed much of this, but it was no match for the GUI-based machines. 

The Mac was in some ways a sort of cut-down Lisa, trying to get that ten-thousand-dollar machine down to a more affordable quarter of the <span style=""line-height: 19.6px;"">price. Sadly, this meant losing the hard disk and the innovative multitasking OS, which were added back later in compromised form -- the latter cursed the classic MacOS until it was replaced with Mac OS X at the turn of the century.</span>
<span style=""line-height: 1.4;""> 
The Amiga was a no-compromise games machine, later cleverly shoehorned into the role of a very capable multimedia GUI coomputer.</span>
</div><div>The ST was also built down to a price, but learned from the lessons of the Mac. Its spec wasn&#39;t as good as the Amiga, its OS wasn&#39;t as elegant as the Mac, but it was good enough.

The result was that games developers aimed at both, limiting the quality of Amiga games to the capabilities of the ST. The Amiga wasn&#39;t differentiated enough -- yes, Commodore did high-end three-box versions, but the basic machines remained too low-spec. The third-generation Amiga 1200 had a faster 68020 chip which the OS didn&#39;t really utilise, it had provision for a built-in hard disk which was an optional extra. AmigaOS was a pain to use with only floppies, like the Mac -- whereas the ST&#39;s ROM-based OS was fairly usable with a single drive. A dual-floppy-drive Amiga was the minimum usable spec, really, and it benefited hugely from a hard disk -- but Commodore didn&#39;t fit one.

The ST killed the Amiga, in effect. By providing an experience that was nearly as good in the important, visible ways, Commodore had to price-cut the Amiga to keep it competitive, hobbling the lower-end models. And as games were written to be portable between them both without too much work, they mostly didn&#39;t exploit the Amiga&#39;s superior abilities.

Acorn went its own way with the Archimedes -- it shared almost no apps or games with the mainstream machines, and while its OS is still around, it hasn&#39;t kept up with the times and is mainly a curiosity. Acorn kept its machines a bit higher-end, having affordable three-box models with hard disks right from the start, and focused on the educational niche where it was strong.

But Acorn&#39;s decision to go its own way was entirely vindicated -- its ARM chip is now the world&#39;s best-selling CPU. Both Microsoft and Apple OSes run on ARMs now. In a way, it won.

The poor Sinclair QL, of course, failed in the market and Amstrad killed it off when it was still young. But even so, it inspired a whole line of successors -- the CST Thor, the ICL One-Per-Desk (AKA Merlin Tonto, AKA <a href=""http://www.classic-computers.org.nz/collection/computerphone.htm"">Telecom Australia ComputerPhone</a>), the Qubbesoft Aurora replacement main board and later the Q40 and Q60 QL-compatible PC-style motherboards. It had the first ever multitasking OS for a home computer, QDOS, which evolved into SMSQ/e and moved over to the ST platform instead. It&#39;s now open source, too. 

And Linus Torvalds owned a QL, giving him a taste for multitasking so that he wrote his own multitasking OS when he got a PC. That, of course, was Linux.

The Amiga OS is still limping along, now running on a CPU line -- PowerPC -- that is also all but dead. The open-source version, AROS, is working on an ARM port, which might make it slightly more relevant, but it&#39;s hard to see a future or purpose for the two PowerPC versions, MorphOS and AmigaOS 4.

The ST OS also evolved, into a rich multitasking app environment for PCs and Macs (MagiC) and into a rich multitasking FOSS version, AFROS, running on an emulator on the PC, Aranym. A great and very clever little project but which went nowhere, as did PC GEM, sadly.

All of these clever OSes -- AROS, AFROS, QDOS AKA SMSQ/E. All went FOSS too late and are forgotten. Me, I&#39;d love Raspberry Pi versions of any and all of them to play with!

In its final death throes, a flailing Atari even embraced the Transputer. The Atari ABAQ could run Parhelion&#39;s HELIOS, another interesting long-dead OS. Acorn&#39;s machines ran one of the most amazing OSes I&#39;ve ever seen, <a href=""http://www.uruk.org/emu/Taos.html"">TAOS</a>, which <a href=""http://www.osnews.com/story/157/Tao_Group_on_ElateOS_AmigaDE_and_More""><i>nearly</i></a> became the <a href=""http://www.eetimes.com/document.asp?doc_id=1141056"">next-generation Amiga OS</a>. That could have shaken up the industry -- it was truly radical.

And in a funny little side-note, the next next-gen Amiga OS after TAOS <a href=""http://www.trollaxor.com/2005/06/how-qnx-failed-amiga.html"">was to be QNX</a>. It <a href=""http://www.theregister.co.uk/1999/07/09/qnx_developer_pleas_for_amiga/"">didn&#39;t happen</a>, but QNX added a GUI and rich multimedia support to its embedded microkernel OS for the deal. That OS is now what powers my Blackberry Passport smartphone. Blackberry 10 is now all but dead -- Blackberry has conceded the inevitable and gone Android -- but BB10 is a beautiful piece of work, way better than its rivals. 

<span style=""line-height: 19.6px;"">But all the successful machines that sold well? </span><span style=""line-height: 19.6px;"">The ST and Amiga lines are effectively dead. </span><span style=""line-height: 19.6px;"">The Motorola 68K processor line they used is all but dead, too. So is its successor, PowerPC.</span>

<span style=""line-height: 19.6px;"">So it&#39;s the two niche machines that left the real legacy. In a way, Sinclair Research <i>did</i> have the right idea after all -- but prematurely. It thought that the justification for 16-bit home/business computers was multitasking. In the end, it was, but only in the later 32-bit era: the defining characteristic of the 16-bit era was bringing the GUI to the masses. True robust multitasking for all followed later. Sinclair picked the wrong feature to emphasise -- even though the QL post-dated the Apple Lisa, so the writing was there on the wall for all to see.</span>

But in the end, the QL inspired Linux and the Archimedes gave us the ARM chip, the most successful RISC chip ever and the one that could still conceivably drive the last great CISC architecture, x86, into extinction. 

Funny how things turn out.</div>"
"eventtime","2016-01-30 19:37:00"
"url","http://liam-on-linux.livejournal.com/46833.html"
"userid","8744"
"itemid","181"
"event","I recently received an email from a reader -- a rare event in itself -- following my recent Reg article about educational OSes:

http://www.theregister.co.uk/2015/12/02/pi_versus_oberton/

They asked for more info about the OS. So, since there&#39;s not a lot of this about, here is some more info about the Oberon programming language, the Oberon operating system written in it, and the modern version, AOS.

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">The homepage for the FPGA OberonStation went down for a while. Perhaps it was the interest driven by my article. ;-)</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">It is back up again now, though:</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">http://oberonstation.x10.mx/</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">The Wikipedia page is a good first source for info on the 2 Oberons, the OS:</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">https://en.wikipedia.org/wiki/Oberon_(operating_system)</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">... and the programming language:</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">https://en.wikipedia.org/wiki/Oberon_(programming_language)</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">Prof. Wirth worked at ETH Zurich, which has a microsite about the Oberon project:</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">http://www.oberon.ethz.ch/</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">There is a native port for x86 PCs. I have this running under VirtualBox.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">http://www.oberon.ethz.ch/archives/systemsarchive/native_new</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">There&#39;s a good overview here:</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">http://ignorethecode.net/blog/2009/04/22/oberon/</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">And the Oberon book is online here:</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">http://www.projectoberon.com/</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">Development did not stop on the OS after Prof Wirth retired. It continued and became AOS. This has a different type of GUI called a Zooming UI. The AOS zooming UI is called &quot;Bluebottle&quot; and sometimes newer versions of the OS are thus referred to as Bluebottle.</span><div style=""font-family: arial, sans-serif; font-size: small; line-height: normal;"">
There is a sort of fan page dedicated to AOS here:

http://sage.com.ua/en.shtml?e1l0</div>"
"eventtime","2016-01-09 16:42:00"
"url","http://liam-on-linux.livejournal.com/46523.html"
"userid","8744"
"itemid","180"
"event","So a regular long-term member of one of the Ubuntu lists is saying that they don&#39;t trust Google to respect their privacy. This from someone who runs Opera 12 (on Ubuntu with Unity) because they had not noticed it had been updated... for <b>three years</b>.

I realise that I could have put this better, but...

As is my wont, I offered one of my favourite quotes:

Scott McNeally, CEO and co-founder of Sun Microsystems, said it best.

He was put on a panel on internet security and privacy, about 20y ago.

Eventually, they asked the silent McNeally to say something.

He replied:

&quot;You <b>have</b> no privacy on the Internet. Get over it.&quot;

He was right then and he&#39;s right now. It&#39;s a public place. It&#39;s what&nbsp;it&#39;s for. Communication, sharing. Deal with it.

Run current software, follow best-practice guidelines from the like of <a href=""https://twitter.com/swiftonsecurity"" target=""_blank"">SwiftOnSecurity on Twitter</a>, but don&#39;t be obsessive about it, because&nbsp;it is totally pointless.

You CANNOT keep everything you do private and secure and also use the&nbsp;21st century&#39;s greatest communications tool.

So you choose. Use the Internet, and stop panicking, or get off it and&nbsp;stay off it.

Your choice.

Modern OSes and apps do &quot;phone home&quot; about what you&#39;re doing, yes, sure.

This does not make them spyware.

http://www.zdnet.com/article/revealed-the-crucial-detail-that-windows-10-privacy-critics-are-missing/?tag=nl.e539&amp;s_cid=e539&amp;ttag=e539&amp;ftag=TRE17cfd61

You want better software? You want things that are more reliable, more&nbsp;helpful, more informative?

Yes?

Then stop complaining and get on with life.

No? You want something secure, private, that you can trust, that you&nbsp;know will not report anything to anyone?

Then go flash some open-source firmware onto an old Thinkpad and run <a href=""http://www.openbsd.org/"" target=""_blank"">OpenBSD</a> on it.

There are ways of doing this, but they are hard, they are a <b>lot</b> more&nbsp;work, and you will have a significantly degraded experience with a lot&nbsp;of very handy facilities lost.

That is the price of privacy.

And, listen, I am sorry if this is not what you want to hear, but if&nbsp;you are not technically literate enough to notice that you&#39;re running&nbsp;a browser that has been out of date for 3 years, then I think that you&nbsp;are not currently capable of running a really secure environment. I am&nbsp;not being gratuitously rude here! I am merely pointing out facts that&nbsp;others will be too nervous to do.

You <i>cannot</i> run a mass-market OS like Windows 10, Mac OS X or Ubuntu&nbsp;with Unity <b>and</b> have a totally secure private computer.

You can&#39;t. End of. It&#39;s over. These are not privacy-oriented platforms.

They do exist. Look at OpenBSD. Look at <a href=""https://www.qubes-os.org/"" target=""_blank"">Qubes OS</a>.

But they are <b>hard work</b> and need immense technical skill -- more than&nbsp;I have, for instance, after doing this stuff for a living for nearly&nbsp;30y. And even then, you get a much poorer experience, like a faster&nbsp;1980s computer or something.

As it is, after being on my CIX address for 25 years and my Gmail address for 12, all my email goes through Gmail now -- the old&nbsp;address, the Hotmail and Yahoo spamtraps, all of them. I get all my&nbsp;email, contacts and diary, all in one place, on my Mac and on both my&nbsp;Linux laptops and on both my Android and Blackberry smartphones. It&#39;s&nbsp;wonderful. Convenient, friendly, powerful, free, cross-platform and&nbsp;based on FOSS and compatible with FOSS tools.

But it means I must trust Google to store everything.

I am willing to pay that price, for such powerful tools for no money.

I am a trained Microsoft Exchange admin. I could do similar with&nbsp;Office 365, but I&#39;ve used it, and it&#39;s less cross-platform, it&#39;s less&nbsp;reliable, it&#39;s slower, the native client tools are vastly inferior and&nbsp;it costs money.

Nothing much else could do this unless I hosted my own, which I am&nbsp;technically competent to do but would involve a huge amount of work,&nbsp;spending money <b>and </b>still trusting my hosting provider.

You have a simple choice. Power and convenience and ease, or, learning&nbsp;a lot more tech skills and privacy but also inconvenience, loss of&nbsp;flexibility and capability and simplicity.

You run a closed-source commercial browser on what [another poster] correctly&nbsp;points out is the least-private Linux distro that there is.

You have <b>already made the choice</b>.

So please, stop complaining about it. You chose. You are free to&nbsp;change your mind, but if you do, off to OpenBSD you go. Better start&nbsp;learning shell script and building from source."
"eventtime","2016-01-07 16:18:00"
"url","http://liam-on-linux.livejournal.com/46119.html"
"userid","8744"
"itemid","179"
"event","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"
"eventtime","2015-12-06 13:11:00"
"url","http://liam-on-linux.livejournal.com/46017.html"
"userid","8744"
"itemid","178"
"event","Not to blow my own horn, but, er, well, OK, yes I am. Tootle-toot.

I'm back writing for <a href=""http://www.theregister.co.uk/"" target=""_blank"">the Register</a> again, in case you hadn't noticed. A piece a month for the last 2 months.

You might enjoy these:

Old, not obsolete: IBM takes Linux mainframes back to the future
Your KVMs, give them to me
http://www.theregister.co.uk/2015/11/02/ibm_linux_mainframes/

From Zero to hero: Why mini 'puter Oberon should grab Pi's crown
It's more kid-friendly... No really
http://www.theregister.co.uk/2015/12/02/pi_versus_oberton/"
"eventtime","2015-12-04 18:40:00"
"url","http://liam-on-linux.livejournal.com/45735.html"
"userid","8744"
"itemid","177"
"event","As far as I can recall, I didn't plug this at the time, as this series of 5 articles for the Register was later collated into my first-ever book - a Kindle ebook of the same title: http://bit.ly/trabhov

However, that was about 4 years ago now and as it's one of the few times in my tech career that I have accurately predicted a future technology trend -- i.e., containers -- I think it's time.

You can buy the ebook here if you would like to support my work -- no, I don't get royalties, but it will endear me to the Reg:

http://www.theregister.co.uk/2011/12/19/short_history_of_virtualisation/

Or, if you're a cheapskate and just want to read the content for free, then here are the component articles:

http://www.theregister.co.uk/2011/07/11/a_brief_history_of_virtualisation_part_one/

http://www.theregister.co.uk/2011/07/14/brief_history_of_virtualisation_part_2/

http://www.theregister.co.uk/2011/07/18/brief_history_of_virtualisation_part_3/

http://www.theregister.co.uk/2011/07/21/brief_history_of_virtualisation_part_4/

http://www.theregister.co.uk/2011/07/25/brief_history_of_virtualisation_part_5/

(Part 3 is the one about containers)

Enjoy. Buy a copy for all your friend, it's the ideal holiday gift!"
"eventtime","2015-10-27 14:38:00"
"url","http://liam-on-linux.livejournal.com/45467.html"
"userid","8744"
"itemid","176"
"event","(The title is a parody of http://www.dreamsongs.com/WIB.html )

Even today, people still rail against the horrors of BASIC, as per Edsger Dijkstra&#39;s famous comment about it brain-damaging beginner programmers beyond any hope of redemption:

https://reprog.wordpress.com/2010/03/09/where-dijkstra-went-wrong-the-value-of-basic-as-a-first-programming-language/

I rather feel that this is due to perceptions of some of the really crap early 8-bit BASICs, and wouldn&#39;t have applied if students learned, say, BBC BASIC or one of the other better dialects.

For example, Commodore&#39;s pathetically-limited BASIC as supplied on the most successful home computer ever, the Commodore 64, in 1982. Despite its horrors, it&#39;s remembered fondly by many. There&#39;s even a modern FOSS re-implementation of it!

https://github.com/mist64/cbmbasic

I&#39;ve long been puzzled as to exactly why the Commodore 64 shipped with such a terrible, limited, primitive BASIC in its ROM: CBM BASIC 2.0, essentially the 6502 version of Microsoft&#39;s MS-BASIC. It wasn&#39;t done for space reasons -- the original Microsoft BASIC fitted into 4kB of ROM and a later version into 8kB:

http://www.emsps.com/oldtools/msbasv.htm

Acorn&#39;s BBC BASIC (first released a year earlier, in 1981) was a vastly better dialect.

AFAIK all the ROMable versions of BBC BASIC (BASIC I to BASIC 4.62) fitted into a 16kB ROM, so in terms of space, it was doable.

http://mdfs.net/Software/BBCBasic/Versions

IOW, CBM had enough room; the C64 kernal+BASIC were essentially those of the original PET, and fitted into an 8kB ROM, I think. And the C64 shipped <i>after</i> the B and P series machines, the CBM-II. OK, CBM BASIC 4 wasn&rsquo;t <i>much</i> of an improvement, but it was better.

Looking back years later, and reading stuff like Cameron Kaiser&rsquo;s &ldquo;Secret Weapons of Commodore&rdquo; site:

http://www.floodgap.com/retrobits/ckb/secret/

&hellip; it seems to me that Commodore management never really had much of an idea of what they were doing. Unlike companies such as Sinclair or Acorn, labouring for years over tiny numbers of finely-honed models, in the 8-bit era, Commodore had multiple teams designing dozens of models of all sorts of kit, often conflicting with one another, and just occasionally chose to ship certain products and kill others &mdash; sometimes early, sometimes when it was nearly ready and the packaging was being designed.

(Apple was similar, but at a smaller scale &mdash; e.g. the Apple /// competing with the later Apple ][ machines, and the Mac competing with the Lisa, and then the Apple ][GS competing with the Mac.)

There were lovely devices that might have thrived, such as the C65, which were killed.

There were weird, mostly inexplicable hacked-together things, such as the C128, a bastard of a C64, plus a slightly-upgraded C64, plus, of all things, a CP/M micro based around an entirely different an totally incompatible processor, so the C128 had two: a 6502 derivative <b>and</b> a Z80. Bizarre.

There were determined efforts to enhance product lines whose times were past, such as the CBM-II machines, an enhanced PET when the IBM PC was already taking over.

There were odd half-assed efforts to fix problems with released products, such as the C16 and Plus-4, which clearly showed that management didn&rsquo;t understand their own successes: the C64 was an wildly-successful upgrade of the popular VIC-20, but rather than learn from that and do it again, Commodore did something totally different and incompatible, launched with some fanfare, and appeared mystified that it bombed.

It&rsquo;s a very strange story of a very schizophrenic company.

And of course, rather than develop their own successor for the 16-bit era, they bought it in &mdash; the Lorraine, later the Amiga, a spiritual successor to the Atari 8-bit machines, which themselves were inspired kit for their time.

This leaving Atari in the lurch, but to which the company responded in an inspired way with the ST: an clever mixture of off-the-shelf parts -- PC-type where that was good enough (e.g. graphics controller), or from the previous generation of 8-bits (e.g. sound chip), plus a bought-in adapted OS (Digital Research&#39;s GEMDOS plus GEM, never crippled like the PC version was due to Apple&#39;s lawsuit, meaning PC disk formats and file compatibility. And of course the brilliant inclusion of MIDI ports, foreseeing an entire industry that was around the corner.

The ST is what the Sinclair QL should have been: a cheap, affordable, usable 16-bit computer. Whereas the poor doomed QL was Sinclair doing its trademark thing too far: a 16-bit machine cut down to the point that it was no better than a decent 8-bit machine.

Interesting times.

Whereas now, almost all the diversity is gone. Today, we just have generic x86 boxes and occasional weird little ARM things, and apart from some research or hobbyist toys, just 2 OS families -- Windows NT or some flavour of Unix."
"eventtime","2015-09-23 16:34:00"
"url","http://liam-on-linux.livejournal.com/45126.html"
"userid","8744"
"itemid","175"
"event","There are moves afoot to implement desktop apps inside containers on Linux -- e.g.

https://wiki.gnome.org/Projects/SandboxedApps/Sandbox

This is connected with the current uptake of Docker. There seems to be a lot of misunderstanding about Docker, exemplified by a mailing list post I just read which proposes running different apps in different user accounts instead and accessing them via VNC. This is an adaptation of my reply.

Corrections welcomed!

Docker is a kind of standardized container for Linux. 

Containers are a sort of virtual machine.

Current VMs are PC emulators for the PC: they virtualise the PC's hardware, so you can run multiple OSes at once on one PC.

This is useful if you want to run, say, 3 different Linux distros, Windows and Solaris on the same machine at once.

If you run lots of copies of the same OS, it is very inefficient, as you duplicate lots of code.

Containers virtualise the OS instead of the computer. 1 OS instance, 1 kernel, but to the apps running on that OS, each app has its own OS. Apps cannot see other apps at all. The virtualisation means that each app thinks it is running standalone on the OS, with nothing else installed.

This means that you can, say, run 200 instances of Apache on 1 instance of Linux, and they are all isolated. If one crashes, the others don't. You can mix versions, have custom modules in one that the others don't have, etc.

All without the overhead of running 200 copies of the OS.

Containerising apps is a security measure. It means that if, say, you have a compromised version of LibreOffice that contains an exploit allowing an attacker to get root, they get root in the container, and as far as they can see, the copy of LibreOffice is the only thing on the computer. No browser, no email, no stored passwords, nothing. 

All within 1 user account, so that this can be done for multiple users, side-by-side, even concurrently on a multiuser host.

It is nothing to do with user accounts; these are irrelevant to it.

<a href=""http://www.gobolinux.org/"">Gobo</a>'s approach to bundling apps mainly just brings benefits to the user: an easier-to-understand filesystem hierarchy, and apps that are self-contained not spread out all over the filesystem. Nice, but not a killer advantage. There's no big <i>technical</i> advantage and it breaks lots of things, which is why Gobo needs the gobohide kernel extension and so on. It's also why Gobo has not really caught on.

But now, containers are becoming popular on servers. It's relatively easy to isolate server apps: they have no GUI and often don't interact much with other apps on the server.

Desktop apps are much harder to containerise. However, containerising them brings lots of other advantages -- it could effectively eliminate the differences between Linux distributions, forever ending the APT-vs-RPM wars by making the packaging irrelevant, while delivering much improved security, granularity, simplicity and more.

In theory all Gobo's benefits <i>at the app level</i> (the OS underneath is the same old mess) plus many more.

It looks like it might be something that will happen. It will have some side-effects -- reducing the ease of interapp communication, for instance. It might break sound mixing, or inter-app copy-and-paste, system browser/email/calender integration and some other things.

And systems will need a lot more hard disk space.

But possibly worth it overall.

One snag at present is that current efforts look to require btrfs, and btrfs is neither mature nor popular at the moment. This might mean that we get new filesystems with the features such sandboxing would need -- maybe there'll be a new ext5 FS, or maybe Bcachefs will fit the bill. It's early days, but the promise looks good."
"eventtime","2015-09-05 14:20:00"
"url","http://liam-on-linux.livejournal.com/44800.html"
"userid","8744"
"itemid","174"
"event","I was just prodded by someone when suggesting that some friends try Linux. I forgot to mention that you can try it without risking your existing PC setup. It prompted me to write this...

I forget that non-techies don&#39;t _know_ stuff like that.

Download a program called VirtualBox. It&#39;s free and it lets you run a whole other operating system - e.g. Linux - under Windows as a program. So you can try it out without affecting your real computer.

https://www.virtualbox.org/

If all you know is Windows, I&#39;d suggest Linux Mint: http://www.linuxmint.com/

It has a desktop that looks and works similarly to Windows&#39; classic pre-Win8 look &amp; feel.

Google for the steps but here&#39;s the basic instructions:

[1] Download and install VirtualBox

[2] Then download the Virtualbox Extensions from the same site. Double-click the extensions file to install it into Vbox. (They have to do it this way for copyright reasons.)

[3] Download Mint. It comes as an ISO file, an image of a DVD.

[4] Make a new VM in VBox. Give it 2-3 gig of RAM. Enable display 3D acceleration in the settings. (Remember, anything you don&#39;t know how to do, Google it.) Leave all the other settings as they are.

[5] Start your new VM. It will ask for an ISO file. Point it at the ISO file of Mint you downloaded.

[6] It will boot and run. Install it onto the virtual hard disk inside Vbox. Just accept all the defaults.

[7] Reboot your new Mint VM.

[8] Install the Vbox Guess Additions. On the VBox Device menu, choose &ldquo;Insert Guest Additions ISO&rdquo;. Google for instruction on how to install them.

[9] When it&rsquo;s finished, reboot the VM.

[10] Update your new copy of Linux Mint. (Remember, Google for instructions.)

That&rsquo;s it. Play with it. See if you can do the stuff you normally do on Windows all right. If you can&rsquo;t, Google for what program to use and how to install it. It&rsquo;s not as quick as a real PC but it works.

Don&rsquo;t assume that because you know how to do something on Windows, it works that way on Linux. E.g. you never should download programs from a website and install them into Linux &mdash; it has a better way. Be prepared to learn some stuff.

If you can work it, then you can install it on your PC alongside Windows. This is called Dual Booting. It&rsquo;s quite easy really and then you choose whether you want Windows or Linux when you turn it on.

All my PCs do it, but I use Windows about once or twice a year, when I absolutely need it. Which is almost never. I only use Windows if someone is paying me too &mdash; it is a massive pain to maintain and keep running properly compared to more grown-up equivalents. (Linux and Mac OS X are based on a late-1960s project; they are very mature and polished. The first version of the current Windows family is from 1993. It&rsquo;s still got a lot of growing up to do &mdash; it&rsquo;s only half the age.)

It&rsquo;s genuinely better. No, you don&rsquo;t get all the Windows programs. There aren&rsquo;t many games for it, for instance. But it can do anything Windows can do, it&rsquo;s faster, it&rsquo;s immune to all the Windows viruses and nasties so you don&rsquo;t need antivirus or a firewall or anything. That means it&rsquo;s faster, too &mdash; antivirus slows computers down, but you need it on Windows.

All the apps are free. All the updates are free, forever. There are thousands of people on web fora who will help you if you have problems, you just have to ask. It&rsquo;s educational &mdash; you will learn more about computers from learning a different way to use them, but that means you won&rsquo;t be so helpless. You don&rsquo;t need to be a white-coated genius scientist, but what it means is you take control back from some faceless corporation. Remember, the world&rsquo;s richest man got that way by selling people stuff they could have had for free if they just knew how."
"eventtime","2015-08-22 18:01:00"
"url","http://liam-on-linux.livejournal.com/44544.html"
"userid","8744"
"itemid","173"
"event","I have ruffled many feathers with my position that the touch-driven computing sector is growing so fast that it&#39;s going to subsume the old WIMP model completely. I don&#39;t mean that iPads will replace Windows PCs, but that the descendants of the PC will look and act more like tablets than today&#39;s desktops and laptops.

But where is it leading, beyond that point? I have a<span style=""line-height: 19.6000003814697px;"">bsolutely no concrete idea</span>. But t<span style=""line-height: 1.4;"">he end point? I&#39;ve read one brilliant model.</span><p class=""p2""><span style=""line-height: 1.4;"">It&#39;s in one of the later </span><i style=""line-height: 1.4;"">Foundation</i><span style=""line-height: 1.4;""> books by Isaac Asimov, IIRC. (Not a </span><span style=""line-height: 1.4;"">series I&#39;m that enamoured of, actually.)</span>

<span style=""line-height: 1.4;"">A guy gets (steals?) a space yacht: a small, 1-man starship. (Set aside </span><span style=""line-height: 1.4;"">the plausibility of this.)</span>

<span style=""line-height: 1.4;"">He searches the ship&#39;s crew quarters. In its few luxury rooms, there is no </span><span style=""line-height: 1.4;"">cockpit. No controls, no instruments, nothing. He is bemused.</span>

<span style=""line-height: 1.4;"">He returns to the comfiest room, the main stateroom, i.e. cabin/bedroom. </span><span style=""line-height: 1.4;"">In it there is a large, bare dressing table with a comfy seat in front of </span><span style=""line-height: 1.4;"">it. He sits.</span>

<span style=""line-height: 1.4;"">Two handprints appear, projected on the surface of the desk, shaped in </span><span style=""line-height: 1.4;"">light.</span>

<span style=""line-height: 1.4;"">He studies them. They&#39;re just hand-shaped spots of light. He puts his </span><span style=""line-height: 1.4;"">hands on them.</span>

<span style=""line-height: 1.4;"">And suddenly, he is much smarter. He knows the ship&#39;s position and speed </span><span style=""line-height: 1.4;"">in space. He knows where all the nearby planetary bodies are, their </span><span style=""line-height: 1.4;"">gravity wells, the speeds needed to reach them and enter orbit.</span>

<span style=""line-height: 1.4;"">Thinking of the greater galaxy, he knows where all the nearby stars are, </span><span style=""line-height: 1.4;"">their masses, their luminosities, their planetary systems. Merely thinking </span><span style=""line-height: 1.4;"">of a planet, he knows its cities, ports, where to orbit it, etc.</span>

<span style=""line-height: 1.4;"">All this knowledge is there in his mind if he wants it; if he allows his </span><span style=""line-height: 1.4;"">attention to move elsewhere, it&#39;s gone.</span>

<span style=""line-height: 1.4;"">He sits back, shocked. His hands lift from the prints on the desk, and it </span><span style=""line-height: 1.4;"">all disappears.</span>

<span style=""line-height: 1.4;"">That is the ultimate UI. One you don&#39;t know is there.</span>

<span style=""line-height: 1.4;"">Any UI where there are metaphors and abstractions and controls you must </span><span style=""line-height: 1.4;"">operate is inferior; direct interaction is better. We&#39;ve moved from text </span><span style=""line-height: 1.4;"">views of marked-up files with arcane names in folder hierarchies to today: </span><span style=""line-height: 1.4;"">hi-res, full-colour, moving images of fully-formatted documents and </span><span style=""line-height: 1.4;"">images. That&#39;s great.</span>

<span style=""line-height: 1.4;"">Some people are happily directly manipulating these &mdash; drawing and stroking </span><span style=""line-height: 1.4;"">screens with all their fingers, interacting naturally. Push up to see the </span><span style=""line-height: 1.4;"">bottom of a document, tap on items of interest. It&#39;s so natural </span><span style=""line-height: 1.4;"">pre-toddlers can do it.</span>

<span style=""line-height: 1.4;"">But many old hands still like their pointing hardware and little icons on </span><span style=""line-height: 1.4;"">screen that they can twiddle with their special pointing devices, and they s</span><span style=""line-height: 1.4;"">hout angrily that it&#39;s more </span><i style=""line-height: 1.4;"">precise</i><span style=""line-height: 1.4;""> and it&#39;s </span><i style=""line-height: 1.4;"">tried and tested</i><span style=""line-height: 1.4;""> and it </span><i style=""line-height: 1.4;"">works</i><span style=""line-height: 1.4;"">.</span>

<span style=""line-height: 1.4;"">Show them something better, no, it&#39;s a toy. OK for idly surfing the web, </span><span style=""line-height: 1.4;"">or reading, or watching movies, but no substitute for the &quot;real thing&quot;.</span>

<span style=""line-height: 1.4;"">It&#39;s a toy and the mere idea that these early versions could in time grow </span><span style=""line-height: 1.4;"">into something that could replace their 4-box Real Computer of System </span><span style=""line-height: 1.4;"">Unit, Monitor, Mouse and Keyboard is a nonsensical piece of idiocy.</span>

<span style=""line-height: 1.4;"">Which is exactly what their former bosses and their tutors said about the </span><span style=""line-height: 1.4;"">Mac&#39;s UI 30y ago. It&#39;s doubtless what they said about the tinker-toy CP/M </span><span style=""line-height: 1.4;"">boxes a decade before that, and so on.</span></p><p class=""p2""></p><p class=""p1"">I&#39;m guilty too. I am using a 25y old keyboard on my tiny silent <span style=""line-height: 1.4;"">near-unexpandable 2011 Mac mini, attached via a convertor that cost more </span><span style=""line-height: 1.4;"">than the keyboard and about a third as much as the Mac itself. I don&#39;t </span><span style=""line-height: 1.4;"">have a tablet; I don&#39;t personally like them much. I like my phablet, </span><span style=""line-height: 1.4;"">though. I gave away my Magic Trackpad - I didn&#39;t like it.</span>

<span style=""line-height: 1.4;"">(And boy did my friends in the FOSS community curse me out for buying a </span><span style=""line-height: 1.4;"">Mac. I&#39;m a traitor and a coward, apparently.)</span>

<span style=""line-height: 1.4;"">But although </span><i style=""line-height: 1.4;"">I personally</i><span style=""line-height: 1.4;""> don&#39;t want this stuff, nonetheless, I think </span><span style=""line-height: 1.4;"">it&#39;s where we&#39;re going. </span>

<span style=""line-height: 1.4;"">If adding more layers of abstraction to the system means we can remove </span><span style=""line-height: 1.4;"">layers of abstraction from the human-computer interface, then I&#39;m all for </span><span style=""line-height: 1.4;"">it. The more we can remove, the simpler and easier and clearer the </span><span style=""line-height: 1.4;"">computers we can make, the better. And if we can make them really small </span><span style=""line-height: 1.4;"">and cheap and thus give one to every child in the poorer countries of the </span><span style=""line-height: 1.4;"">world &mdash; I&#39;d be delighted. </span>

<span style=""line-height: 1.4;"">If price was putting Microsoft and Apple out of business and destroying </span><span style=""line-height: 1.4;"">the career of everyone working with Windows, and replacing it all with </span><span style=""line-height: 1.4;"">that nasty cancerous GPL and Big-Brother-like services like Google &mdash; still </span><span style=""line-height: 1.4;"">worth it.</span></p>"
"eventtime","2015-06-11 13:12:00"
"url","http://liam-on-linux.livejournal.com/44448.html"
"userid","8744"
"itemid","172"
"event","(Repurposed CIX post.)

Don&rsquo;t get me wrong. I like Apple kit. I am typing right now on an original 1990 Apple Extended II keyboard, attached via a ABD-USB convertor to a Core i5 Mac mini from 2011, running Mac OS X 10.10. It&rsquo;s a very pleasant computer to work on.

But, to give an example of the issues &mdash; I also have an iPhone. It&rsquo;s my spare smartphone with my old UK SIM in it.

But it&rsquo;s an iPhone 4. Not a lot of RAM, under clocked CPU, and of course not upgradable.

So I&rsquo;ve kept it on iOS 6, because I already find it annoyingly slow and iOS 7 would cause a reported 15-25% or more slowdown. And that&rsquo;s the latest it will run.

Which means that [a] I can&rsquo;t use lots of iPhone apps as they no longer support iOS 6.x and [b] it doesn&rsquo;t do any of the cool integration with my Mac, because my Mac needs a phone running iOS 8 to do clever CTI stuff.

My old 3GS I upgraded from iOS 4 to 5 to 6, and regretted it. It got slower &amp; slower and Apple being Apple, *you can&rsquo;t go back*.

Apple kit is computers simplified for non-computery people. Stuff you take for granted with COTS PC kit just can&rsquo;t be done. Not everything &mdash; since the G3 era, they take ordinary generic RAM, hard disks, optical drives, etc. Graphics cards etc. can often be made to work; you can, with work, replace CPUs and runs OSes too modern to be supported.

But it takes work. If you don&rsquo;t want that, if you just max out the RAM, put a big disk in and live with it, then it&rsquo;s fine. I&rsquo;m old enough that I want a main computer that Just Works and gives me no grief and the Mac is all that and it cost me under &pound;150, used. The OS is of course freeware and so are almost all the apps I run &mdash; mostly FOSS.

I like FOSS software. I use Firefox, Adium, Thunderbird, LibreOffice, Calibre, VirtualBox and BOINC. I also have some closed-source freeware like Chrome, Dropbox, TextWrangler and Skype. I don&rsquo;t use Apple&rsquo;s browser, email client, chat client, text editor, productivity apps or anything. More or less only iTunes, really.

What this means is that I can use pretty much the same suite of apps on Linux, Mac and Windows, making switching between them seamless and painless. My main phone runs Android, my travelling laptop is a 2nd-hand Thinkpad with the latest Ubuntu LTS on it.

As such, many of the benefits of an all-Apple solution are not available to me &mdash; texting and making phone calls from the desktop, seamless handover of file editing from desktop to laptop to tablet, wireless transparent media sync between computers and phone, etc.

I choose not to use any of this stuff because I don&rsquo;t trust closed file formats and dislike vendor lock-in.

Additionally, I don&rsquo;t like Apple&rsquo;s modern keyboards and trackpads, and I like portable devices where I can change the battery or upgrade the storage. So I don&rsquo;t use Apple laptops and phones and don&rsquo;t own a tablet. iPads are just big iPhones and I don&rsquo;t like iPhones much anyway. The apps are too constrained, I hate typing on a touchscreen &ldquo;keyboard&rdquo; and I don&rsquo;t like reading book-length texts from a brightly-glowing screen &mdash; I have a large-screen (A4) Kindle for ebooks. (Used off eBay, natch.) TBH I&rsquo;d quite like a backlight on it but the big-screen model doesn&rsquo;t offer one.

But I don&rsquo;t get that with Ubuntu. I never used UbuntuOne; I don&rsquo;t buy digital content at all, from anyone; my Apple account is around 20 years old and has no payment method set up on it. I have no lock-in to Apple and Ubuntu doesn&rsquo;t try to foist it on me.

With Ubuntu, *I* choose the laptop and I can (and did) build my own desktops, or more often, use salvaged freebies. My choice of keyboard and mouse, etc. I mean, sure, the Retina iMac is lovely, but it costs more than I&rsquo;m willing to spend on a computer.

Android is&hellip; all right. It&rsquo;s flakey but it&rsquo;s cheap, customisable (I&rsquo;ve replaced web browser, keyboard, launcher and email app, something Apple does not readily permit without drastic limitations) and it works well enough.

But it&rsquo;s got bloatware, tons of vendor-specific extensions and it&rsquo;s not quick.

Ubuntu is sleek as Linuxes go. I like the desktop. I turn off the web ads and choose my own default apps and it&rsquo;s perfectly happy to let me. I can remove the built-in ones if I want and it doesn&rsquo;t break anything.

If I could get a phone that ran Ubuntu, I&rsquo;d be very interested. And it might tempt me into buying a tablet.

I&rsquo;ve tried all the leading Linuxes (and most of the minor ones) and so long as you&rsquo;re happy with its desktop, Ubuntu is the best by a country mile. It&rsquo;s the most polished, best-integrated, it works well out of the box. I more or less trust them, as much as I trust any software vendor.

The Ubuntu touch offerings look good &mdash; the UI works well, the apps look promising, and they have a very good case for the same apps working well on phone and tablet, and the tablet becoming a usable desktop if you just plug a mouse in.

Here&rsquo;s a rather nice little 3min demo:
https://www.youtube.com/watch?v=c3PUYoa1c9M

Wireless mouse turned on: desktop mode, windows, title bars, menus, etc.
Turn it off, mid-session: it&rsquo;s a tablet, with touch controls. *With all the same same apps and docs still open.*
Mouse back on: it&rsquo;s in desktop mode again.

And there&rsquo;s integration &mdash; e.g. phone apps run full-size in a sidebar on a tablet screen, visible side-by-side with tablet apps.

Microsoft doesn&rsquo;t have this, Apple doesn&rsquo;t, Google doesn&rsquo;t.

It looks promising, it runs on COTS hardware and it&rsquo;s FOSS. What&rsquo;s not to like?

I suspect, when the whole plan comes together, that they will have a compelling desktop OS, a compelling phone OS and a compelling tablet OS, all working very well together but without any lock-in. That sounds good to me and far preferable to shelling out thousands on new kit to achieve the same on Apple&rsquo;s platform. Because C21 Apple is all about selling you hardware &mdash; new, and regularly replaced, too &mdash; and then selling you digital content to consume on it.

Ubuntu isn&rsquo;t. Ubuntu&rsquo;s original mission was to bring Linux up to the levels of ease and polish of commercial OSes.

It&rsquo;s done that.

Sadly, the world failed to beat a path to its door. It&rsquo;s the leading Linux and it&rsquo;s expanded the Linux market a little, but Apple beat it to market with a Unix that is easier, prettier and friendlier than Windows &mdash; and if you&rsquo;re willing to pay for it, Apple makes nicer hardware too.

But now we&rsquo;re hurtling into the post-desktop era. Apple is leading the way; Steve Jobs finally proved his point that he knew how to make a tablet that people wanted and Bill Gates didn&rsquo;t. Gates&rsquo; company still doesn&rsquo;t, even when it tries to embrace and extend the iPad type of device: millions of the original Surface tablets are destined for landfill like the Atari ET game and Apple Lisa. (N.B. *not* the totally different Surface Pro, but people use it as a lightweight laptop.)

But Apple isn&rsquo;t trying to make its touch devices replace desktops and laptops &mdash; it wants to sell both.

Ubuntu doesn&rsquo;t sell hardware at all. So it&rsquo;s trying to drag proper all-FOSS Linux kicking and screaming into the twenty-twenties: touch-driven *and* by desk-bound hardware-I/O, equally happy on ARM or x86-64, very shiny but still FOSS underneath.

The other big Linux vendors don&rsquo;t even understand what it&rsquo;s trying to do. SUSE does Linux servers for Microsoft shops; Red Hat sells millions of support contracts for VMs in expensive private clouds. Both are happy doing what they&rsquo;re doing.

Whereas Shuttleworth is spending his millions trying to bring FOSS to the masses.

OK, what Elon Musk is doing is much much cooler, but Shuttleworth&rsquo;s efforts are not trivial."
"eventtime","2015-04-28 16:07:00"
"url","http://liam-on-linux.livejournal.com/44140.html"
"userid","8744"
"itemid","171"
"event","<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">They&#39;re a bit better in some ways. It&#39;s somewhat marginal now.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">OK. Position statement up front.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">Anyone who works in computers and only knows one platform is clueless. You <b>need </b>cross-platform knowledge and experience to actually be able to assess strengths, weaknesses, etc.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">Most people in IT this century only know Windows and have only known Windows. This means that the majority of the IT trade are, by definition, clueless. </span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">There is little real cross-platform experience any more, because so few platforms are left. Today, it&#39;s Windows NT or Unix, running on x86 or ARM. 2 families of OS, 2 families of processor. That is <b>not </b>diversity.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">So, only olde phartes, yeah like me, who remember the 1970s and 1980s when diversity in computing <b>meant </b>something, have any really useful insight. But the snag with asking olde phartes is we&#39;re jaded &amp; curmudgeonly &amp; hate everything.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">So, this being so...</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">The Mac&#39;s OS design is better and cleaner, but that&#39;s only to the extent of saying New York City&#39;s design is better and cleaner than London&#39;s. Neither is good, but one is marginally more logical and systematic than the other.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">The desktop is much simpler and cleaner and prettier.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">App installation and removal is easier and doesn&#39;t involve running untrusted binaries from 3rd parties, which is such a hallmark of Windows that Windows-only types think it is normal and natural and do not see if for the howling screaming horror abomination that it actually is. Indeed, put Windows types in front of Linux and they try to download and run binaries and whinge when it doesn&#39;t work. See comment about cluelessness above.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">(One of the few places where Linux is genuinely ahead -- far ahead -- today is software installation and removal.)</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">Mac apps are fewer in number but higher in quality.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">The Mac tradition of relative simplicity has been merged with the Unix philosophy of &quot;no news is good news&quot;. Macs don&#39;t tell you when things work. They only warn you when things <b>don&#39;t </b>work. This is a huge conceptual difference from the VMS/Windows philosophy, and so, typically, this goes totally unnoticed by Windows types.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">Go from a Mac to Windows and what you see is that Windows is <b>constantly</b> nagging you. <i>Update this. Update that. Ooh you&#39;ve plugged a device in. Ooh, you removed it. Hey it&#39;s back but on a different port, I need a new driver. Oh the network&#39;s gone. No hang on it&#39;s back. Hey, where&#39;s the printer? You have a printer! Did you know you have an HP printer? Would you like to buy HP ink? </i></span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">Macs don&#39;t do this. Occasionally it coughs discreetly and asks if you know that something bad happened.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">PC users are used to it and filter it out.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">Also, PC OSes and apps are all licensed and copy-protected. Everything has to be verified and approved. Macs just trust you, mostly.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">Both are reliable, mostly. Both just work now, mostly. Both rarely fail, try to recover fairly gracefully and don&#39;t throw cryptic blue-screens at you. That difference is gone.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">But because of Windows&#39; terrible design and the mistakes that the marketing lizards made the engineers put in, it&#39;s howlingly insecure, and vastly prone to malware. This is because it was implemented badly.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">Windows apologists -- see cluelessness -- think it&#39;s fine and it&#39;s just because it dominates the market. This is because they are clueless and don&#39;t know how things <i>should</i> be done. Ignore them. They are loud; some will whine about this. They are wrong but not bright enough to know it. Ignore them.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">You <i>need</i> antimalware on Windows. You don&#39;t on anything else. Antimalware makes computers slower. So, Windows is slower. Take a Windows PC, nuke it, put Linux on it and it feels a bit quicker.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);"">Only a bit &#39;cos Linux too is a vile mess of 1970s crap. If it still worked, you could put BeOS on it and discover,<i> holy shit wow lookit that, this thing is really fsckin&#39; fast and powerful</i>, but no modern OS lets you feel it. It&#39;s under 5GB of layered legacy crap.</span>

<span style=""font-family: arial, sans-serif; font-size: small; line-height: normal;"">(Another good example was RISC OS. Today, millions of people are playing with Raspberry Pis, a really crappy underpowered &pound;25 tiny computer that runs Linux very poorly. Raspberry Pis have ARM processors. The ARM processor&#39;s original native OS, RISC OS, still exists. Put RISC OS on a Raspberry Pi and suddenly it&#39;s a very fast, powerful, responsive computer. Swap the memory card for Linux and it crawls like a one-legged dog again. This is the difference between an efficient OS and an inefficient one. The snag is that RISC OS is horribly obsolete now so it&#39;s not much use, but it does demonstrate the efficiency of 1980s OSes compared to 1960s/1970s ones with a few decades of crap layered on top.)</span><div style=""font-family: arial, sans-serif; font-size: small; line-height: normal;"">
Windows <i>can</i> be sort of all right, if you don&#39;t expect much, are savvy, careful and smart, and really need some proprietary apps. 

If you just want the Interwebs and a bit of fun, it&#39;s a waste of time and effort, but Windows people think that there&#39;s nothing else (see clueless) and so it survives. 

Meanwhile, people are buying smartphones and Chromebooks which are good enough if you haven&#39;t drunk the cool-aid.

But really, they&#39;re all a bit shit, it&#39;s just that Windows is a bit shittier but 99% of computers run it and 99% of computer fettlers don&#39;t know anything else.

Once, before Windows NT, but after Unix killed the Real Computers, Unix was the only real game in town for serious workstation users.

Back then, a smart man wrote:

&ldquo;I liken starting one&rsquo;s computing career with Unix, say as an undergraduate, to being born in East Africa. It is intolerably hot, your body is covered with lice and flies, you are malnourished and you suffer from numerous curable diseases. But, as far as young East Africans can tell, this is simply the natural condition and they live within it. By the time they find out differently, it is too late. They already think that the writing of shell scripts is a natural act.&rdquo; &mdash; Ken Pier, Xerox PARC<div>That was 30y ago. Now, Windows is like that. Unix is the same but you have air-conditioning and some shots and all the Big Macs you can eat.</div><div>
It&#39;s a horrid vile shitty mess, but basically there&#39;s no choice any more. You just get to choose the flavour of shit you will roll in. Some stink slightly less.</div></div>"
"eventtime","2015-01-30 18:14:00"
"url","http://liam-on-linux.livejournal.com/43948.html"
"userid","8744"
"itemid","170"
"event","I had a brief play with one on my last trip through Stansted Airport back to Czechia. I disliked the feel of the keyboard, then I realised how very fast &amp; accurate it had been on the few test lines that I had typed.

As the only sensible-sized smartphone on the market today with an actual hardware keyboard, I&#39;m very tempted. I&#39;m also kinda fed up with Android.

With a little luck, my Note 2 might still have some resale value, too.

Unfortunately, all the reviews I can find are dreck like this:

<a href=""http://www.theverge.com/2014/9/24/6837943/blackberry-passport-review"" target=""_blank"">BlackBerry Passport review: Getting stuff done or getting in the way?</a></div><div><a href=""http://www.theverge.com/2014/9/24/6837943/blackberry-passport-review"" target=""_blank"">By Dan Seifert &nbsp;on September 24, 2014 10:00 am &nbsp;Email @dcseifert</a>

It contains a lot of the typical bollocks that normally makes me denigrate smartphone reviews.

Whinge whinge it&#39;s too big whinge no Instagram whinge no Snapchat whinge no $shitty_proprietary_bullshit_toy_chat_app whinge videos don&#39;t look nice whinge.

No archiving in Gmail is a slight snag, but unlike Dan, I understand folders and filters and they do 99% of my archiving for me, so I don&#39;t care that much.

Well I am not a hormonal teenager who wants to give or get cock-shots. I don&#39;t give a flying fuck about Snapchat, Instagram or any of that puerile drivel.

I don&#39;t watch videos on my phone, because it&#39;s a tool not a toy, but I type on it <i>all the time</i>. I <i>detest </i>virtual keyboards. I&#39;m a middle-aged bloke with proper big man-sized hands; I can use a Galaxy Note 2 one-handed, no problem, and if one of these many little nappy-wearing pseudo-journos with the hands of a 12 year old girl can&#39;t grip it, that&#39;s a <i>good thing</i> because I can&#39;t use tiny crappy toys like normal iPhones. The 6+ is the first ever iPhone that is remotely big enough to be usable to me, and it&#39;s too thin and its battery too weedy. I want an inch-thick phone with circa 5 amp-hours in it, like I had 6 or 7y ago, please, not some svelte buttonless hairdressers&#39; phone.

So, not very helpful review, directly, inasmuch as the man-child who wrote it clearly wants something I&#39;d perceive as a teen&#39;s plaything. I <b>am</b> the kind of boring old pharte with a job to do that he tries &amp; utterly fails to imagine being.

But they&#39;re all like that, the Passport reviews. They&#39;re by bloody children who regard Flappy Bird as a mission-critical app.

But, OTOH, while Mr Still-Spattered-With-Spit-From-School there can&#39;t swap images of his small, soft and as-yet hairless genitals with his other playmates on it, he does manage to tell me that it&#39;s big, boring, solid and wide. These are <i>good things</i>.

My Note 2 is if anything too small. It doesn&#39;t reach from ear to mouth, as a proper phone should, it has no physical buttons, and at 2y old its battery lasts about 4-6h.

(So does its 1y old replacement battery.) But it&#39;s too wide, because it&#39;s made for watching videos on, and it wastes space on a pointless stylus when really I want it 1cm thicker with a QWERTY keyboard and in an ideal world 2 SIM slots and 2 batteries.

Really, I want a big bricklike Nokia Communicator (or at a push an HTC Universal; mine had an inch-thick 4800 mAh battery, weighed 450g &amp; was the last smartphone I owned with a <b>good</b> battery life)... but with a modern OS.

Sadly, though, all the phone companies are too busy wanking over leaked pictures of Apple products and making shitty compromised me-too toys to produce something for aging adults with dimming eyesight and big hands.

I was just wondering if the last bastion of vaguely sensible boring phones had made something worth buying."
"eventtime","2015-01-23 18:02:00"
"url","http://liam-on-linux.livejournal.com/43568.html"
"userid","8744"
"itemid","169"
"event","I have been meaning to try <a href=""https://www.archlinux.org/"" target=""_blank"">Arch Linux</a> for <i>years</i>.

As a former RPM user, once I finally made the switch <span class=""il"">to</span> Ubuntu, more or less exactly 10y ago, well, since then, I have become so wedded <span class=""il"">to</span> APT that I hesitate with non-APT distros.

My spare system on this machine is Crunchbang, which I like a lot, but is a bit too Spartan in its simplicity for me. Crunchbang is based on the stable version of Debian, which gives it one big advantage on my 2007-era built-for-Windows-Vista hardware: it uses a version of X.org so old that the ATI fglrx drivers for my Radeon HD 3470 GPU still work, which they haven&#39;t done on Ubuntu for 2 years now.

But there was a spare partition or 2 waiting. I <span class=""il"">tried</span> Elementary -- very pretty, but the Mac OS X-ness is just skin-deep; it&#39;s GNOME 3, very simplified. No ta. Deepin is too slow and doesn&#39;t really offer anything I want -- again, it&#39;s a modification of GNOME 3, albeit an interesting one. Same goes for Zorin-OS. I&#39;ve <span class=""il"">tried</span> Bodhi before -- it&#39;s interesting, but not really pretty <span class=""il"">to</span> my eyes. (Its Enlightenment desktop is all about eye-candy; as a desktop, it&#39;s just another Windows Explorer rip-off. If it shipped with a theme that made it look like one of those shiny floaty spinny movie-computer UIs, I might go for it, but it doesn&#39;t, it&#39;s all lairy glare that only a teenage metalhead could love.) Fedora won&#39;t even install; my partitioning is too complex for its installer <span class=""il"">to</span> understand. SUSE is a bit bloaty for my tastes, and I don&#39;t like KDE (or GNOME 3), which also rules out PCLinuxOS and Deepin.

So <span class=""il"">Arch</span> was the next logical candidate...

I&#39;ve been a bit sheepish since an Imaginary Internet Friend, Ric Moore, tried it with considerable success a month or two ago. (As I write, he&#39;s in hospital having a foot amputated. I&#39;ve been thinking of him tonight &amp; I hope he&#39;s doing well.)

So I have finally done it. Downloaded it, burned it <span class=""il"">to</span> a CD -- yes, it&#39;s that small -- installed it on one of my spare partitions and I am in business.

After a bit of effort and Googling, I found a simple walkthrough, used it, got installed -- and then discovered that Muktware only tells you about KDE, and assumes you&#39;ll use that and nothing else. I don&#39;t care for KDE in its modern versions, so I went with Xfce.

Getting a DM working was non-trivial but now I have LXDM -- the 3rd I <span class=""il"">tried</span> -- and it works. I have an XFCE4 desktop with the &quot;goodies&quot; extras, Firefox, a working Internet connection via Ethernet, and not much else.

It does feel very quick, though, I must give it that. <b>Very</b> snappy. I guess now begins the process of hunting down all the other apps that I use until I&#39;ve replicated all my basic toolset.

The install was a bit fiddly, much more manual than anything I&#39;ve done since the mid-1990s, but actually, it all went on very smoothly, considering that it&#39;s a lot of hand-entered commands which actually do not seem to depend much on your particular config."
"eventtime","2014-11-02 03:52:00"
"url","http://liam-on-linux.livejournal.com/43423.html"
"userid","8744"
"itemid","168"
"event","[Recycled (part of) a mailing list post: another crack at trying to explain what was significant about LispMs.]

One of the much-ignored differences between different computer architectures is the&nbsp;machine language, the Instruction Set Architecture (ISA). It&#39;s a key difference. And the reason it doesn&#39;t get much attention is that these days, there&#39;s really only one type left: the C machine.

There used to be quite a diversity -- there were various widely-divergent CISC architectures, multiple RISC ones, <a href=""https://en.wikipedia.org/wiki/Modified_Harvard_architecture"">Harvard</a> versus von Neumann designs, <a href=""https://en.wikipedia.org/wiki/Stack_machine"">stack machines</a> versus register machines,&nbsp;and so on.

Most of that has gone now -- either completely disappeared, or shrunk into very specific niches.
<lj-cut>
I&#39;m interested in weirdness and oddities, and the Lisp Machines had one of the weirder architectures.

The Lisp language&#39;s fundamental bricks are lists, and there are a small core set of tools for manipulating lists: one for extracting the first item, one for separating off the rest of a list, tools for manipulating &quot;atoms&quot; -- list members -- and their contents, and for building new lists out of atoms. Lisp Machine processors didn&#39;t execute Lisp code directly -- it was compiled down to a machine language -- but that machine language understood lists, atoms, heads and tails, and it stored lists <b>as lists</b> in special kinds of memory, which had hardware tags containing metadata about the contents.

The C model is that memory is a long flat series of bytes or words. You allocate blocks of that resource for this and that, and software keeps track of what is where and what it is. If you need to manage more memory than you have, you implement this in hardware, which lies to the software about what blocks are where.

The C model is much simpler, which means it&#39;s much cheaper to implement. Processors started out as a whole bunch of hand-wired components. Later, some of them were integrated into small chips, but a processor was still a whole cage full of boards. Then those were shrunk down to a bunch of chips, and finally, onto one chip. In parallel and overlapping with this, there were tiny simple processors designed and built as just one chip, while the contemporaneous minicomputers still had whole assemblies of boards. It wasn&#39;t a linear development -- there was a lot of overlap, and by the time that single-chip CPUs were getting reasonably capable (e.g. the Motorola 68000 in the late 1970s), the single-chip minicomputer processors looked huge and overcomplicated and expensive.

So rather than minis evolving into micros, the microprocessors started over, from the first very simple designs, which were gradually upgraded. But lessons learned from minis with the basic C-style design predominated in the new microchips, because it scales down a long way: you can implement it on tiny, simple chips, because really, it doesn&#39;t call for much complexity. So the first-gen microprocessors were just gradually upgraded with wider and wider word sizes -- 4, 8, 9, 16, 18, 32, 36 -- until they grew up and standardised into really big and complicated 32-bit single-chip processors, as capable as the 1970s minicomputer processors that went before them -- but faster and cheaper, so they outcompeted the mini-derived kit into oblivion.

(And to be fair, they didn&#39;t start with C - it&#39;s only when the 16-bit micros started to appear that C became a viable development tool. It was too complex for 8-bits, and 8-bit C compilers were very rudimentary -- e.g. the Hisoft C compiler I used on my ZX Spectrum only supported integer arithmetic.)

Meantime, a different branch of the computer evolutionary tree developed fancy single-user workstations, based on the most powerful, productive language: Lisp Machines. Not doing the minicomputer thing -- no rings, no privilege levels, no fancy instructions for maths, but instead, their complexity was in an entirely orthogonal direction -- hardware that was actually fitted to the language, so the processor directly handled the core constructs of the language: managing lists. The chips had facilities for taking the first element of a list, the rest of it, for splitting and concatenating lists, for traversing lists, etc. You didn&#39;t deal in bits and bytes, in blocks of RAM, you deal in higher-level structures and the processor implemented them in hardware.

Meantime, the crazy mixed-up proprietary world of proprietary OSes on complicated, incompatible architectures seized on one element of IBM&#39;s Stretch mainframe. This became the next big idea. It said, look, you have all these fancy facilities microcoded into your processors, designed to support 1970s languages and OSes that you don&#39;t use much any more. (For instance, I have heard it argued that the Motorola 680x0 design can be considered as an implementation of the DEC PDP-11.) Now everything is Unix and C, and it doesn&#39;t need all that legacy stuff. So throw it away, get rid of all the microcode, and just implement the bare minimum.

So we got the 2nd generation of microprocessors: dead simple, small, cheap, fast. RISC.

And RISC boxes, with a few exceptions, only ran Unix.

For a while, this was such a compelling solution that it killed off Lisp Machines in academia, their stronghold, because LispMs were expensive and ran an arcane language which is capital-H Hard to learn and to use, and universities are always short of money. (Which is why NeXT failed too.)

Only one proprietary architecture survived this: 80x86, running DOS, mainly due to a critical mass of DOS apps. x86 boxes ran Unix fine, and more cheaply than proprietary Unix boxes, but the business world was wedded to DOS and DOS apps, partly due to Microsoft&#39;s very successful building of a practical monopoly on business computing.

IBM tried to address the failings of DOS and the PC design with a new and better OS, OS/2, running on new and better x86 hardware -- but it didn&#39;t work, because it didn&#39;t run DOS apps well.

Microsoft shrugged, bolted the OS/2 1.2 GUI onto its failed Windows product for DOS, enhanced its ability to multitask DOS apps, and to its surprise created a huge hit.

RISC, oddly, ended up converging with x86. Modern x86 chips <b>are</b> RISC chips, with a translation layer bolted on top. Complicated but it works really well and they still have that critical mass of software. Meantime, playing catch-up, all the workstation RISC chips got bigger and more complex too, until they&#39;re just as big and just as complex as x86, but they don&#39;t have the volume, so they ended up more expensive, running proprietary Unix.

And here we are today. There are really just 2 families of processor and OS left.

X86, running Windows. (Windows has never been a hit on RISC chips and I am willing to bet that it never will.)

And RISC, running Unix -- which today means ARM running Linux, plus some small proprietary contenders on the sides: AIX on POWER, Solaris on SPARC, both looking moribund.

ARM has survived because it&#39;s stayed simple. It doesn&#39;t compete directly against x86: it focuses on small, battery-powered, passively-cooled devices, or embedded stuff. ARM chips are considerably slower than x86, and all the proprietary ARM OSes are dead now. Symbian, RISC OS, QNX, vxWorks, iTron -- all dead or nearly, or specialised into niches where they don&#39;t compete with the big boys.

On the high end, there&#39;s one real player: x86 running Windows... and the parasitic FOSS Unix that burst out of Wintel&#39;s chest, Linux. All the other Unixes are basically dead, except tiny FOSS players, which in turn parasitise on Linux. (Oh, all right, they&#39;re commensal or symbiotic, but it doesn&#39;t sound as good.)

What have they got in common? Really only one thing: the C model.

LispMs weren&#39;t out-competed -- they just got caught in the crossfire.

I am not, note, arguing that we should try to reanimate their corpse. The only other interesting development I&#39;ve seen, though, is the <a href=""http://jakob.engbloms.se/archives/2004"">Mill CPU</a> from OOB Computing.

Google it, <a href=""https://www.youtube.com/channel/UCKdGg6hZoUYnjyRUb08Kjbg"">watch the (long) videos</a>. It&#39;s hard to understand but it seems to be trying to find a way to cross the DSP model of processor design with general-purpose CPUs.

They&#39;re not <i>that</i> interested in the software side.

I am. I&#39;ve spent 20y looking at a 30,000&#39; view of comparative OS design.

Where we are now is a position of business pragmatism: taking what works, improving it, and sod the theoretical stuff. We&#39;ve been doing nothing else since the 1980s. Finally, in the 1990s, the hardware-memory-managed world of Unix workstations converged with the x86 world. The workstations died out, x86 was re-invigorated.

We have Unix and we have Windows, which successfully transitioned from its DOS-based roots (which grew out of the 16-bit PDP-11 via CP/M) and made the leap to NT, which is something more like the hardware-memory-managed VAX and VMS.

Now, like the end of Orwell&#39;s <i>Animal Farm,</i> it&#39;s hard to see the difference.

&ldquo;The creatures outside looked from pig to man, and from man to pig, and from pig to man again; but already it was impossible to say which was which.&rdquo;

Both have complex monolithic kernels, kernel-space subsystems, and userlands on top. Both support multiple architectures. Both have a VM-like model of execution, with processes in their own memory spaces. Both are primarily written in C-family languages. Windows has borrowed design elements from Unix and is still doing so -- for instance the way that the console-mode kernel and its services are being split off from a separate GUI layer, held together via scripting laguages. Meanwhile Unix has thrived by borrowing its UI wholesale from Windows.

(And a bit from the Mac, but then, Windows borrowed direct from the Mac too, so it&#39;s interwoven.)

They are built out of compiled, static binaries, running on a very simplistic memory model shaped by the assumptions of C. Any interpreted dynamic stuff is a thin layer on top.

What we have are two different implementations of the same basic concepts, which have fed off one another.

The temporary 1980s explosion in multitasking GUI OSes -- AmigaOS, Atari MINT, RISC OS, BeOS, EPOC, NewtonOS, GEOS, QDOS, Taos/Intent, QNX -- they&#39;re basically all gone. Most are dead and forgotten, all that survive at all are obscure and niche.

Lots&nbsp; of stuff has been tried, but really, taking a long view, you can see that they all share the same model and the same assumptions. Stuff that experiments with changing this in profound ways -- microkernels (Chorus, Sprite, Amoeba, Hurd, CoyotoOS), network OSes (Plan 9/Inferno, possibly arguably HeliOS), OSes written in managed code (e.g. Microsoft Singularity) -- has all failed: the established model is too entrenched.

Incremental improvement doesn&#39;t change the fundamentals. Stuff that does change the fundamentals breaks compatibility so never catches on.

But it works. It&#39;s not elegant, it&#39;s not great, but it does the job.&nbsp;It&#39;s &quot;good enough&quot;. <a href=""http://www.jwz.org/doc/worse-is-better.html"">Worse-is-better won</a>.

But now, I really think it&#39;s holding us back. <b>Everything</b> that has been tried to move it forward has failed. We&#39;re still polishing the same turd.

So maybe, we should re-evaluate the <b>other</b> option, the way that we <b>didn&#39;t</b> go.

Trying to fix what we have has failed. It&#39;s good enough. It&#39;s messy, insanely complicated, more duct tape than original design, but it works. We have tried to re-design it but no change to it gives enough benefit to be worth the cost.

So maybe we need something <i>totally</i> different, from that other, long-abandoned school of design. The MIT/Stanford style, the Lisp style. Metaprogramming, programmable programming languages, code that is a list which manipulates lists, and which is lists all the way down to the metal.

Only because it&#39;s the <b>only</b> other model that really competed, that made it right into the GUI and Internet era. It&#39;s the other side of the coin from the line that went Fortran -&gt; Basic -&gt; C, compilers and interpreters as 2 separate things.

There were OSes in high-level languages in the minicomputer era (e.g. Burroughs); they died out. Not enough advantages over the C model.

There were OSes and languages that got away from native-code compilation - Taos, Intent, Java, Limbo on Dis on Inferno. They died out, or were absorbed (Java). Not enough advantages.

There are niche OSes that are strong in embedded systems, that eschew the whole Unix/NT multitasking model - vxWorks made it to Mars, even! But they&#39;re only strong in their special niches and even there, they&#39;re losing ground.

There is only <b>one</b> alternative that has lasted, which has stood up to the assault of the C monoculture. It&#39;s in the FOSS world as Emacs and SBCL. It&#39;s inside lots of commercial software.

It&#39;s the only other option. It nearly made it, but RISC killed it, and x86 killed RISC on the desktop, leaving us back at an enhanced Square 1.

I can&#39;t see any other contender.  </lj-cut>"
"eventtime","2014-07-27 19:11:00"
"url","http://liam-on-linux.livejournal.com/43040.html"
"userid","8744"
"itemid","167"
"event","Long time, no post. This is because since April, I have started a new job where I actually get paid to write technical stuff for a living.

(Hint - I&#39;m going to have to change that usericon...)

Anyway, this subject came up in conversation with my colleague Pavel recently. In my department, there are some Vi[m] advocates, at least one Emacs user in the wild (approach with caution), and when I said I used Gedit <i>from choice</i>, I got pitying looks. :&not;)

Which gave me a chance to have my usual rant about the deep and abiding nastiness of both Vi and Emacs, which did at least provide some amusement. It also led Pavel to ask, quite reasonably, what I did want from a console/shell text editor that wasn&#39;t provided by, say, Joe, Nano or Pico.

I said CUA and then had to explain what CUA was, and pointed at SETedit, which I&#39;ve linked to before. Sadly, i<span style=""line-height: 1.4;"">t hasn&#39;t been updated in a while. Packages are only for old versions of popular distros.</span><div>http://setedit.sourceforge.net/

<span style=""line-height: 1.4;"">This led him to look thoughtful and go off and do some digging. He came back with some gems.</span></div>
Firstly, there&#39;s the rather fun Text Editors Wiki, which is not as comprehensive as it might be but has a lot of interesting reading.
http://texteditors.org/cgi-bin/wiki.pl

First, he pointed me at <span style=""line-height: 1.4;"">XWPE. It certainly looks the part, but sadly the project seems to have died. I did get it running on Fedora 20 by installing some extra libraries and symlinking them to names XWPE wanted, but it crashes very readily.</span><div>http://www.identicalsoftware.com/xwpe/

<span style=""line-height: 1.4;"">After some more hunting, he also found </span><span style=""line-height: 1.4;"">eFTE, enhanced FTE. I rather like this. Not all the shortcuts do what I expect, but it works well nonetheless.</span></div><div>http://sourceforge.net/projects/efte/

<span style=""line-height: 1.4;"">Incidentally, eFTE seems to be a fork of a no-longer-maintained older editor, FTE:</span></div><div>http://fte.sourceforge.net/

<span style=""line-height: 1.4;"">More recently, I&#39;ve also discovered Tilde. It </span><span style=""line-height: 1.4;"">is currently maintained and has recent packages available. It looks a bit richer than eFTE, but sadly, the Alt key doesn&#39;t work in a window. Clearly this is a known issue as there&#39;s a workaround using Esc instead, but it makes it 2 keystrokes rather than one with a modifier.</span></div><div><div>http://os.ghalkes.nl/tilde/

<span style=""line-height: 1.4;"">I remain surprised that these things are obscure &amp; little-known. I&#39;d have thought that given how many people are moving from other OSes to Linux, a lot more MICROS~1 &eacute;migr&eacute;s would have wanted such tools.</span></div></div>"
"eventtime","2014-06-27 17:07:00"
"url","http://liam-on-linux.livejournal.com/42908.html"
"userid","8744"
"itemid","166"
"event","I am already sick and tired of listening to clueless noobs who think they&#39;re techies saying &quot;XP is fine, stop worrying&quot; or &quot;I don&#39;t do Linux, it&#39;s too different&quot; or &quot;I tried it in 2002 and it was rubbish&quot;.

Well it&#39;s longer since Ubuntu came out (2004) than the gap from Windows for Workgroups 3.11 to Windows XP. Remember the extent of those changes? Well Linux changes a <b>lot</b> faster.

And so what if it&#39;s not the same? It&#39;s not like changing from a car to a motorbike. It&#39;s not that different any more.

To a techie who looks under the hood, who does their own maintenance, sure, it&#39;s going from petrol car to electric bike or something. Very little in common.

To someone who uses a desktop, browses the web, plays some simple Flash games or Solitaire etc., occasionally opens a PDF and prints it, or opens an MS Office doc, completes a form and sends it back, stuff like that, then an appropriately-chosen Linux is <b>more like</b> WinXP than Win8 is <b>by far</b>.

But there is galloping fear of the alien in IT. Probably, I suspect, because there are millions of people working in IT who know nothing at all except MS Windows. Everything else is foreign to them, alien and terrifying, and they instantly react like a pod-person in the 1970s <i>Invasion of the Body Snatchers</i>.

<lj-embed id=""24""/>

And you know what? It&#39;s appropriate, because you&#39;re all fucking pod people. Stamped out, copies, clones, with no originality and no imagination. You&#39;re neophobic.

Anyone who <b>actually</b> knows about computers - <b>real</b> tech people - can handle any OS, any machine. I&#39;ve troubleshot and fixed problems with machines I have never seen or heard of before; I&#39;ve sorted out stuff on AS/400 and IBM System/360 mainframes, I&#39;ve got a DEC PDP-11 talking and doing file exchange with a classic Mac running System 7, and before I walked in, I had never even <b>seen</b> a PDP-11 in my life before.

Software is an office supply, like paperclips. Today it all does much the same, in much the same way.

Imagine the contempt you&#39;d feel for someone who bleated and whinged and complained that they were given a different brand of stapler, or they had to change filing cabinets to one where the keyhole&#39;s on the other side. You&#39;d sneer at someone who demanded a training course to help them adjust.

Anyone who only knows one platform, one OS, is not a techie at all, not of any type. If you can&#39;t drive half a dozen kinds of computers then you can&#39;t drive.

Any competent biker could switch from a 125 trailie to a Gold Wing to a superbike and not kill themselves when they twisted the throttle. They&#39;d go with respect and care and caution, not bleat like an infant because one of the buttons on the handlebars had moved and was a different colour.

So bloody well grow up.

Linux is an answer to a problem. If you have an old XP computer, <b>no</b> it is <b>not</b> safe to use it any more, and yes, you should replace it. But if you get a new one with Win8, it will be very very different indeed. They don&#39;t even have a BIOS any more, let alone a bloody Start menu.

And if you don&#39;t have the money for a new one, well, an old XP computer won&#39;t run Windows 7 properly, no.

But there&#39;s a perfectly good alternative that is faster, simpler, safer, more secure, more reliable and it doesn&#39;t even cost anything. It&#39;ll run on anything XP runs on, works great and all you have to do is get your hands oily. Use Google. Don&#39;t think &quot;I know this.&quot; You don&#39;t. No, not all computers install software by downloading a binary and running it - in fact, that&#39;s a fucking stupid design, which spreads malware. Not all computers use a website for updates - that&#39;s a fucking stupid design, too.

So grow a pair. Stop whinging. Google &quot;how to install skype ubuntu&quot; rather than downloading and fucking about and breaking it. Download &quot;how to enable GeForce 240 ubuntu 12.04&quot; before you go wasting time. Google &quot;transfer IE bookmarks Firefox&quot; or &quot;libreoffice excel compatibility&quot; or whatever.

Don&#39;t assume you know. Assume you don&#39;t. You have the entire world&#39;s information resource at your fingertips. Use it. Ask the Internet. Ask bloody Ixion.

But stop fucking whinging that &quot;it doesn&#39;t run my copy of Anus Invaders 6&quot; or &quot;my crappy plastic &pound;30 printer from PC World doesn&#39;t work&quot; and buy a better one. It&#39;s cheaper than a new ink cartridge anyway.

Learn. Life is learning. Life is growth. Stop acting like a corpse and live."
"eventtime","2014-04-12 20:22:00"
"url","http://liam-on-linux.livejournal.com/42567.html"
"userid","8744"
"itemid","165"
"event","Frankly, coming from a background in 1980s and 1990s OSes, I think modern ones are appalling shite. They&#39;re huge, baggy, flabby sacks of crap that drag themselves around leaving a trail of slime and viscera - but like some blasphemous shoggoth, they have organs to spare, and the computers they run on are <i>so</i> powerful and have <i>so</i> much storage that the fact that these disgusting shambling zombie Frankenstein&#39;s-monster things, stitched together from bits of the dead, dropping eyeballs and fingers, actually work for weeks on end.

On the server, no problem, run hundreds of instances of them, so when they implode, spawn another.

It&#39;s crap. It&#39;s all terrible, blatantly obvious utter crap, but there&#39;s almost nobody left who remembers any other way. I barely do, from old accounts, &amp; I&#39;m near 50.

We have layers of sticking-plaster and bandages over kernels that are hugely-polished turds, moulded into elegant shapes. These are braindead but have modules for every conceivable function and so can run on almost anything and do almost anything, so long as you don&#39;t mind throwing gigabytes and gigahertz at the problem.

And those shiny turds are written in braindead crap languages, designed for semi-competent poseurs to show off their manliness by juggling chainsaws: pointless Byzantine wank like pointer arithmetic, missing basic types for strings, array bounds-checking, and operator overloading. Any language that even allows the possibility of a buffer or stack overflow is hopelessly broken and should be instantly discarded. The mere idea of a portable assembly language is a vestige of days when RAM was rationed and programmers needed to twiddle bits directly; it should have been history before the first machine with more than a megabyte of RAM per user was sold.

Computers should be bicycles for the mind. They let us take our existing mental tools and provide leverage, mechanical advantage, to let us do more.

We work in patterns, in sets, in rich symbols; it is how we think and how we communicate. That, then, should be the native language to which our computers aim: the logic of entities and sets of entities, that is, atoms and lists, not allocated blocks of machine storage - that is an implementation detail, it should be out of sight, and if it&#39;s visible, then <a href=""http://www.loper-os.org/?p=55"">your design is faulty</a>. If you routinely <i>need</i> to access things, then your design is <a href=""http://rationalwiki.org/wiki/Not_even_wrong""><b>not even wrong</b></a>.

By the late &#39;50s we had a low-level programming language that could handle this. It&#39;s unreadable, but it was only meant to be the low-level; we just never got the higher level wrapper to make it readable to mortals. The gods themselves can work in it; to lesser beings, <a href=""http://xkcd.com/297/"">it&#39;s all parens</a>.

Now, we have a rich <a href=""http://lambda-the-ultimate.org/node/3253"">choice</a> of higher-level <a href=""https://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/lang/lisp/code/syntax/cgol/0.html"">wrappers</a> to make it all <a href=""http://readable.sourceforge.net/"">nice</a> and easy and <a href=""https://opendylan.org/"">pretty</a>. Really very <a href=""https://opendylan.org/about/apple-dylan/screenshots/index.html"">pretty</a>.

And later, people built machines specifically to run that language, whose processors <a href=""http://pt.withy.org/publications/LispM.html"">understood</a> its <a href=""http://www.loper-os.org/?p=46"">primitives</a>.

But they lost out. CPUs were expensive, memory was expensive, so instead, OSes grew simpler; Unix replaced Multics, and CPUs grew simpler too, to just do what these simple OSes written in simple languages did. Result, these simple, stripped-down machines and OSes were way more cost-effective, and they won. The complex machines died out.

Then the simpler machines - which were still quite big and expensive - were stripped down even more, to make really cheap, <a href=""http://www.intel.com/content/www/us/en/history/museum-story-of-intel-4004.html"">rudimentary 4-bit CPUs for calculators</a>, ones that fitted on one chip.

They sold like hotcakes, and were developed and refined, from 4-bit to <a href=""http://www.cpu-world.com/CPUs/8008/"">8-bit</a>, from primitive 8-bit to <a href=""http://www.cpu-world.com/CPUs/8080/index.html"">better 8-bit</a>, with its own de-facto standard OS which was a <a href=""https://groups.google.com/forum/#!topic/alt.folklore.computers/wR45iTJY-A4"">dramatically simpler version</a> of a simple, obsolete OS for 16-bit minicomputers.

And that chip begat a <a href=""http://www.cpu-world.com/CPUs/8086/index.html"">clunky segmented 8/16-bit one</a>, and that a <a href=""http://www.cpu-world.com/CPUs/80286/"">clunky segmented 16-bit</a> one, and that a bizarre <a href=""http://www.virtualdub.org/blog/pivot/entry.php?id=85"">half-crippled</a> 32-bit one that could <a href=""http://pdos.csail.mit.edu/6.828/2011/readings/i386/c15.htm"">emulate lots of the 8/16-bit one in hardware</a> FFS. And that redefined the computer industry and it was nearly <b>two decades</b> until we got something slightly <a href=""http://news.cnet.com/AMD-releasing-details-on-64-bit-Sledgehammer/2100-1001_3-244278.html"">better</a>, a somewhat-improved version of the same old same old.

And that&#39;s where we are now. The world runs on huge, vastly complex scaled-up go-faster versions of a simplified-to-the-maximum-extent-possible calculator chip. These chips grew out of a project to scale-down simple, dumb, brain-dead chips built to be cheap-but-quick because the proper ones, that <a href=""http://web.mit.edu/~simsong/www/ugh.pdf"">people actually liked</a>, were too expensive 40 years ago. Of course, now, the descendants of those simplified chips are vastly more complex than the big expensive ones their ancestors killed off.

And what do we run on them? Two OSes. One a descendant of a quick-n-dirty lab skunkworks project to make an old machine useful for games, still today written in portable assembler with richer portable-assembler things written in the lower-level one running on top of it. And a descendant of a copy of a copy of a primitive &#39;60s mini OS which has been extensively rewritten in order to imitate the skunkworks thing.

But these turds have been polished so brightly, moulded into such pretty shapes, that they&#39;ve utterly dominated the world since my childhood. It&#39;s still all made from shit but it&#39;s been refined so much that it <a href=""http://www.foodista.com/blog/2011/06/14/japanese-scientist-makes-fake-meat-from-human-feces#"">looks, smells and tastes quite nice now</a>.

We still are covered in shit and flies - &quot;binaries&quot;, &quot;compilers&quot;, &quot;linkers&quot;, &quot;IDEs&quot;, &quot;interpreters&quot;, &quot;disk&quot; versus &quot;RAM&quot;, &quot;partitions&quot; and &quot;filesystems&quot;, all this technical cruft that better systems banished before the first Mac was made, before the 80286 hit the market.

But as the preface to the Unix-Hater&#39;s Handbook says:
<blockquote>
``I liken starting ones computing career with UNIX, say as an undergraduate, to being born in East Africa. It is intolerably hot, your body is covered with lice and flies, you are malnourished and you suffer from numerous curable diseases. BUT, as far as young East Africans can tell, this is simply the natural condition and they live within it. By the time they find out differently, it is&nbsp;too late. They already think that the writing of shell scripts is a natural act.&#39;&#39;</blockquote>

- Patrick Sobalvarro

<i>Nobody knows any better</i> any more. And when you try to point out that there <i>was</i> once something better, that there <i>are</i> other ways, that it doesn&#39;t need to be like this... people just ridicule you.

And no, in case it&#39;s not clear, I am not a Lisp zealot. I find it unreadable and cannot write &quot;hello world&quot; in it. I also don&#39;t want 1980s Lisp Machines back - they were designed for Lisp programmers, and I&#39;m not one of them.

I want rich modern programming languages, as easy to read as Python, as expressive as Lisp, with deep rich integration into the GUI - not some bolt-on extra like a tool to draw forms and link them to bits of code in 1970s languages. There&#39;s no implicit reason that why the same language shouldn&#39;t be usable by a non-specialist programmer writing simple imperative code, and also by a master wielding complex class frameworks like a knight with a lightsabre. It&#39;s all code to the computer: you should be able to choose your preferred viewing level, low-level homoiconicity or familiar Algol-like structures. There shouldn&#39;t be difference between interpreted languages and compiled - it&#39;s all the same to the machine. JIT and so on solved this years ago. There&#39;s no need for binaries at all - look at Java, look at Taos and <a href=""http://c2.com/cgi/wiki?TaoIntentOs"">Intent Elate</a>, look at <a href=""http://c2.com/cgi/wiki?InfernoOs"">Inferno</a>&#39;s Limbo and Dis. Hell, look at Forth over 30 years ago: try out a block of code in the interpreter; once it works, name it and bosh, it&#39;s compiled and cached.

Let&#39;s assume it&#39;s all FOSS. No need for licences mandating source distribution: the end-product <b>is all source</b>. You run the source directly, like a BASIC listing for a ZX Spectrum in 1983, but at modern speeds. If you aren&#39;t OK with that, you don&#39;t like distributing your code, fine, go use a proprietary OS and we wish you well. &nbsp;Hope it still works on their next version, eh?

It could be better than we have. It <i>should</i> be better than we have. Think the Semantic Web all the way down: your chip knows what a function is, what a variable is, what a string or array is - there&#39;s no level transition where suddenly it&#39;s all bytes. There doesn&#39;t need to be.

And this stuff isn&#39;t just for programmers. I&#39;m not a programmer. Your computer should know that a street address is an address, and with a single command you can look up anyone&#39;s address that is in any document on your machine - no need to maintain a separate address-book app. It should understand names and dates and amounts of money; there were apps that could do this in the 1980s. That we still need separate &quot;word processors&quot; and &quot;spreadsheets&quot; and &quot;databases&quot; today is a sick joke. 

I have clients who keep all their letters in one huge document, one per page or set of pages per correspondant... <i>and there&#39;s nothing wrong with that</i>. We shouldn&#39;t be forced to use abstractions like files and documents and folders if we don&#39;t want to.

I have seen many clients who don&#39;t understand what a window is, what a scrollbar does; these abstractions are too complex for them, even for college professors after decades of use of GUIs. That&#39;s why iPads are doing so well. You reach out and you pull with a fingertip.  

And that&#39;s fine, too. The ancestor of the iPad was the Newton, but the Newton that got launched was a crippled little thing; the original plan was a pocket Lisp Machine, with everything in Dylan all the way down to the kernel.

And the ancestor of the Macintosh was Jef Raskin&#39;s &quot;information appliance&quot;, with a single global view of one big document. Some bits local, some remote; some computed, some entered; some dynamic, some static; with the underlying tools modular and extensible. No files, no programs, just commands to calculate this bit, reformat that bit, print that bit there and send this chunk to Alice and Charlie but not Bob who gets that other chunk.

Sounds weird and silly, but it was, as he said, humane; people worked for millennia on sheets of paper before we got all this nonsense of icons, files, folders, apps, saving, copying and pasting. The ultimate discrete computer is a piece of smart paper that understands what you&#39;re trying to do.

And whereas we might be able to get there building on bytes in portable assembler, it will be an awful lot harder, tens to hundreds of times as much work and the result won&#39;t be very reliable."
"eventtime","2014-04-07 21:28:00"
"url","http://liam-on-linux.livejournal.com/42285.html"
"userid","8744"
"itemid","164"
"event","Apparently, it&#39;s the ultimate Linux, and with his tweaks to the current development kernel and a custom scheduler, it&#39;s insanely responsive, and if you haven&#39;t tried it, you&#39;re not a Linux god.

So I said...

Um. Good for you. I am pleased you&#39;ve found a system you find nicely responsive.

Me, I just want something simple, low-maintenance and reliable, with a
good polished rich UI, that does what I need. The less work I have to
do to achieve this, the better the OS is, for me.

Yours sounds very high-maintenance indeed and I&#39;m not remotely
interested in going to all that work.

I don&#39;t consider myself a Linux god. I am reasonably clueful. I&#39;ve
been using Ubuntu since it came out in 2004, SuSE for a couple of
years before that, Caldera for a couple of years before that. That
followed a good few years on NT 3.51 and NT 4, which followed Windows
95. I switched to Windows 95 from OS/2 - I was a keen OS/2 user from
2.0 to 2.1 to 3.0. It really was the best 32-bit OS for PCs back then.

Before that, at work, I built, ran and supported servers running SCO
Unix and before that SCO Xenix. My Unix experience goes back to about
1988, which is when I switched over from the VAX/VMS I used at
University.

I have also used IBM AIX and SUN SunOS and Solaris, but not much.

Plus Novell Netware - I was a bit of a guru on Netware 2 and 3 but
wasn&#39;t so impressed with Netware 4 and have barely used 5. I wrote a
masterclass on building a small-business server with Red Hat 6 for PC
Pro magazine in the late 1990s. I&#39;ve also reviewed about 20 or 30
Linux distros over the years, so I feel I know the Linux landscape
well.

I&#39;m also very interested in alternative (non-Unix) OSes, especially
for the PC. BeOS is my personal all-time favourite.

Off PC hardware, I&#39;m also pretty good on Mac OS X and classic Mac OS,
before thzat Acorn RISC OS and Psion EPOC and its successor Symbian,
and have some knowledge of AmigaOS, Atari GEM (I was peripherally
involved in the GPL FOSS FreeGEM project to revive PC GEM; my name&#39;s
in the credits of FreeDOS, to my startlement.)

I was definitely an MS-DOS guru back in the late 1980s/early 1990s and
supported all the major networking systems - 3Com 3+Share, 3+Open, DEC
Pathworks, AppleShare, Sage MainLAN, Personal Netware, Netware Lite,
NT Server from the very first version, etc.

So I guess you could say that my knowledge is broad but in places
shallow, rather than very deep in any one area, such as Linux. :-)

But I feel really sorry for you if you think that /any/ Linux system
is genuinely fast and responsive. It&#39;s not. It&#39;s a huge lumbering
sloth of an OS. You really need to try BeOS, or failing that Haiku, if
you want to experience what a fast responsive OS on PC hardware feels
like.

Sadly, there just weren&#39;t the apps for it, and no VMs in those days.

And for something vastly more responsive than Haiku, try Acorn&#39;s RISC
OS. It&#39;s the original OS for the ARM chip that these days struggles to
run bloated leviathans like Apple iOS and Android. RISC OS is the
single most responsive system I&#39;ve ever used, because the entire core
OS - kernel, GUI, main accessory apps - fits into about 6MB of Flash
ROM.

No, that&#39;s not a typo. Six megabytes. Complete Internet-capable
multitasking GUI OS with network clients etc.

It runs on the Raspberry Pi and RISC OS itself is now shared-source
freeware so you can download it from Risc OS Open Ltd. for nothing and
run it on a &pound;25 computer - on which it performs very very well, many
tens of times faster than a lightweight cut-down Linux such as
Raspbian.

So, no, not a Linux god, but, you know, not a n00b either.

Try some of these OSes. Prepare to be surprised. You might enjoy the experience.

Most of them have nice friendly GUI text editors, too, way friendlier
than Vi /or/ Emacs. ;-D"
"eventtime","2014-04-07 12:37:00"
"url","http://liam-on-linux.livejournal.com/42129.html"
"userid","8744"
"itemid","163"
"event","From <a href=""http://forums.theregister.co.uk/forum/2/2014/03/25/microsoft_releases_operating_system_source_code/?post_received=2145003#c_2145003"" target=""_blank"">this Reg forum</a>...

No, the DOS limits were /much/ earlier and older.

From old old memory:
MS-DOS 1.x didn&#39;t support hard disks.
MS-DOS 2.x did, but just one, of up to 10MB.
MS-DOS 3.0 supported a single hard disk partition (per drive) of up to 32MB.
MS-DOS 3.2 supported two partitions per drive, so 2 x 32MB.
MS-DOS 3.3 supported one primary and an extended partition containing as many 32MB &quot;logical drives&quot; as you wanted. (I built an MS-DOS fileserver with a 330MB hard disk once - it had drive letters C:, D:, E:, F:, G:, H:, I:, J:, K: and a leftover 11MB L: drive. Messy as hell but all you could do without 3rd party &quot;disk extenders&quot; such as Golden Bow&#39;s one. The server OS was 3Com 3+Share if anyone remembers that.)

Lots of vendors implemented hacks and extensions to allow bigger disks, but they were all mutually incompatible and many failed to work with some 3rd party software. Of course, anything that directly accessed disk data structures, like a defragger or a disk-repair tool such as Norton Utilities was 100% guaranteed to catastrophically corrupt any such extended disk setup.

The one that caught on was Compaq DOS 3.31. It used an extension of FAT16 that allowed bigger clusters - still just 65,535 of them, but multiple 512 byte sectors per cluster, permitting bigger partitions. The max cluster size was 16KiB so the max disk size was 65535*16KiB = 2GiB.

This is the one that IBM adopted into MS-DOS 4 and it became the standard. However, disks over 512MB used inefficient 8KiB clusters - i.e. files were allocated with a granularity of 8KiB and even a 1 byte file took 8KiB. An 8.0001KiB file would take 16KiB.

This became disastrous over 1GiB where the granularity was 16KiB. Roughly 20-30% of disk space would be wasted because of this granularity as inaccessible &quot;slack space&quot;.

This was only fixed in Windows 95 OSR2 with FAT32, which permitted huge disks - up to 2TiB - with much finer granularity.

But all of DOS 4, 5 and 6.x permitted disk partitions of up to 2GiB."
"eventtime","2014-03-26 18:36:00"
"url","http://liam-on-linux.livejournal.com/41912.html"
"userid","8744"
"itemid","162"
"event","No, honestly, I do have a real reason for asking.

As per my previous post, I have just upgraded my big laptop from 2 x 250GB drives to 120GB SSD + 1TB HD. This, clearly, leaves me with 2 spare drives. One I&#39;ve put in a Firewire case I had - it&#39;s a gift for a friend - but the other is going begging, as it were.

Now, I planned to put it in my small ultraportable - a Thinkpad X200s which has a 160GB drive. However, I now discover that it&#39;s a 7200rpm drive.

So the question is, will I be able to notice the performance degradation? I have little experience with such things - is it dramatic or marginal? It&#39;s a fairly fast laptop (C2D, 4GB RAM) so it doesn&#39;t struggle..."
"eventtime","2014-03-03 17:14:00"
"url","http://liam-on-linux.livejournal.com/41641.html"
"userid","8744"
"itemid","161"
"event","So, with some of the money from the Great Decluttering I am doing, I bought an SSD and a 1TB hard disk for my big desktop-replacement laptop. It&#39;s a big beast &amp; takes 2 x 2.5&quot; hard disks.

I&#39;ve put my root folder on the SSD and /home on the HD, and merged all the stuff from my Windows home directory and data directory and Linux&#39; /home into one big shared NTFS partition.

For Ubuntu, all I had to do was boot off a recent CD and reinstall GRUB and everything worked. (OK, I had to manually point it at my new swap partition.) These UUID things are excellent.

Windows required finding an install DVD, starting off it and doing a &quot;Startup Repair&quot; but then it was happy.

With Win7 the difference is not all that dramatic. It&#39;s quicker, sure, but still takes tens of seconds.

But Ubuntu... *wow*. From GRUB menu to login screen in about 2-3 seconds. From there to desktop in ~1 sec.

Stunning. I had heard it was good, but those poor Windows users just have no idea."
"eventtime","2014-02-28 23:26:00"
"url","http://liam-on-linux.livejournal.com/41338.html"
"userid","8744"
"itemid","160"
"event","Ted Nelson, creator of Project Xanadu, widely-hailed as the inventor of hypertext. Xanadu never quite happened; what we got was a very watered-down version, the WWW.

Nelson&#39;s written several books, none of which I&#39;ve read - but now, I really want to. In a series of bite-sized, sub-15-min videos, he presents his jaundiced but remarkably perceptive and insightful overview of the history of the personal computer - he&#39;s been there since the start.

Videos 0 through 4 are excellent high-level overviews. Number 5 is on hypertext, his specialist field, and&nbsp;he starts letting his bias show a bit; numbers 6 and the concluding N are, with the best will in the world, rants - but amusing ones and still informative as anything you could hope to find.

<b>Strongly</b> recommended viewing.

<lj-cut>
<lj-embed id=""16"" />

<lj-embed id=""17"" />

<lj-embed id=""18"" />

<lj-embed id=""19"" />

<lj-embed id=""20"" />

<lj-embed id=""21"" />

<lj-embed id=""22"" />

<lj-embed id=""23"" />

</lj-cut>"
"eventtime","2014-01-29 00:26:00"
"url","http://liam-on-linux.livejournal.com/41036.html"
"userid","8744"
"itemid","159"
"event","<div>The title is from this Guardian story by Charles Arthur: <a href=""http://www.theguardian.com/technology/2013/dec/19/why-tablets-killing-pc-ipad-apple-microsoft"" target=""_blank"">&#39;My iPad has Netflix, Spotify, Twitter &ndash; everything&#39;: why tablets are killing PCs</a>.

There are a whole bunch of competing factors here which seems to baffle many observers. This isn&#39;t an encyclopaedic list, but...<lj-cut></div><div>&bull; The rise in x86 performance fell off a cliff in about 2007 or so.</div><div>PCs aren&#39;t getting much quicker any more.</div><div>This means that the less-informed (or more specialist) media report rises in multicore performance &amp; GPU performance, which are irrelevant to most people. That blurs the picture -- most people, including in the PC industry, don&#39;t understand the difference.</div><div>It&#39;s also meant that PC sales have stalled. 2013 PCs aren&#39;t vastly quicker than 2008 PCs.</div><div>The few upgrades that make a real difference now are things like SSDs. Otherwise, capabilities now exceed the needs of typical users &amp; only improving in irrelevant ways (e.g. surplus RAM; surplus disk storage; surplus unusable CPU cores.)</div><div>Now, as Arthur says, it&#39;s turning into a replacement-only market.</div><div>Big deal, you may say -- but this fact alone will kill most of the players in the PC industry. It is an industry built and predicated on rapid performance increases leading to one of most rapid replacement cycles in hi-tech manufacturing. Like BA competing with Ryanair &amp; Easyjet et al, it /cannot/ transform itself into the rivals that are killing it.</div><div>Also note that in that same timeframe, *the* big PC S/W vendor starting doing backflips in an effort to force upgrades, and many of its efforts are duds -- e.g. Vista &amp; Win8. But more importantly, consider Office 2007 et seq: a needless &amp; ineffective UI revamp, a file format change, both desperate bids to make people buy new stuff. The same is happening on the server side -- Windows Server 2003 &amp; Exchange 2003 work as well as they ever did. But upgrading is a massive (and very expensive) PITA, so one might as well shrug &amp; outsource it to a hosted solution. Cheaper /and/ much easier.</div><div>Also see: storage capacity increasing to the point of irrelevance for most users.</div><div>&bull; PC OSes are complex &amp; require considerable maintenance.</div><div>However, now, the main use for PCs for most users is Internet access, which is technically quite easy by modern standards, now offers a rich environment, app-like functionality, personalisation, data storage, etc.</div><div>You don&#39;t need a PC for this. A phone or a tablet does it, is easier to use, and requires way less maintenance and tech savvy. PC technical types are mostly utterly blind to just how complex &amp; arcane PCs are. Yes, this includes Macs.</div><div>In fact, mainly thanks to Ubuntu, Linux is less work than either now. Both PC and Mac types are also blind to this as they cannot see past to-them essential tools and apps which Linux doesn&#39;t offer. But that doesn&#39;t matter, as we&#39;re entering a post-PC era.</div><div>Servers are as hard as PCs to run, but using virtualisation and datacentre hosting, they become disposable -- you keep a small herd of fungible tech staff, in 2 grades: [1] Windows grunts to do the basic 1st/2nd line stuff by remote control, tossing &amp; replacing broken VMs rather than fixing them. This is coming to desktops too, fast. [2] A small number of expensive *skilled* techies, increasingly running *huge* numbers of FOSS servers where licencing doesn&#39;t matter. Doing this makes servers relatively cheap &amp; easy.</div><div>Result: commercial OS vendors entrapped in a pincer attack: replaceable client devices, FOSS servers in the cloud, declining need for expert tech staff.</div><div>&bull; Older users as ever wedded to legacy kit &amp; methods.</div><div>This is just human nature. Few in IT are old enough to remember the last big transition -- to GUIs, 20-25y ago. Essentially *all* IT experts dismissed the new tech as a useless toy, a pointless distraction. Now, a precisely analagous transition is occurring again, but everyone&#39;s forgotten.</div><div>There&#39;s a lot of legacy tech that now seems essential to old-timers which are actually clunky, cumbersome distractions that can be discarded with little or no loss: windowing &quot;desktops&quot;; menus; hardware keyboards; pointing devices; expansion slots; removable media.</div><div>It is the late &#39;80s/early &#39;90s all over again. Then, ordinary users didn&#39;t need GUIs, GUIs were toys, that wasted power and time. Ordinary users didn&#39;t need multitasking. Ordinary users didn&#39;t need multimedia. But to get work done, people needed efficient text screens, they needed the performance of text-based software, they needed the power &amp; reliability of Big Systems with Big OSes.</div><div>Now, ordinary users don&#39;t need touchscreens. They don&#39;t need tablets. But they do need mice. (_Mice_, FFS.) They absolutely must have windows so that they can see more than one thing at once, side by side. The new devices are /unusable/ because they will get fingerprints on their screen. (Oh the horror!)</div><div>The majority of the players in the PC hardware industry cannot and will not transition to a small, slow, gradual-replacement-based sales model. Ergo, they&#39;re dead, like everyone from Apricot through Micropolis to Wabash and Xebec, who failed to make the last transition.</div><div>The PC software industry is similarly predicated on rich clients running rich local apps, with regular replacement based on much faster clients &amp; added features. That model is dead, too. And with it goes the model of expensive proprietary back-end systems running hardware-restricted licenced software. Both of those are as dead as minis and mainframes, they just haven&#39;t seen it yet.</div><div>When DVDs replaced videocassettes, everyone didn&#39;t throw out their VCRs overnight. It took years. Ditto CDs and compact cassettes &amp; LPs. Many cling on to their old media &amp; old players. I do myself. I&#39;m typing on a 22yo keyboard, almost totally surrounded by print books.</div><div>So sure, yes, today, tablet owners have PCs too.</div><div>But the PCs won&#39;t be replaced &amp; will eventually be thrown out. The toy tablets will get locked-down deskbound versions that will fill millions of office desks &amp; those offices won&#39;t /have/ server rooms or local servers or local IT tech staff.</div><div>But it won&#39;t be overnight.</div><div>And I confidently predict that every single case of &quot;essential&quot;, &quot;indispensable&quot;, tech will in a few years seem laughably retro to 99.9% of users. WIMPs will be as retro as a commercial Unix box running Motif apps on CDE is today: in other words, tiny numbers of users will pay lots of money to do it, but it will be utterly irrelevant to all but a tiny number of specialist users.</lj-cut></div><div>&quot;... a new scientific truth does not triumph by &nbsp;convincing its opponents and making them see the light, but rather because its &nbsp;opponents eventually die, and a new &nbsp;generation grows up that is familiar with it.&quot; (Max Planck, 1949.)</div><div>Well, IT is the same... only the old techies don&#39;t need to die, they just need to be promoted into management.</div>"
"eventtime","2013-12-19 14:24:00"
"url","http://liam-on-linux.livejournal.com/40892.html"
"userid","8744"
"itemid","158"
"event","Facebook readers may have noted my post yesterday, when I mentioned that I was trying to resurrect an old notebook with a dead screen by using a screenreader. I commented:

&quot;<span style=""font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; line-height: 18px; background-color: rgb(255, 255, 255);""><font color=""#37404e"" size=""2"">Just spent an hour trying to update a fresh install of Windows XP SP3 on a PC with no screen, using speech alone. Haven&#39;t felt so lost since 1988. It&#39;s currently on 100 of 125, though, which is a sort of success...&quot;</font>

<font color=""#37404e"" size=""2"">Well, I&#39;ve spent a little more time on it today. </font>

<font color=""#37404e"" size=""2"">According to http://update.microsoft.com I now have all essential updates installed. I&#39;m not feeling brave enough to tackle the optional updates just yet - I&#39;m still terrible at navigating web pages.</font>

<font color=""#37404e"" size=""2"">I&#39;ve also managed to install MS Security Essentials, and currently, Ninite claims to be installing Opera, OpenOffice and a FOSS PDF reader.</font>

<font color=""#37404e"" size=""2"">It&#39;s a very chastening experience. I am a dab hand with driving Windows without a mouse - I learned on Windows 2.0 in the days when my employers didn&#39;t own a PC mouse. But much of the XP and Windows apps&#39; UI is either inaccessible by keyboard, unreadable or just unlabelled. </font>

<font color=""#37404e"" size=""2"">For instance, stepping through the icons in the notification area, I get &quot;icon... icon... NVDA... icon... Automatic updates... clock.&quot; Selecting each icon and opening it is the only way to find out what it&#39;s the icon for. One gives the wireless network connection info, for instance, but some lazy-ass Microsoft programmer forgot to give it a text label.</font>

<font color=""#37404e"" size=""2"">The entire UI of the MS Security Essentials consists of the following:&nbsp;&quot;home... update... options... scan... exit.&quot; That&#39;s it. No legible text at all. I can open Task Manager and move between the tabs, but there&#39;s no way to sort the list of tasks to find what is hogging the system. That needs a mouse-click.</font>

<font color=""#37404e"" size=""2"">Progress bars are unreadable, but NVDA makes a series of rising beeps to tell you that something&#39;s happening. It&#39;s hard to tell how far you&#39;ve got, though. The mandatory Windows Genuine Authentication installer stops at about 80%, every time, even after 3 reboots. I gave up and used a third-party WGA killer app to nuke it into oblivion. </font>

<font color=""#37404e"" size=""2"">And I&#39;ve compared notes with </font><lj user=""ednun"" /> on this. Ubuntu seems to be about the best Linux for accessibility, with an integrated screenreader, Orca - but it can read<i> considerably</i> less than NVDA can. Windows does seem to be the best option.

It&#39;s quite scary. Certainly I&#39;m nowhere <b>near</b> being able to post status updates from a screenless PC.

(Weird font changes courtesy of the LJ rich-text edit control. Sorry about that.)</span>"
"eventtime","2013-12-16 14:47:00"
"url","http://liam-on-linux.livejournal.com/40632.html"
"userid","8744"
"itemid","157"
"event","<div style=""font-family: arial; font-size: small; line-height: normal;""><div>They may interest -- or at least amuse -- folk.</div><div>I actually meant this to be part 2 of a set of 3, but hey. </div><div>Working title: &quot;Some more of Ballmer&#39;s greatest mistakes&quot;</div><div>http://www.channelregister.co.uk/2013/12/12/feature_microsoft_caught_in_virtual_monkey_trap/</div><div>Part 1 (working title: &quot;Where Ballmer went wrong&quot;) is here:</div><div>http://www.theregister.co.uk/2013/11/14/microsoft_surface_rt_stockpile/</div><div>And what was meant to be Part 3 (working title: &quot;Who is this Ballmer person, anyway?&quot;) came out 2nd...</div><div>http://www.theregister.co.uk/2013/12/06/steve_ballmer_fondleslab_rear_guard_action/</div></div>"
"eventtime","2013-12-12 15:49:00"
"url","http://liam-on-linux.livejournal.com/40395.html"
"userid","8744"
"itemid","156"
"event","Just how threatening is BadBIOS, the virus that allegedly communicates through PCs&#39; mic/speakers?&nbsp;Bruce Schneier is <a href=""https://www.schneier.com/blog/archives/2013/11/badbios.html"" target=""_blank"">unsure</a>, but let <a href=""https://en.wikipedia.org/wiki/Rupert_Goodwins"" target=""_blank"">Rupert Goodwins</a> <a href=""https://www.facebook.com/lproven/posts/10151755739876691"" target=""_blank"">explain</a> [FB thread link]:
<blockquote>
<i>On the list of security issues to worry about, it&#39;s somewhere down there alongside sentient raspberry jelly evolving the ability to eat your flash drives and telepathically transmit your banking codes to a Mafia-controlled suet pudding.</i></blockquote>
&nbsp;"
"eventtime","2013-12-04 19:07:00"
"url","http://liam-on-linux.livejournal.com/40046.html"
"userid","8744"
"itemid","155"
"event","&quot;<span style=""line-height: 1.4;"">We took about 10 to 12 man years to do the Ivory chip, and the only comparable chip that was contemporaneous to that was the MicroVAX chip over at DEC. I knew some people that worked on that and their estimates were that it was 70 to 80 man-years to do the microVAX. That in a nutshell was the reason that the Lisp Machine was great.&quot;</span>

https://docs.google.com/file/d/0Bw4Wz8Ir0pl1cmNRaHYwdU1wdXM/edit

<span style=""line-height: 1.4;"">Source: </span>http://www.loper-os.org/?p=932"
"eventtime","2013-11-21 01:46:00"
"url","http://liam-on-linux.livejournal.com/39718.html"
"userid","8744"
"itemid","154"
"event","Linux on modern PC hardware is harder work today than it was say 5y ago. Also, the Linux desktop today is inferior to that of 5y ago, more splintered and incoherent, with lots of new tech and new desktops which are not generally well-liked by users. And the thing that nobody is spotting is that all this is a direct result of Microsoft&#39;s efforts over the last 5-6y.

As a result of Microsoft action, now we have:

&bull; UEFI
&bull; SecureBoot
&bull; Windows 8.x OEM deals that <i>require</i> the above

And on Linux:

&bull; GNOME 2 is no more; instead we have GNOME 3, Unity, Cinnamon, Mat&eacute;, Consort &amp; more.
<lj-cut><xml:namespace ns=""livejournal"" prefix=""lj""><xml:namespace ns=""livejournal"" prefix=""lj"">
The latter is, I submit, a direct result of Microsoft legal bluster

The Linux world has been fragmented in all directions and MICROS~1 didn&#39;t even have to find anyone to sue. Meanwhile, the x86 PC is now a much less friendly platform to non-MS OSes. The biggest commercial Unix - Solaris - has been castrated. The non-x86 Unix platforms are all failing, and ARM is not very Linux-desktop-friendly either; that is why Linaro even exists.

MICROS~1 has closed down the x86 ecosystem, wounding Linux, and it&#39;s wounded Linux directly too. It&#39;s also screwing Windows, but for good solid reasons - it is trying to adapt to both the Apple sales &amp; marketing model <b>and</b> to the coming tablet revolution. It&#39;s not doing very well but I can&#39;t see how it could have done any better.

We are where we are because a bunch of very smart, very motivated people at MICROS~1 sat down around the time of Vista and said to each other:

&quot;We are fucked. Apple is doing great, Linux is doing great, we&#39;re tanking. What can we do? How can we emulate what Apple is succeeding with (back then, the iPod and iTunes Music Store) and also knife Linux?&quot;

And they came up with a brilliant plan, executed it and it succeeded magnificently.

What they didn&#39;t see is the iPhone and iPad coming and they totally failed to respond effectively to that - but they are trying.

The Zune was a public experiment - a whole new model for MS media sales and DRM, a whole new UI, etc. It failed as a product but as an experiment it gave the desired results; the UI and tech was refined on the xBox 360 and then a big revision was tried with Windows Phone 7.

They binned all their old phone/media/mobile stuff, from Windows Mobile to ActiveSync, and built something new on an NT kernel - while trialling the new UI on the WinCE kernel as an interim move.

It&#39;s a smart, clever, well-planned, well-executed response to Apple and Linux.

It&#39;s not good enough, but it&#39;s as good as I can even imagine.

The stuff going on with Windows Server is also very good. Hyper-V and PowerShell and so on are very good tech.

I&#39;m not sure it&#39;s good enough - I think they&#39;re in for a long slow decline into irrelevance - but it&#39;s good.

Do not for a moment think that it&#39;s due to lack of effort from the Linux folk, though. It&#39;s not. The Linux lot have been outmanoeuvred, comprehensively and skilfully.

The snag is, both Microsoft and desktop Linux have been outmanoeuvred by Apple and Google. Apple has the commercial devices and online store and revenue stream; Google is playing catch-up there, doing it on the cheap using Linux and FOSS tech. And it&#39;s doing a very good job, too.

Google&#39;s model is &quot;we&#39;re rich, we&#39;ll give the tech away for nothing, to hinder our rivals, and then we&#39;ll make money off the ancillary stuff - ads etc. - later.&quot; That&#39;s working splendidly, too.

But nobody even noticed Microsoft&#39;s 2 biggest, very cunning moves against Linux, and Microsoft&#39;s moves against Apple and Google, while valiant, are not good enough and I suspect never will be.

Against Apple: a locked-down Windows on both x86 and ARM with merged desktop and touch interfaces - a single OS on desktop, tablet and phone - and an Apple-style online store. Embrace and extend, the classic move. Not working well, not yet, but it&#39;s early days. V3.1 will be when it all comes together. When that is remains to be seen - v1.1 just came out. (Zune, Windows Phone 7.x etc can be seen as equivalent to Windows 1.0 and 2.0 - trial runs that bombed but were half-expected to.)

Against Google: errrr, we&#39;ll copy what you&#39;re doing and give it away. Bing, Bing Maps, Outlook.com, WinPhone as against Android, etc. Trying to embrace and extend, but not quite making it. Probably near-total failure but the war&#39;s not over yet.

Against Linux: knife the most popular desktop, splintering the market: total success. Knife the ability to install and run on any arbitrary PC hardware: near total success. Knife the ability to run at all on Microsoft ARM hardware (as they don&#39;t control anything else): total success, but nobody cares as it&#39;s bombing.</xml:namespace></xml:namespace></lj-cut><xml:namespace ns=""livejournal"" prefix=""lj""><xml:namespace ns=""livejournal"" prefix=""lj""></xml:namespace></xml:namespace>"
"eventtime","2013-11-14 21:16:00"
"url","http://liam-on-linux.livejournal.com/39573.html"
"userid","8744"
"itemid","153"
"event","<span style=""line-height: 1.4;"">I think the more significant long-term question is to ask which of the various Gtk2-based desktops are going to successfully transition to other toolkits.</span>

Apparently, LXDE is switching to Qt:
http://blog.lxde.org/?p=1013

Which leaves the question of how easy it would be for Xfce and Mat&eacute; to move.
<lj-cut>
Mat&eacute; is GNOME 2, of course. GNOME stands for GNU Network Object Model Environment. It&#39;s a collection of components - objects - interacting over CORBA. I don&#39;t know much about its underlying usage of Gtk, which really isn&#39;t &quot;GIMP Toolkit&quot; any more but is now &quot;GNOME toolkit&quot;.

XFCE once stood for XForms Common Environment, although later it was ported from XForms (a rather elderly, simple API/widget set) to Gtk. Since it&#39;s been ported once, it *probably* doesn&#39;t depend too deeply and closely on Gtk, but that&#39;s a guess.

Perberos did talk about a possible move to Gtk3 long ago, but Mat&eacute; has really only gathered momentum since then:
http://archive.is/0rUi

This is my half-assed effort at translating the Spanish part:

&lt;&lt;
<i>Many people ask me: if the goal is to port MATE to Gtk3, then wouldn&#39;t it be better to just take the GNOME 3 programs and include them, as they are already based on GTK3?

The truth is that many applications have been ported to GNOME3 Gtk3, breaking Gtk2 compatibility. Besides that, they have been modified to make them easier to use: &quot;easy&quot; and &quot;cleaner&quot; in terms of usability means giving them dumber interfaces, which greatly limits intermediate and advanced users, as well as those who are used to the Gtk2 versions.

Also, if we only use GNOME 3 applications, then this fork of GNOME2 would be meaningless. But I hope that everyone can choose which applications they want to install.

It&#39;s a complicated discussion and does not lead anywhere, so let&#39;s avoid it.</i>
&gt;&gt;

He might change his mind.

I think he has good points.

It&#39;s quite easy to put both Mat&eacute; and GNOME 3 on the same install of Ubuntu - now Mat&eacute; has renamed all the packages, there are no clashes and they co-exist cleanly.

His point about dumbed-down apps is a good one. Some GNOME 3 apps look near-identical to their GNOME 2 versions, but most have no menu bar any more, just a single one-word menu with all the options collated onto that. I have mixed feelings about this: yes, it&#39;s simpler. As an old Acorn RISC OS user, I am perfectly comfortable with apps that have a single global menu, just divided into sections - there&#39;s less hunting around.

But GNOME 3 apps still have a menu bar, and that being so, I don&#39;t see any benefit.

Some apps have dropped features. That&#39;s a general trend of GNOME development and I don&#39;t like it. Yes, simplification is good, generally, but wholesale feature removal isn&#39;t the best way to do it. Making a fresh start (a la iOS or Android) is better, I suspect.

Bottom line:

I think Gtk2 is dead. I suspect Mat&eacute; and Mat&eacute;&#39;s developers are both too wedded to it and that means a vast project: the whole GNOME 2 desktop, plus all its applets, plus all of Gtk2. I don&#39;t think that can be kept going, but a move to Gtk3 would, as he implies, remove much of the desktop&#39;s reason for existing. So I think Mat&eacute; will stay on Gtk2 and eventually die.

For a flexible, modular, componentised desktop that you can reconfigure how you want - top panel, bottom panel, both, menu launcher, dock launcher, both, etc. - then Xfce 3 is a better bet. GNOME 3 does not try to be or do this and I doubt it ever will. GNOME Classic and Cinnamon don&#39;t try to be either, they just try to reproduce the default Windows-style taskbar-and-start-menu experience.

Xfce is looking at adopting Gtk3:
http://www.phoronix.com/scan.php?page=news_item&amp;px=MTM5OTA

But Gtk3 is not yet stable and each release (3.4, 3.6, 3.8&hellip;) has apparently had major revisions - 3rd-party users are struggling.
http://comments.gmane.org/gmane.comp.desktop.xfce.devel.version4/20865

Other projects are allegedly evaluating Gtk3 and deciding &quot;not yet&quot;:
http://forum.xfce.org/viewtopic.php?id=7598

As I said above, Gtk really is the GNOME Toolkit now, not the GIMP Toolkit. Gtk3 is the GNOME 3 Toolkit and it&#39;s changing so much with each release that it&#39;s not good for anyone else -- yet. GNOME 3 is still young and maturing. It has a long way to go.

So for now, I think Gtk2 has a modest future. Lots of projects use it and they won&#39;t all move to Gtk3 any time soon. It will survive for a few years yet and it is still safe enough to choose and deploy Gtk2-based solutions. Merely being based on Gtk2 is not reason enough to avoid any product.

But in time, yes, I think Gtk2 will die.

For now, though, Gtk3 is not mature or stable enough for widespread adoption among Gtk2 users. This is because Gtk3 is part of GNOME 3 and GNOME 3 itself is not yet mature or stable enough for widespread adoption. It may never be.

Perhaps, in a few years, GNOME 3 will survive, grow up, mature and settle down into something stable and consistent. In that case, Gtk3 will too. If GNOME 3 dies, then Gtk3 may well split off and itself become mature and stable, and then, the many Gtk2 projects will migrate to it.

My impression is, though, that it&#39;s too soon to move to it yet. Of the notable Linux desktops using Gtk2, I suspect that Mat&eacute; will soldier on with Gtk2 and never make the move. As long as Gtk2 is supported, that will be fine, but eventually, Gtk2 will die and if it hasn&#39;t moved on by then, Mat&eacute; will die with it. LXDE will switch to Qt and merge with the Razor-QT project. Xfce will switch to Gtk3 but probably not for a year or two at the earliest - it&#39;s a much slower-moving project than GNOME 2 was or GNOME 3 is. As such, if you want a customisable desktop like GNOME 2 was, my recommendation would be Xfce, and if you just want something simple, lightweight, clean and vaguely Windows-like, go with LXDE.

If all you want is a vaguely Windows-like window manager and you don&#39;t want desktop icons and file managers and all that, then IceWM and Fvw95 are still around and then all this stuff won&#39;t matter to you. :&not;)
</lj-cut>"
"eventtime","2013-10-28 16:19:00"
"url","http://liam-on-linux.livejournal.com/39353.html"
"userid","8744"
"itemid","152"
"event","I recently saw a mailing list post condemning Mat&eacute; (the GNOME 2 fork)
as something to be deprecated and avoided because it uses Gtk2 and
that is now superseded code.

I think that&#39;s a bit sweeping to&nbsp;denigrate all Gtk2 desktops like that.

Yes, GNOME Classic and Cinnamon both offer Windows-like desktops
now with taskbars and start menus. If you don&#39;t like Unity or GNOME
Shell, then there are &quot;traditional&quot; alternatives.

But the un-Windows-like nature of Unity and GNOME Shell are not the
only reasons that people use them.&nbsp;There are other issues than the
cosmetics to consider.
<lj-cut>
Mat&eacute; is a 2D desktop that can use compositing if you have it, as is XFCE.

GNOME 3 and its variants GNOME Classic &amp; Cinnamon are 3D compositing
desktops. They <i>need</i> hardware 3D to work; if you don&#39;t have it, they
fake it using software OpenGL rendering, which is CPU-intensive and
rather slow, especially on lower-end machines, single-core boxes and
VMs. This also means that if your OpenGL rendering is less than
perfect, they can experience significant display corruption - for
example, try running Ubuntu GNOME under VirtualBox, enabling VBox&#39;s
OpenGL passthrough and watch what happens to the display.

Unity is also a 3D compositing desktop, albeit using a different
technology (and planning to change to another one soon).

There are lots of scenarios where one might wish to avoid this:
* you don&#39;t have 3D hardware
* you have 3D hardware but no suitable driver support
* you are running inside a VM &amp; the hypervisor only offers 2D virtual graphics
* you are running inside a VM with 3D passthrough but the hypervisor&#39;s
3D support is not sufficiently compatible
* you are running inside a VM &amp; the host doesn&#39;t have 3D hardware, so
software rendering would impose an unacceptable system load
* you are running on older hardware which doesn&#39;t do full OpenGL
* you are running on older hardware which does full OpenGL but only at
a low resolution
* you are running on older hardware &amp; the CPU load of software OpenGL
is unacceptable
* you have 3D hardware but don&#39;t want to use it:
** you want to remotely-control your machine over the network
** you use accessibility tools to run your machine &amp; they&#39;re incompatible

Etc. etc.

As a hint, I have personally experienced more than half of these.

My list is not comprehensive - I welcome corrections and additions.

Because of these and similar factors, there is a strong argument that
there&#39;s a place for 2D desktops which do not need or rely upon
compositing or OpenGL. Happily, there is a choice of them:

* Mat&eacute;
* Xfce
* LXDE
* KDE
* ROX
* GNUstep
* Enlightenment
* Etc. etc.

Also note that for Ubuntu users, used to Gtk apps, 3 of these are
based on Gtk and so have or can have a familiar look &amp; feel that&#39;s
consistent across the normal Ubuntu apps:

Mat&eacute;, Xfce and LXDE.

And guess what? They&#39;re all based on Gtk2.

That suggests to me that for now, Gtk2 has enough users and developers
that it will stay in support for some time to come.

To write it off as dead is thus premature just yet, I think.</lj-cut>"
"eventtime","2013-10-26 13:16:00"
"url","http://liam-on-linux.livejournal.com/38949.html"
"userid","8744"
"itemid","151"
"event","<span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">Not only do I have recent, decent-performance, still-perfectly-usable </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">PC hardware that can&#39;t boot off USB, or can but can&#39;t <i>remember</i> the </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">setting* so that it has to be done every time you need it, but I also </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">note that the BIOS in the current shipping versions of both VirtualBox </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">and VMware cannot boot from USB devices.</span>

<span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">It is <i>not</i> a rare or uncommon problem.</span>

<span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">Yes, I have had dozens of techies say they&#39;ve never seen it. Well, </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">tough. It&#39;s not rare; it just means that they&#39;ve had a lot less </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">breadth of experience than I have.</span>

<lj-cut>
<span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">-----</span>

<span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">* Some BIOSes &quot;see&quot; a USB key as a hard disk. You can place it at the </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">top of the boot order, but if you reboot the machine without the key </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">present, the BIOS sees that that specific disk device has disappeared </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">and removes it from the boot device list. This also means that <i>every </i></span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);""><i>different USB key comes up as a different device</i> meaning that it is </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">impossible to set the BIOS to boot from USB first, then rotating </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">media.</span>

<span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">Also, some BIOSes offer a choice of USB HD, USB FD or USB CD - all as </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">different, mutually-incompatible devices. Go on then, which is a </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">memory stick - a virtual hard disk, virtual floppy disk or a virtual </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">optical disk?</span>

<span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">Hint: it depends on the partitioning, not the hardware.</span>

<span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">Floppies aren&#39;t partitioned; the BIOS has to look for a boot <b>sector</b> </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">at the start of the media.</span>

<span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">Hard disks are partitioned (except in RAIDs, sometimes), so the BIOS </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">needs to look for the master boot record, find the bootable partition, </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">locate that, find <b>its</b> boot sector and then load <b>that</b>.</span>

<span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">Optical drives, including fake ones, are ISO9660 filesystems or a </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">variant thereof, which means a 3rd method. Except rewritables, which </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">could be UFS, meaning universal filesystem not Unix File System as </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">that TLA means under BSD-like OSes.</span>

<span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">Oh, yes, and on some manufacturers&#39; kit, *their own-brand USB </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">removable drives* appear as system-integrated (e.g. PATA/SATA) ones to </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">the BIOS, whereas generic USB removable drives don&#39;t and are seen as </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">USB devices. IBM/Lenovo are good at this, but it&#39;s not unknown on HP, </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">Dell and other kit. Install a Lenovo USB floppy drive on a Lenovo </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">notebook, &nbsp;the BIOS makes it appear to be on the floppy controller. </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">Put the same drive on a Toshiba, it doesn&#39;t work; put a Toshiba drive </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">on the Lenovo, it works fine but as a USB drive and although MS-DOS </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">boots, it then can&#39;t see the floppy drive it booted from and your BIOS </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">reflash fails.</span>

<span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">Getting the impression that I&#39;ve fought a lot of battles with bootable </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">USB media yet?</span>

<span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">I&#39;ve fought a <b>lot</b> of battles with bootable USB devices.</span>

<span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">Including with the PLOP boot manager, which on some PCs, can only </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">access certain USB ports and not others.</span>

<span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">There&#39;s a reason I like CDRs and DVDRs. They&#39;re cheap and they </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">generally just work and anything younger than a Pentium II can usually </span><span style=""color: rgb(0, 0, 0); font-family: arial, sans-serif; font-size: 13px; line-height: normal; background-color: rgb(255, 255, 255);"">boot off them.</span></lj-cut>"
"eventtime","2013-10-22 20:13:00"
"url","http://liam-on-linux.livejournal.com/38665.html"
"userid","8744"
"itemid","150"
"event","&gt; In short, the more somebody sounds off about a language or OS, the
&gt; less they should be trusted. But I think you&#39;re reaching the same
&gt; conclusion.

Well, yes...

&gt; First, from a &quot;wisdom of the ancients&quot; POV: what have we lost out
&gt; on?

That&#39;s what I am trying to work on elucidating, understanding and finding
out how to explain.

In general, because I don&#39;t yet know enough to have any alternative, I
have no recourse except to be vague and hand-wavey:

What we have now are very fast, very capacious, very very stupid
computers. What ought to be arcane internal concepts are exposed at the UI
and users have to learn to manipulate them: files, folders, file *types*,
documents, binaries and executables, source code, interpreters versus
compilers, and so on.

As Stanislav Datskovskiy put it in http://www.loper-os.org/?p=55 :

&lt;&lt;
The computers we now use are descended from 1980s children&rsquo;s toys. Their
level of bedrock abstraction is an exceedingly low one. This would be
acceptable in a micro with 64K of RAM, but when scaled up to present
proportions it is a nightmare of multi-gigabyte bloat and decay.
&gt;&gt;

<lj-cut>We run a big program which we use to navigate among a collection of
different, separate, smaller programs, which we run and then use to open
data files, manipulate their contents, then save them. Then we use client
programs to send messages over network protocols to remote servers.

And ordinary people are expected to keep track of all this, to learn these
concepts and become fluent with them.

To nerds and geeks, it&#39;s easy, and we have become so used to this stuff
that we are blind to the fact that it is /profoundly/ arcane and abstruse.

What we have are not the descendants of the big powerful multiuser systems
of the 1960s and 1970s. No, what happened was that the tech got cheap
enough for tiny, rudimentary, simplistic, brain-damaged versions to be
sold as toys. And then those toys have grown and been enlarged and
expanded and made thousands of times faster, without learning any of the
lessons of the computers from pre-toy times.

Those big &#39;60s and &#39;70s machines were horribly clunky, yes, but they had
solved (or at least had answers to) important problems to do with
security, permissions, parallelism, portability and more, none of which
have come across and which are being re-invented on all sides today.

Even obscure failed micro operating systems made important advances that
have been ignored. There is no technical reason why executable programs
should be tied to an underlying CPU architecture; Taos showed this. There
is no technical reason that it should make any difference what machine on
a network a resource (storage, processing, data) is on; that kind of thing
should be transparent to the user. Perhaps not to the admin, no, but users
should not have to know. Plan 9 demonstrated this, and Inferno added
Java-style binary portability to the mix. They still failed.

If the user needs to know about file formats, about markup, about server
names or what program handles what file, about how to move information
between programs, about what kind of computer can run what kind of
program, about any of this kind of low-level techie wank, then we have
failed.

And this means we have all failed. We have all worked at some point in the
chain between making chips and delivering products to buyers. And we have
singularly screwed the pooch. We&#39;ve taken the easiest, simplest, stupidest
way and delivered sad broken crap that has not actually improved in what
it does since about 1975. All it&#39;s done is got cheaper and quicker.

So don&#39;t ask about what will it do better that my current computer does.
Don&#39;t ask about performance. Performance is not the criterion that
matters. It&#39;s like judging a pen by how fast it can lay down ink:

http://www.loper-os.org/?p=300

Now, doubtless, you&#39;ll want concrete examples and I can&#39;t give them, not
yet. I&#39;m still working on it.

But moving aside from the LispM model, which is a sort of information
appliance for programmers, there&#39;s Jef Raskin&#39;s work on the Canon Cat and
the Swyft UI:

http://www.jagshouse.com/swyft.html

http://oldcomputers.net/canon-cat.html

It doesn&#39;t have programs, it doesn&#39;t have files, it has no distinction
between OS and apps. If you type text, you can format it and lay it out.
If you type expressions, you can evaluate them. If you type a grid of
numbers, you can apply operations to blocks of them (i.e., what a
standalone spreadsheet does).

As planned, it would be able to send data to other people, and you could
obtain new methods and capabilities from other vendors and integrate them
into your system... but Canon crippled or removed all that.

A richer, more sophisticated example was the Apple Newton, conceived of as
a pocket info appliance. The original design for the Newton was a pocket
Lisp Machine, but with an algebraic-notation Lisp called Dylan with an
insanely rich desktop development UI that makes Visual Studio look like
Edlin.

It got canned. They then came up with the much simpler NewtonScript and
wrote it in that.

No files, no folders and a UI driven by a pen and natural language. Go to
the &quot;do&quot; box and write &quot;lunch with bob&quot; and it works out, from what you&#39;ve
done in the past, when you take lunch, who the &quot;Bob&quot; that you most
frequently contact is, adds an appointment in your diary shared with him
and sends him an email with the times in.

Or draw a flowchart and it turns your wobbly freehand lines into neat
crisp shape objects, manipulable with the pen.

Or write text and have it become editable print.

Or any mixture.

In conception, it makes Windows 8 look like a VIC20.

So the big question is, why did all this stuff fail? Why did we get
scaled-up CP/M boxes instead, running &quot;a knockoff of a 1970s operating
system well past its sell-by date.&quot; (http://www.loper-os.org/?p=69)

Well, partly, because it was slower and less efficient.

But that doesn&#39;t matter that much any more, because now we have vastly
more power and most of it&#39;s going unused in idle CPU cores. Now, the
dreams of the 1970s and 1980s are eminently doable, whereas we don&#39;t have
any big fresh new ideas. We&#39;re reduced to stripping out all the complexity
from our desktop OSes to make them fathomable to ordinary people by
stroking little glass slabs without hardware buttons.

&gt; Second, from the POV of machine architecture, the dominant
&gt; architecture is, compared with some of the older alternatives,
&gt; somewhat RISC-like.

&quot;The aim of Loper is to build a sane computing environment on top of the
ubiquitous yet nauseatingly flawed X86-64 architecture. I believe that it
is possible to abstract away its most damning shortcomings, such as the
lack of direct hardware support for capabilities, orthogonal persistence,
type-checking, and garbage collection.&quot;

http://www.loper-os.org/?p=33

&gt; The Big Question is whether something based on
&gt; e.g. an internal Lisp organisation could be equally efficient at
&gt; representing code fragments, and at least as efficient when it came
&gt; to representing a rich OS and application bundle.

Mu.

While it&#39;s an interesting question, I&#39;m not sure it&#39;s entirely a useful
one. We have what we have; let&#39;s see if there&#39;s a smarter way to use it.

&gt; Third, from the POV of the existing code corpus, there&#39;s a vast
&gt; amount of legacy C and C++ code around, much of it in daily use,
&gt; and most unmaintainable except by the original author. There&#39;s a
&gt; smaller amount of BASIC, Pascal, APL, Prolog and so on. The Big
&gt; Question

What, another one? :&not;)

&gt; is whether something based on e.g. an internal Lisp
&gt; organisation could be unconditionally compatible with the existing
&gt; corpus, or at the very least with stuff that was expected to run at
&gt; an application level (i.e. ignoring device drivers and so on).

It possibly could - LispMs ran other languages - but I am not proposing
replacing our current CPUs. I&#39;m proposing a different kind of software
environment that replaces our current OSes and apps. It would almost
certainly be worse at doing the sort of stuff current OSes do, and its
likeliest role for most people might be running in a VM under Linux or
something, with the host OS doing mundane stuff like browsing the WWW and
editing MS Word files.</lj-cut>"
"eventtime","2013-10-19 19:13:00"
"url","http://liam-on-linux.livejournal.com/38539.html"
"userid","8744"
"itemid","149"
"event","The only reason why the language should intrude into the discussion is the side-question of whether certain language facilitate or hinder certain types of work.

Look at it from a different angle. For many a working programmer (and writer, believe me), the WWW is a massive distraction. A necessary one, but one that is separate.

What do you need to do, if you are co-working on a significant project? Edit code, obviously. Save it, compile it, run it, probe it with debuggers. Navigate your filesystem, load and view other files, move stuff between them. Occasionally, read and write email, or IRC, or (more historically) newsgroups, to discuss what you&#39;re doing. Possibly retrieve files from remote servers or put them there.

The point being, Emacs has extensions to do all this, so that you can do it all in a consistent fashion in a consistent (if horrible) UI, so that you can spend your entire day inside a single Emacs session and never leave and thus mentally never have to change gear.

Emacs, for some of its fans, is their entire OS. Their computer runs something that lets them launch Emacs and has an accessory function of browsing the web.

I don&#39;t do this - I don&#39;t speak Emacs at all - but I know people who do and really like it.

Well, turn that inside out. Consider an OS whose sole purpose is to do this: a Lisp interpreter running on the metal, which runs Emacs, and inside that, you have all the other functions. No distinction between &quot;OS&quot; and &quot;apps&quot;, or between different apps. The OS runs the HLL you&#39;re writing natively, so there&#39;s no interpreter or compiler or linker. Everything you see is drawn by your editor and is live code that is executing in the environment - you can, if you wish, tweak the email function or the file manager to your taste, or if you want, go grab a new one off the Internet and plug it in instead.
<lj-cut>
The environment is modular and extensible by design, so if you want a spreadsheet function, you grab a file off a remote server onto your machine and suddenly you can now manipulate grids of numbers. Want charting? Grab a charting module. No binaries, no installing, no libraries - it&#39;s all one big (multithreaded?) program.

The online help is another program - you can use a shell to search its database, or edit it, or view it, as you wish.

That&#39;s a half-assed crappy description of the LispM environment at my current meagre level of understanding. Everything&#39;s live, it&#39;s all right there to be inspected and called upon.

Compare with the Unix model: lots of tiny VMs, some big, some small, calling static compiled binaries which link to thousands of other static compiled binaries. Much quicker, but everything is walled off from everything else by design. They&#39;re all written in different languages, all configured by different text files in different formats in different places. Yes, there&#39;s a `wc` command to count words in a plain text file, but it can&#39;t do it on a Word file, and if you install LibreOffice, it has a different word count function, and if you install Abiword, it has a different one too. You can&#39;t use Abiword&#39;s functions in LibreOffice or vice versa - they&#39;re separate binaries. Both use Gtk so they look alike, unlike KWrite which reduplicates all that functionality on a different set of foundation libraries - all 3 in C++ but that makes no difference at all.

If you install GNUstep and get Ink, it can&#39;t interoperate with them or share functions, and it&#39;s written in Objective-C.

Other bits of your OS are in Ocaml and Perl and Python and Ruby and shell and Tcl and who knows what, some interpreted, some compiled.

And while there are elaborate systems of protocols for GNOME apps to communicate with different GNOME apps, they don&#39;t talk to KDE apps, and neither talk to GNUstep apps. All have to drop down to a very basic level of folders with text files in, because that&#39;s the highest abstraction that they share.

And that structure of text files is different from the ones that Windows and Mac OS use. It&#39;s even a bit different to the different ones that FreeBSD and NetBSD and OpenBSD and DragonflyBSD use. So different that you can&#39;t even use maildir on Windows, because Windows&#39; filesystem conventions are too different and prohibit it, so no cross-platform mail client like Thunderbird can use maildir because it&#39;s incompatible with the dominant OS.

It&#39;s a really simple, clear level of abstraction, but it&#39;s /old/ and very very limited - so each different implementation of it does it differently.

And now that we have rich GUI layers on top, they&#39;ve reinvented it all again, multiple different times, so now, there are umpteen different desktops and they barely talk to each other.

It&#39;s simple and it&#39;s efficient and there was a time when that was all-important, so once these simple OSes could do the fancy stuff that LispMs and Amigas and Macs etc. could do, they out-competed them.

Then they got into a bunfight of trying to outcompete each other, and Linux came along and swept all that away with Free, open code and just outcompeted the hell out of everyone. All the proprietary ones are now slowly dying: vestiges live on on servers, nothing else.

Windows is going to go the same way next. Apple will follow once the free stuff is good enough.

And what we&#39;re left with is an arcane agglomeration of weird crusty old Unix stuff:

&lt;&lt;
&ldquo;The Jolitzes believed that from its origins as a series of quick, if elegant, hacks, Unix had hardened into a series of unquestioned rituals for getting things done. Many of these rituals were fossils &mdash; work-arounds for hardware that no longer existed. &ldquo;It&rsquo;s amazing how much we were still tied to the past,&rdquo; Lynne says. &ldquo;The physical machines had been hauled away, but elements of the operating systems are still being implemented in the same way.&rdquo; She believes today&rsquo;s Linux and BSD developers are carrying on that unhelpful tradition.&rdquo; &ldquo;It&rsquo;s like they&rsquo;re doing incantations,&rdquo; she says. &ldquo;They&rsquo;re repeating what they&rsquo;ve been taught, and they don&rsquo;t know what it means.&rdquo;
&gt;&gt;

&quot;Unix, the Living Anachronism&quot;: http://www.loper-os.org/?p=34

The people who /invented/ Unix moved on, 30y ago - but the inertia of Unix was already too strong, and Unix 2 (called Plan 9) and Unix 3 (Inferno) failed to attract any momentum.

In the mid-1980s, another trendy idea nearly changed things: microkernels, carving Unix up into manageable chunks. There have been 3 notable efforts that escaped academia and obscurity.

#1: Mach

Widely adopted at the time; only one version prospered, NeXTstep.

Not really a microkernel any more: to achieve Unix compatiblity, they built most of the BSD kernel into their microkernel as a &quot;Unix server&quot;, meaning the result is just a slightly odd Unix.

#2: QNX

Unix-like, proprietary, hugely commercially successful in embedded systems but minimal impact on the wider market. Nearly happened 3 times over: [1] it was nearly the basis of the next-generation Amiga, then [2a] it was the basis of RIM&#39;s doomed tablet, and now [2b] it&#39;s the basis of Blackberry Inc&#39;s really rather good smartphone - but just not Good Enough to successfully attack the entrenched market leaders Apple and Android.

#3: Minix 3

A real live working FOSS microkernel Unix from the guy who originally pointed out that &quot;Linux is obsolete&quot;.

Minix 3 is what the GNU Hurd tried and failed to be, only it&#39;s finished and it works fine.

Very clever, but too late to make a real difference. Part of me would like to see Minix 3 replace Linux in the fullness of time, when the kernel becomes just unmaintanable - but unfortunately, tools are advancing to keep it viable to manage such an obscenely huge monolithic bolus of C.

So monolithic Unix has lumbered on, a very good metaphorical approximation of Frankenstein&#39;s Monster: a stitched-together patchwork of different bits, ugly but so big and strong that it is thriving.

The next best thing to The Son of Unix was BeOS. A brave attempt, to keep the notions and concepts of Unix (like Posix and the shell and so on), couple them with the open hackability and rich media support of the Amiga, and the graphical attractiveness &amp; simplicity of the Mac. It was brilliant: fast, sleek, gorgeous, friendly and easy. I loved it.

But it too failed, partly because MS stabbed it in the back (the Hitachi bundling deal and strong-arm tactics over OEM licensing, a dirty little episode now mostly forgotten) but mostly because although it was good, it wasn&#39;t good /enough./

If Apple had bought it, Apple would be a fading memory now.

What RIM, sorry, Blackberry is doing with QNX now seems to me to be a very good parallel to what Be tried and failed to do. It&#39;s very good, but it&#39;s not so astoundingly good as to be compelling.

*Nothing* is strong enough to bring down Torvalds&#39; Monster, but the Monster *is* strong enough to bring down the other shambling behemoths: Windows NT and The Son of Mach, Apple OS X and iOS.

We&#39;re heading for another monoculture, but this time, a FOSS one. It&#39;s an improvement, I guess, but not much of one.

And I don&#39;t think anyone can outdo Linux by trying to be better at being a Unix-type system. I don&#39;t think it&#39;s possible. Haiku is a FOSS BeOS, a better small sleek media-friendly GUI desktop OS - but Linux is good enough at that. OS X is better at being a smooth, easy, friendly, desktop Unix - but Ubuntu actually brings you most of that, for free, but as per /In The Beginning was the command line/, they literally can&#39;t give it away.

&lt;&lt;
These are not old-fashioned, cast-iron Soviet tanks; these are more like the M1 tanks of the U.S. Army, made of space-age materials and jammed with sophisticated technology from one end to the other. But they are better than Army tanks. They&#39;ve been modified in such a way that they never, ever break down, are light and maneuverable enough to use on ordinary streets, and use no more fuel than a subcompact car. These tanks are being cranked out, on the spot, at a terrific pace, and a vast number of them are lined up along the edge of the road with keys in the ignition. Anyone who wants can simply climb into one and drive it away for free.
[...]
The group giving away the free tanks only stays alive because it is staffed by volunteers, who are lined up at the edge of the street with bullhorns, trying to draw customers&#39; attention to this incredible situation.
&gt;&gt;
http://pauillac.inria.fr/~weis/info/commandline.html

Windows is trying desperately to jump over the fence and be a pretty, smoother, more integrated Android but with Apple&#39;s device-plus-online-store model. It&#39;s failing at both.

In the form of Android, Linux is out-iOS-ing Apple and out-fondleslabbling Microsoft.

It&#39;s the Thing, it&#39;s the Blob. Whatever you do, it can just copy you and do it well enough. It is Legion and it spawns Pod People at a vast rate.

But it&#39;s not pretty or elegant or small or streamlined. It&#39;s a huge Lovecraftian agglomeration with eyes and mouths and other distressing moist orifices all over, but the cheap commodity hardware has come up to meet it and now it runs well enough to be pleasant on hardware that costs &pound;25 for the main board and &pound;40 for a whole tablet.

So if you can&#39;t take it on at its own game, is there any other way to take it on? What does it not do? What&#39;s it bad at?

Well, it&#39;s very big and complex and arcane. Something smaller and simpler that doesn&#39;t do lots of different things, but does one thing really well, that might work.

It&#39;s composed of lots of different types of system - a modern desktop Linux contains bits written in literally dozens of different speciality languages, all very good at their one narrow purpose and quite poor at other things, some compiled, some interpreted.

So that might be a clue: you might want to use a language that is versatile and excels at mutating on the fly to make little sub-languages that are good for their narrow task. Domain-specific languages, in other words.

It&#39;s a Unix, meaning that it&#39;s a system that excels at juggling lots of isolated processes, all sealed away from one another to variable degrees. That&#39;s how it works.

So perhaps, a response might be a system that does the reverse: that keeps everything inside one process, written in one language, and shares code as much as possible.

Linux is the ultimate embodiment of the config-file based system: where all state is kept in myriads of little plain-text files, which are read at bootup or when processes are launched and determine how everything hangs together.

And yet, at the moment, I&#39;m refurbishing some 1980s classic 680x0 Macs, which are usable multitasking GUI computers, with rich apps and are rudimentarily WWW-capable, which don&#39;t have a single configuration file on the entire OS. Not one, anywhere. Nothing. Lots of metadata, yes, but the OS manages a database of it, invisibly. Developers need to handle it, but users cannot even see that it is there.

I&#39;m not saying this is a better way, but it certainly shows that something that is utterly integral to Windows, Unix and OS X in all their forms is not actually necessary. You can do without it and the result is better for the user.

But the Mac has a filesystem, binaries and data files, programs and libraries, and it doesn&#39;t multitask well. The Newton dispensed with those concepts as well and worked very well without them - in a far smoother, more attractive way than the iPad does today.

I cannot point to something and say &quot;look, this is what we should do.&quot; But I can point to various elements of various systems and say &quot;look, they did this stuff better than anything we have today.&quot;

But when I do, I get pooh-poohed. There is this entrenched belief that we live in the best of all possible worlds: that modern computers are wonderful and just getting better.

And they really are not. The emperor really is stark naked, but nobody will listen.</lj-cut>"
"eventtime","2013-10-19 19:04:00"
"url","http://liam-on-linux.livejournal.com/38170.html"
"userid","8744"
"itemid","148"
"event","A chap on CIX responded to my last piece on Lisp, and it led to a long answer, which my CIX client then crashed and threw away. So if I have to rewrite it, I&#39;ll do it here and it will maybe be read by a few more people. Perhaps, ooh, a dozen.

&gt; Trouble is, it comes across a bit as a &quot;lost wisdom of the ancients&quot;
&gt; story.

Yes, it does. But I am OK with that, if I can turn it into a coherent article that tells a comprehensible story.

&gt; Lisp was a niche language in 1960, it&#39;s a niche language today. It
&gt; has been a niche language for all the intervening period and I expect it
&gt; to be a niche language for all time to come.

It&#39;s a fair point, but there are ways around that. One of the problems, though, is that the Lisp community are very resistant to them.

The thing that my research and my various discussions online are leading me to believe is this:

There are many things about Lisp that used to be distinctive, powerful features decades ago &ndash; not merely the functional programming model, but lambda calculus, closures, higher-order functions, tail recursion, lazy evaluation and so on. However, today, other languages can do these things. Perhaps some can do all of them, others only a subset, but that doesn&#39;t matter if these are the tools you need to crack your particular problematic nut. And the other languages that include these features do not have the feature that is the biggest problem with Lisp: its obfuscatory syntax, or as the Lisp advocates would have it, its *lack* of syntax. (Of course, as in the case of Perl, for example, they may have their own obfuscatory issues.)

But the problem is that that syntax is both the biggest obstacle to learning and using it, and yet at one and the same time, also absolutely integral to the one feature that sets Lisp apart from pretty much all other languages: its syntactic macros.

<lj-cut>Lisp manipulates structured lists. Lisp code comes in the form of just such structured lists. Therefore, it is possible to write Lisp code that transforms parts of itself as required to solve particular problems. It seems to me that what sets the various Lisps apart from other languages are Lisp macros.

This seems to be the core of the thing. The power of Lisp is in its macros; you need the syntax to make the macros work; so even though the syntax is really hard to deal with, you do so anyway, because it facilitates the macros. This seems to be an article of faith among Lisp programmers, as discussed by Xah Lee here (among other places):

http://xahlee.info/comp/lisp_syntax_vs_perl_syntax.html

As he says, it&#39;s like a religion, or more to the point, a cult.

And Lisp macros are really hard to explain. I can illustrate this a bit, albeit perhaps poorly, by unpacking a blog post in reverse.

Here, on the original Ward Cunnigham Wiki, is an interesting if flawed attempt to explain Lisp macros. It&#39;s long and it&#39;s not entirely successful but it&#39;s one of the better ones I&#39;ve found so far.

http://c2.com/cgi/wiki?LispMacro

The trouble is, it doesn&#39;t entirely work. This frustrating inability to convey what they are, how they work and more to the point, why they are so powerful seems to be a familiar sensation to Lisp gurus - for example:

https://news.ycombinator.com/item?id=645338

Which finally leads me to the blog post that is the source of those links:
http://www.loper-os.org/?p=401

It makes some good points, but I am not sure that it will stand alone very well unless you&#39;ve read a lot of the other posts on that site - which I have now. It&#39;s not very long - worth a go.

So this is one of the key things. Lisp&#39;s biggest weakness is also Lisp&#39;s biggest strength, in a sort of demented yin-yang relationship which erects a high wall around the whole language &ndash; and it doesn&#39;t really help to try to persuade people of how wonderful it would be if they only climbed that wall.

And that is what is sometimes called the Lisp Curse:

http://www.winestockwebdesign.com/Essays/Lisp_Curse.html

Or, expressed more prosaically, in the title of another Stanislav Datskovskiy blog post:

&quot;Where Lisp Fails: at Turning People into Fungible Cogs.&quot;
http://www.loper-os.org/?p=69

Which incidentally has a wonderful footnote:

&lt;&lt;
[1] Ever wonder why it is customary to speak of the wildly different Common Lisp, Scheme, ZetaLisp, Emacs Lisp, etc. as though they were a single language? I would like to suggest that we start referring to C/C++, Java, Python, etc as ALGOL.
&gt;&gt;

- - - - - -

But the thing is, it&#39;s becoming increasingly apparent to me that actually, this Big Problem *is* fixable. Yes, Lisp&#39;s prefix notation makes it much easier to write macros which can manipulate and transform Lisp code, but you *can* actually write a set of macros that transforms Lisp into an infix-notation, algebraic style language. And if you do that, it suddenly becomes quite a clear, simple, readable one.

And once done, while it becomes harder to write syntactic macros, it is still possible.

There have been multiple attempts to do this, again, as documented by the estimable Xah Lee:

http://xahlee.info/comp/lisp_sans_sexp.html

Since then, I&#39;ve rewritten the Wikipedia article on CGOL to make it clear that it doesn&#39;t only run on the obsolete MacLisp dialect but was also ported to Common Lisp:

https://en.wikipedia.org/wiki/CGOL

I&#39;ve also discovered a new one that I was previously unaware of &ndash; PLOT: &quot;Programming Language for Old Timers&quot;.

Discussion: http://lambda-the-ultimate.org/node/3253
Homepage: http://users.rcn.com/david-moon/PLOT/

PLOT was devised by one of the less-well-known old-time Lisp gurus, David Moon, meaning it has an impeccable pedigree.

The discussion page there talks a little bit about how an algebraic-notation language can still have syntactic macros. This page, from one of the other recent projects to give Lisp a more familiar, readable syntax, goes into more depth:

http://www.dwheeler.com/readable/

He has a more specific discussion about the syntax issues here:

http://www.dwheeler.com/readable/retort-lisp-can-be-readable.html

And a more recent presentation as a movie, here:

http://readable.sourceforge.net/

- - - - -

(I said this on Facebook a while ago, in the discussion here: https://www.facebook.com/lproven/posts/10151634320686691 )

So, one possible answer to the problem of Lisp being such a niche language might be, much though the Puritan Lisp illuminati scorn it, to incorporate one, or better still all, of these approaches to making a more readable, more familiar, infix-notation Lisp.

After all, JMC himself only ever intended S-expressions to be the low-level language. But it became apparent that some Giant Brains are able to learn to work in S-expressions and need nothing more. Good for them. Mere mortals, I fear, need something more.

There are at least 4 candidates that readily spring to mind as simpler meta-languages that could be implemented on top of raw Lisp.

#1 CGOL - (as just mentioned and linked to above)
#2 Logo - https://en.wikipedia.org/wiki/Logo_(programming_language)
#3 Dylan - https://en.wikipedia.org/wiki/Dylan_(programming_language)
#4 PLOT

I put CGOL first because it already exists; it&#39;s there, it&#39;s freeware and it works on Common Lisp.

Logo is simple enough for children -- and me -- to grasp.

And Dylan, I think, is possibly the single biggest missed opportunity in the history of pre-NeXT-merger Apple.

All were bootstrapped on Lisp before. All are FOSS. It&#39;s possible.

Sorry if this is a bit disjoint. It&#39;s an attempt to recreate a long, more coherent post that was lost. Best I can do.</lj-cut>"
"eventtime","2013-10-19 02:57:00"
"url","http://liam-on-linux.livejournal.com/37931.html"
"userid","8744"
"itemid","147"
"event","<span style=""line-height: 1.4;"">I guess that it all stems out of a vague feeling of ennui that&#39;s been growing in me for years concerning computers.</span>

My Spectrum was an amazing toy (and I do use the word advisedly). I played with CBM PETs and ZX-81s but while interesting they could not do pictures or sound, which were things of more interest to me around 12YO or so. The Spectrum delivered sound, pictures, and a usable BASIC (I switched to Beta BASIC quite early on) at a price well below anything else. The VIC20 was too limited, the C64 had great hardware but a crappy BASIC, the Acorn 8-bits were vastly too expensive, and so on.

Then I got a job and could afford a used Archimedes. Simple, comprehensible OS, *really* good BASIC, wonderful graphics and sound beyond my meagre abilities to exploit and vast CPU power. As the late gkewney@cix said of the IBM PC-AT: &quot;my first experience of Raw Computer Power&quot;. Well, for me it was the Archimedes, and dickp@cix&#39;s review of it in Personal Computer World was a clincher.

(You can read that here and I recommend it. It&#39;s one of the few computer reviews ever to contain quotable lines: http://acorn.chriswhy.co.uk/docs/Mags/PCW/PCW_Aug87_Archimedes.pdf )

Then I went x86. Horrible Byzantine OSes, a wide choice of programming languages but nothing that delivered the simple benefits of BBC BASIC, and I quickly lost interest in programming as a result.

What follows is 20Y of supporting the things instead.
<lj-cut>
From the late 1980s to the late 1990s, there was a lovely trend: steadily improving capabilities, significant removal of bottlenecks, and regular addition of new facilities.

For me, as someone who made his living supporting Windows, this basically ended in 2000. Windows 2000 had all the multitasking power of NT /and/ all the UI refinements of Win 9x. Like Win9x, it did plug&amp;play, but better. It did sound and hardware 3D and so on. It was fast - slower than 9x, yes, but it gave you so many more facilities. And unlike NT4, it wasn&#39;t fragile. Change the IRQ of its network card, it didn&#39;t fall in a heap - it found the new one for itself and kept going. It could even survive changing disk controllers underneath it!

And it throttled the CPU, could sleep and wake and do it reliably, so you used it in confidence it would come back and your work would all be there.

Then it started to go downhill. XP added ugly cosmetic bloat, bundled crapware you can&#39;t remove and so on. This never reverted, it just got worse from then on.

And on the server side, whereas NT4 Server was limited in many ways, it was comprehensible and great for a one-or-2-server LAN such as in a small business. It tromped all over Netware 4 which had all the baggage of NDS which I never wanted or needed and which got in the way with all its big-multisite-network directory crap.

Then came Windows 2000 Server, which destroyed the simplicity of NT 4 Server by adding this vast unfathomable baggage of big-multisite-network directory crap.

Now I look back from a decade later.

There&#39;s a parallel in what&#39;s happened in the transition from the iPad to the iPad 2 and iPhone 3 series to the iPhone 4 series.

The Gen 3 iPhone (and the iPad, the big version) had all these new facilities - 3rd party apps, folders, kinda sorta multitasking, cut&amp;paste - lots of critical stuff missing before was now there, and reasonably quick CPUs and decents amount of RAM.

Then the maker, casting around for a way to improve them, added super-hi-res displays - no functional improvement, just looked nicer, but made them more expensive and more power hungry, but slightly slower at some things.

But behind the surface, life got much harder for programmers, who instead of only supporting 1 screen size and 1 resolution now had to support multiple ones, some very hi-res, and that meant huge new artwork inside the program bundles. &quot;Assets&quot; they&#39;re called these days, I believe.

iPhone 4 and iPad 2 apps are many times larger than those for the previous generation, but with identical functionality - because all the buttons and backgrounds and icons have to be included in a whole array of resolutions, some very large. What was a few hundred kB of bitmaps now is tens of megabytes of them, just so it looks nice on a bigger screen. Because the OS isn&#39;t smart enough to have smart scaled graphics formats and so on, and it doesn&#39;t provide real resolution-independence, just pixel doubling and quadrupling.

Huge increase in size, zero increase in functionality. Now your downloads are huge, your storage gets eaten way quicker, you battery runs down faster and what do you get in return? Nothing at all except it looks a bit prettier.

Well, for me, XP and Vista and 7 and 8 are more of the same. More shiny graphics, more visual effects, tens of gigs of disk eaten, but they can&#39;t do anything significant that the older versions couldn&#39;t.

It&#39;s the same kernel, now in 64-bit. Now, instead of 20 or 30 background tasks, you have 200 or 300. There are little daemons running compositing the screen in 3D, rendering thumbnails of this and previews of that and indexing the other. There are things caching the output of other things.

Stuff is generated in one format, rendered in another and passed to something else for rasterisation, which is then transformed into a texture, saved as a bitmap, passed through 3 levels of driver and composited onto the surface of a 3D object depicting a flat partly-translucent rectangle for display.

This was developed on the Mac as a way to accelerate a Display Postscript-derived rendering language. On Windows and Linux, it&#39;s just chrome: they had perfectly good display-acceleration tech already.

On the Mac, the global search tool is lightning-fast and smart. On an overloaded 300MHz RISC chip from 1999 or so, it can instantly find a word and display a list of apps named that, documents containing it and so on, in well under a second.

On Windows 8 or Ubuntu, if I type &quot;motepad&quot; instead of &quot;notepad&quot;, it doesn&#39;t know what the hell I am talking about and returns no results and possibly an album I don&#39;t want from an online music store I never use.

They&#39;re not even doing /good/ copies of original source. They don&#39;t understand what they&#39;re copying, so they just imitate the surface representation.

Result: we have hugely powerful computers, running huge and hugely-complex OSes, but at the end of the day, they don&#39;t really do anything that Windows for Workgroups didn&#39;t. Disks, directories, data files and binaries. Network client and server built in, big deal. Programs open files in proprietary formats, often over proprietary protocols using reverse-engineered clients for proprietary services.

Same old stuff, but now rendered in true-colour in hardware 3D.

It hasn&#39;t progressed. There&#39;s been no real material benefit in a decade, and in the decade before that, they just got the stuff they&#39;d already delivered working properly.

It looks nicer now. It&#39;s a lot more stable and reliable, too. And there are some nice little functional improvements - even a crappy broken search tool is better than none. When I can and did get an 8-gig quad-core PC of Freecycle, I don&#39;t mind spending some megs of RAM and CPU cycles on self-healing stuff in the background.

But I had hoped for more. I&#39;m not talking about jetpacks and flying cars here, but in the 1980s I had powerful capable programming languages that didn&#39;t require volumes of documentation - they were designed with the express intention of being easy, even for children.

Now, they&#39;re designed to be powerful instead - with vast libraries of code for manipulating text files or building rich web apps or what-have-you.

BBC BASIC was orders of magnitude smaller and simpler, yet it was rich enough and fast enough and expressive enough that Sophie Wilson used it, from choice, to model the ISA of what is now the world&#39;s most successful CPU architecture. These were kid&#39;s tools but they weren&#39;t toys. But the simplicity, the accessibility has been lost.

So this led me to go digging. What happened, when and where? What went wrong?

It&#39;s easy to go back to the origins of the IBM PC, of the ARM and Acorn, of Sinclair, but there is no insight there. The early tools in this family were simple only because they were built to a price and they couldn&#39;t afford anything other than simplicity. The inherent technical restrictions of the time compelled it to be clean and minimal. If we used enhanced Archimedes descendants today, or enhanced Sinclair boxes, they&#39;d be no better than what we have: the really nasty bits of the PC design - conventional memory, segmented memory, register starvation, they&#39;re all fading memories now.

(Indeed, fascinatingly, in the former Communist countries, the Spectrum /did/ evolve into a 16/32-bit desktop PC with ISA slots. You can see glimpses of that parallel universe. It&#39;s not pretty. Nor are the descendants of the Amiga OS and so on.)

So I dug deeper. What did Unix try to grow up into, before a Finnish genius cloned it and freed it forever from the Unix Wars, by trapping it in its own monolithic past.

That leads to Plan 9 and to microkernels and all sorts of exotic fun. Fascinating, but all blind alleys, really. Unix was a low-end, basic answer to perceived 1960s bloat - Multics and so on. It succeeded; it outlived Multics. It also resulted in the extinction of some of the cool tech that it didn&#39;t implement from Multics and so on, such as multi-ring security models.

So the Unix line is not rewarding either. Different answers from the 8-bit home-micro stuff, but only a little different. They even tried to converge, repeatedly - Cromix and OS-9 and things. Didn&#39;t work. The hardware was too limited then.

So I dug into different directions.

One was GUIs. Linux GUIs are basically knock-offs of Windows. (Got an article out of that.) Windows is a knock-off of the Mac. (That story is *way* too old and familiar, even though most Windows advocates still don&#39;t understand the real details of what Apple did.)

Apple&#39;s GUI was a knock-off of Xerox&#39;s. That&#39;s well-known too.

But *here*, here there is a story that is almost untold. The heartbreak of the Xerox engineers - that is occasionally mentioned. But *why* they were heartbroken is not.

Apple only ripped off one-third of the idea. It independently re-invented the other third later on - the hooks were left in.

It ripped off the presentation layer - the GUI. And it massively enhanced it to make it useful on a standalone desktop computer with traditional apps on a traditional single-tasking OS with a traditional filesystem. It changed the world, but it was only the cosmetic angle.

And the other third - the Mac had networking built in. Poor slow serial networking, but much later, they added in the other Xerox tech - Ethernet.

But the missing third was the OS behind the GUI. The GUI was the visible expression of a whole new way of programming, of a live system of interrelated objects. An OS with no binaries, no data files, no fixed static filesystem. It didn&#39;t even have &quot;source code&quot; which passed through a &quot;compiler&quot; to generate &quot;object code&quot; that then went through a &quot;linker&quot; to make &quot;executables&quot;. All that 1960s guff was superceded.

And Apple&#39;s techies didn&#39;t even /notice/ that. They took how it /looked/ and built a copy on top of a 1960s-style OS.

/That&#39;s/ what broke the Xerox folks&#39; hearts. The core, the most clever part - that&#39;s the part that got left behind and ended up a forgotten footnote.

Later, some ex-Apple guys went and tried to rebuild part of this, but on the basis of Unix and an enhanced C. The result is NeXT and in the end theirs became the most valuable company in the history of the world. And that&#39;s based on a pale broken shadow of the real idea, mixed in with enough legacy stuff to make it not too scary and alien.

(That&#39;s one big story. I&#39;ve not really dug into that one yet - I think the parts that are known are too well-known, they eclipse the important bit.)

And after those guys had left, some other Apple guys tried to reinvent the OS and the GUI again, with a new effort that ran on a new RISC chip and dispensed with a filesystem and so on and ran in a wonderful new readable high-level language - but that got canned. A crippled shadow of the product got released, arguably the most sophisticated commercial GUI and platform the world has seen to date, and it bombed and got canned too.

The product that got out was the Newton. The language that /should/ have powered it was Dylan. Read up on Dylan and see a startling vision of a world that could have been.

OK, so that didn&#39;t spring full-formed from its creators&#39; brows. So where did that come from?

Aha. It came from Lisp. And behind that is another story. A story of another very different kind of computing, of a different kind of programming language, of a different kind of programming altogether... One that today is almost forgotten but whose fading memory now erupts as joke &quot;memes&quot;:

http://www.catonmat.net/blog/what-would-john-mccarthy-say-if-he-saw-you-programming/

I am not yet sure but it seems to me that maybe the Smalltalk stuff, the Xerox workstations, all that, took part of its genesis from an effort to make a more accessible, human-readable, even kid-friendly interpretation of the same ideas that the Lisp pioneers had.

Smalltalk boxes and Lisp machines seem to be almost brothers in some ways. Compared to them, there is so little difference between Ubuntu and Windows 8 that you&#39;d need a microscope to spot it.

That is the real story here. It&#39;s the biggest I have ever even glimpsed in my field. And it is so big that when I try to tell people, try to discuss it, that they don&#39;t even believe that it could be real. They just say &quot;oh well Python does that today&quot; or &quot;who cares when I have Debian?&quot;

It&#39;s like a real-life version of the elephant in the room. Everyone else thinks it&#39;s a wall of the room. Actually, they&#39;re in a cubbyhole that the elephant is leaning against. The whole world of computing is in that cubbyhole. As far as I can tell, there are about a dozen people writing or speaking today who have noticed this, in the entire world of computing from ENIAC and Turing to Windows 8.1 and Ubuntu.

I want to get people to notice that the wall has wrinkles and hairs on it and that it&#39;s breathing.</lj-cut>"
"eventtime","2013-10-07 15:21:00"
"url","http://liam-on-linux.livejournal.com/37749.html"
"userid","8744"
"itemid","146"
"event","<span style=""line-height: 1.4;"">So here&#39;s the thought. From things like reading the <a href=""http://web.mit.edu/~simsong/www/ugh.pdf"" target=""_blank"">Unix Hater&#39;s </a></span><a href=""http://web.mit.edu/~simsong/www/ugh.pdf"" target=""_blank"">Handbook</a> [PDF]&nbsp;and so on, I get this impression that there was a time when <a href=""https://en.wikipedia.org/wiki/Lisp_machines"" target=""_blank"">Lisp&nbsp;Machines</a> were widely considered by some very smart people to be the&nbsp;ultimate programmer&#39;s tool, the best lever for the intellect, as it were.

But they&#39;re all dead and gone now.

What I&#39;m wondering is if the Lisp Machine idea could be resurrected on x86&nbsp;using only Free Software.

There are several components. ISTM that if they could be brought together,&nbsp;they could form the core of a Free LispM OS for COTS x86 boxes.

<lj-cut><u>Section 1</u>

<b>Part 1: Movitz</b>

<span style=""line-height: 1.4;""><a href=""http://common-lisp.net/project/movitz/"" target=""_blank"">Movitz </a>is a bare-metal Common Lisp interpreter for x86.</span>

It doesn&#39;t do much - it boots and runs but there&#39;s no filesystem or any&nbsp;way to save anything. Someone&#39;s written an Emacs-alike editor for it and&nbsp;that&#39;s about it.

<b>Part 2: SBCL</b>

<span style=""line-height: 1.4;""><a href=""http://www.sbcl.org/"" target=""_blank"">Steel Bank Common Lisp</a> is a complete GPL Common Lisp environment. It is, I believe, quite </span><span style=""line-height: 1.4;"">feature-complete and rich.</span>

<b>Part 3: the OS</b>

In 2005, MIT <a href=""http://www.unlambda.com/cadr/"" target=""_blank"">released the source of its LispM OS</a> as open source.

This is nowhere near as mature and rich as Symbolics OpenGenera, but it&#39;s&nbsp;a start.

<u>Section 2</u>

<b>Part 4: the weird notation</b>

Probably the biggest thing stopping programmers from other languages&nbsp;investigating Lisp is its odd Polish-prefix-style notation. Old Lisp hands&nbsp;maintain that this is essential to its homoiconicity, but I think that&nbsp;this has been disproved by (e.g.) Dylan, which uses Algol-style algebraic&nbsp;infix notation but (AIUI) retains homoiconicity.

There have been many attempts to offer infix notation for Lisp:
<span style=""line-height: 1.4;"">http://xahlee.info/comp/lisp_sans_sexp.html</span>

The one I personally find most readable, from small code snippets, is&nbsp;Dylan, but that is a whole different language. One I thought was lost to&nbsp;history was <a href=""https://en.wikipedia.org/wiki/CGOL"" target=""_blank"">CGOL</a>.

However, something I only just discovered is that CGOL still exists and is&nbsp;AFAICS it&#39;s now <a href=""http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/lang/lisp/code/syntax/cgol/0.html"" target=""_blank"">freeware</a>...&nbsp;And it <a href=""http://abcl-dev.blogspot.co.uk/2010/04/cgol-on-abcl.html"" target=""_blank"">works on (some)</a> modern versions of Common Lisp:

So, if CGOL was fixed to work on SBCL, there would be one of the barriers&nbsp;considerably lowered.

<b>Part 5 - the weird editor</b>

The other thing that seems to be generally held is that Emacs is the best&nbsp;editor to use for Lisp coding. But Emacs is rather hard if you&#39;re only&nbsp;familiar with computers with late-C20 or C21 GUIs.

However, that too is being resolved, principally in the forms of <a href=""http://ergoemacs.org/"" target=""_blank"">ErgoEmacs </a>and indeed <a href=""http://aquamacs.org/"" target=""_blank"">AquaMacs</a>.

<u>Section 3 - summary</u>

It is probably a very much non-trivial amount of work, but ISTM that&nbsp;there&#39;s potential to assemble some existing FOSS tools and components, and&nbsp;from them build something like a rudimentary LispM OS for x86 hardware,&nbsp;and also provide it with something like a more modern, standards-compliant&nbsp;editor and a language that programmers used to C, VB etc. would not find&nbsp;as intimidating and alien as ordinary Lisp.

And if the LispMs really were as good and as much of a pleasure to work on&nbsp;as many claim, that this would be something well worth doing.</lj-cut>"
"eventtime","2013-10-02 23:23:00"
"url","http://liam-on-linux.livejournal.com/37435.html"
"userid","8744"
"itemid","145"
"event","Package management systems are one of the deepest divides between Linux distros.

One family uses .DEB - Debian and things made from Debian. What makes&nbsp;.DEB good is not the format itself or the basic tools (`dpkg`,&nbsp;`dselect`) that handle it; it&#39;s the meta-package-management tool on&nbsp;top, `apt`. Apt has automatic recursive dependency resolution. This&nbsp;means packages much be fetched from carefully-structured repositories,&nbsp;primarily over the Internet.

RPM is much more basic and for years didn&#39;t have a meta-manager on top&nbsp;and had no form of dependency resolution. I started to use Red Hat in&nbsp;1996 or so and stuck with it for 2-3y. Installing something new&nbsp;usually meant going and finding and installing, <b>in the right order</b>,&nbsp;sometimes <u>hundreds</u> of libraries and dependencies. Certainly&nbsp;typically 4-5-6, maybe dozens.

It was a <i>nightmare</i>.
<lj-cut>
However, this loose coupling made RPM portable.

The first Linux distro ever, pretty much, was the Softlanding Linux System, SLS.

SUSE was originally based on SLS, as was Slackware. SUSE later adopted RPM.

Mandriva started out as Mandrake, which was &quot;Red Hat with KDE&quot;&nbsp;(because KDE was not 100% free at first, because KDE is based on Qt&nbsp;and Qt wasn&#39;t Free.) Thus Mandrake inherited RPM from RH.

Thus the 3 main families of RPM-based distro today:

* Red Hat, meaning RHEL and Fedora and spins thereof (BLAG, CentOS,&nbsp;White Box, Scientific Linux, Oracle Unbreakable Linux, etc) - these&nbsp;use YUM from Yellow Dog for dependency resolution. (YD was Red Hat for&nbsp;PowerPC.)

* Mandriva (inc. Mageia, PCLinuxOS, Ulteo) - forked off RH; uses URPMI&nbsp;for dependency resolution

* SUSE (SLES, SLED, OpenSUSE), which uses YAST for dependency resolution.

That&#39;s one family tree.

The other significant one is Slackware, which tried not to use package&nbsp;management - it&#39;s tarballs all the way down. Remixes include Slitaz,&nbsp;Zipslack and others.

Then there are some recent distros which have invented their own&nbsp;package managers - e.g. Arch and Gentoo. Gentoo builds everything from&nbsp;source and has a tool for managing source trees, `portage`,&nbsp;inspired by FreeBSD&#39;s&nbsp;`ports` tool.

Arch is reinventing the wheel. Personally I don&#39;t see the point.

Even back in the late 1990s, hardcore techies were saying Debian&#39;s&nbsp;system was far and away the best. I tried it then but it was just too&nbsp;hard to install. Some metadistros fixed this to some degree:

- Libranet (commercial, I never tried it)
- Storm Linux, AKA Stormix (Debian with a good graphical installer. I&nbsp;liked it but once installed you were On Your Own.)

Then the first big attempt to make a usable end-user Linux, Corel&nbsp;LinuxOS. I really liked it but it was commercial (with closed-source&nbsp;bits) and fussy about hardware. It died when Corel signed up with&nbsp;Microsoft to use VBA and Office toolbars in Corel Office - there&#39;s no&nbsp;proof but after signing the licensing deal, *suddenly* Corel dropped&nbsp;all its Linux projects.

Corel LinuxOS was spun off and became Xandros. Again, very good but&nbsp;commercial and fussy about hardware.

Noted FOSS advocate Bruce Perens said &quot;something must be done&quot; and&nbsp;proposed an easy, end-user-focused, one-CD all-free Debian-based&nbsp;distro. All-free meant GNOME. His project produced a couple of betas&nbsp;and then foundered and died.

Then along came the SABDFL, and paid for to be done properly. That&#39;s Ubuntu.

All remixes of Debian.

So, in summary, if it uses DEB it&#39;s based on Debian.

RPM was inferior but not so tied to the distro, repositories, etc. and&nbsp;so caught on and was adopted by other distros leading to 3 main&nbsp;families.

And there are some &quot;independents&quot; who&#39;ve invented their own.</lj-cut>"
"eventtime","2013-10-01 15:43:00"
"url","http://liam-on-linux.livejournal.com/37317.html"
"userid","8744"
"itemid","144"
"event","<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">E-book readers are full of electronics. These require large expensive factories, which use a lot of resources. Then the devices are shipped, consuming resources - such hi-tech manufacture is expensive, therefore is done somewhere cheap, meaning international shipping. Books are cheap to print.</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">Then you need a computer with Internet access to get your ebooks - more hi-tech, more distant manufacturing and transport. It downloads books from big websites, meaning big datacentres, meaning lots and lots of manufacturing and power.</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">Then the devices need regular charging - so more power, more fuels being burned, more power distribution.</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">Books tend to last. They&#39;re cheap, need no power, have no DRM (photocopy &#39;em or scan &#39;em if you want - it&#39;s laborious but perfectly doable), can be reused many times by many people, can be lent and borrowed (think libraries), etc.</span>

<lj-cut><span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">So, yes, ebooks are a great and very handy thing, but beware labelling them as very green.</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">OTOH, the newspaper and magazine industries consume vast quantities of wood for a product sold in huge numbers which lasts a single day, perhaps a week or at best a month, is frequently used by one person and then thrown away.</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">That industry is far more environmentally-damaging. Books last for a long time. They&#39;re made from trees. Trees make themselves, as if by magic, from atmospheric carbon dioxide. Trees are actual functioning self-replicating nanotech. Make a tree into a pile of books that will be kept for years and that CO₂ stays locked up.</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">As such, books ain&#39;t so bad.</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">But if tablets replace newspapers with e-book documents, I won&#39;t mind too much.</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">I have an old dead basic Sony e-reader. Its sync software only works on Windows. But now I have an SD card (512MB, i.e., huge) and a memory stick pro (32MB, i.e., still enough for dozens of PDFs) so I just copy files onto them. Much easier.</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">It also displays photos (not too badly), acts as a notepad with a stylus (odd, clever but not very useful)... and plays MP3s, which is odd. I mean, I&#39;m sure the functionality was really cheap to add, but as an ereader, its CPU only wakes occasionally when you turn the page. That means it needs only a tiny battery. But music playback means continuous operation and thus nukes the battery. Not a sensible addition. Text-to-speech for ebooks for visually-impaired people is fine, but phones and MP3 players can do that. No need for it on a tablet or ereader, really.</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">Sure, I wish I had a Kindle with wireless synching but they&#39;re too expensive for me still - and also limited. I also won&#39;t pay for a tablet. I want something that&#39;s both a tablet (arbitrary content, programs, web access etc.) *and* an ereader (thin, light, runs for days to weeks on a charge, like my 1990s PDAs did.)</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">You know the big difference between a 1990s PDA and a tablet? Apart from the fact that my PDAs had really good keyboards?</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">The screen. 1990s meant mono screens. Mono screens use very little power. Colour screens use loads.</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">While we&#39;re waiting for colour e-ink displays, I&#39;d like an Android tablet with a mono LCD screen, please. A big one, iPad sized not Kindle sized. Narrow - 16:9 &nbsp;or something - would be fine.</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">Big tablets are nice gadgets, but expensive, heavy, and as they&#39;re rather fragile do not handle being dropped well.</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">The Mac Portable in 1989 or so had a wonderful mono active-matrix LCD - pin-sharp, daylight-readable without a backlight, or bright and readable indoors in low-light conditions with a backlight. http://lowendmac.com/pb/macintosh-portable.html</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">That was the NINETEEN-EIGHTIES, FFS. It should not be beyond the wit of man to make one now, 25Y later. So I can have something that weighs a couple of hundred ounces but lasts for 3-4 days, so I can use it every day and only charge it twice a week. It would even be able to play video, just in black and white. And it&#39;d be cheap.</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">The Apple display was monochrome. Now we should be able to do 256-level greyscale no problem at all. That looks pretty good in black and white.</span>

<span style="font-family: arial; font-size: small; line-height: normal; background-color: rgb(255, 255, 255);">While we&#39;re waiting for the fancy colour displays, this seems like an obvious move to me. I don&#39;t know if anyone is still manufacturing large mono LCD displays - perhaps for some kind of machine controls or something - but if not, making &#39;em on a colour LCD line wouldn&#39;t be impossible, I think.</span></lj-cut>"
"eventtime","2013-10-01 02:40:00"
"url","http://liam-on-linux.livejournal.com/37116.html"
"userid","8744"
"itemid","143"
"event","I have some editing work looming and I need a small portable machine&nbsp;to do it on. My netbook is a bit <i>too</i> small so I&#39;ve resurrected my&nbsp;2004 Thinkpad X31.

I&#39;ve wiped Lubuntu (as the latest versions need PAE support &amp; although&nbsp;I managed to hack it on there, loads of stuff stopped working), and&nbsp;replaced with with LXLE, the &quot;life extension&quot; for Lubuntu 12.04 (which <b>isn&#39;t</b> an LTS release).

And I&#39;ve wiped Linux Mint Debian Edition and replaced it with the&nbsp;latest Crunchbang.

Then wasted the rest of my evening tweaking Crunchbang, so that GPU&nbsp;acceleration works, compositing is turned off, my Thinkpad&nbsp;middle-mouse button works, turned CapsLock into Super and Right-Alt&nbsp;into Compose, added Zram and tweaked it for a single swap file,&nbsp;dropped the colour depth to 16-bit and more. But I think everything&#39;s&nbsp;working now. I&#39;ve replaced Iceweasel with Firefox &amp; Chrome,&nbsp;Abiword/Gnumeric with LibreOffice 4.1 from wheezy-testing, added&nbsp;Pidgin and some odds and sods.

The question is, which will be more use and more pleasant on a 9YO&nbsp;notebook with just a gig of RAM?

Any guesses? :&not;)"
"eventtime","2013-09-30 15:40:00"
"url","http://liam-on-linux.livejournal.com/36705.html"
"userid","8744"
"itemid","142"
"event","Prompted by this:
http://www.zdnet.com/the-windows-ecosystem-in-2013-is-more-diverse-than-you-think-7000021181/?s_cid=e539&amp;ttag=e539

Linux is doing great at the moment - but for the wrong reasons. Not because it&#39;s caught up with or surpassed the apps or integration of Windows; it hasn&#39;t, it&#39;s a decade or more behind.
<lj-cut>

The reason it&#39;s doing great is that the computer market is shifting, towards tablets and phones and thin clients. They don&#39;t need to do much. A C21 computer is a device for accessing the Web - not the Internet, mostly just the Web over HTTP and HTTPS. And for that, Linux is not merely as good as Windows, it&#39;s better - it&#39;s more secure, meaning no need for anti-malware, meaning it&#39;s quicker. And no licensing means you can just put it on anything anywhere and not worry about it being valid or genuine.

Ubuntu is a pretty good OS - nearly as good as Win7 in many ways, arguably better than W8, and getting quite close to parity with OS X.

In apps, it&#39;s leagues behind - LibreOffice is merely a passable standalone/single-user office suite. Microsoft leads in templates, workflow, macros and scripting. The pairing of Outlook and Exchange is the world&#39;s leading, most feature-rich groupware system by a country mile, for all its many faults. It&#39;s so rich -- and expensive and complex -- that companies are starting to realise they don&#39;t need it all and can switch to Google Apps or something and save a packet.

In terms of deployment and management, Windows Pro plus Windows Server are again a decade ahead. Active Directory, Group Policies etc. are an over-complex mess but they&#39;re essentially free with Windows client and server and they&#39;re an immensely powerful combo - poor support for them is one reason for the lack of penetration of Firefox and so on in corporates.

Windows Server is actually a bloody good OS with excellent rich management tools. Linux isn&#39;t; it&#39;s a toolkit from which you can build a server OS. It&#39;s an DIY kit of a plane compared to a Learjet.

But Linux is just the thing for throwing together huge clouds of virtual servers, and if you&#39;re not too fussed about the fine details of their config - you just want them configured, up and working, *now* - then Linux has the cool tools - Puppet, Chef, Nagios, Docker, loads of them. It&#39;s cheap, fast and scalable, and a few expensive devops can run a huge farm of the things.

What is benefiting Linux is that the landscape is changing. The cloud means thin clients and huge server farms, and Linux is good at that - and cheap. Windows is getting better at it, but it&#39;s expensive and very complex to work out what you owe - which is one reason VMware are still thriving.
</lj-cut>"
"eventtime","2013-09-26 22:36:00"
"url","http://liam-on-linux.livejournal.com/36443.html"
"userid","8744"
